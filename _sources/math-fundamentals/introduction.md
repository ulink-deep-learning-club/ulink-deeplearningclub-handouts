# 引言

## 学习目标与核心概念

在深入探讨计算图、反向传播和梯度下降之前，我们先明确本章的学习目标：

```{admonition} 学习目标
:class: note

- **看懂计算图**：学会把数学表达式变成直观的流程图
- **搞懂反向传播**：理解神经网络如何用链式法则快速计算梯度
- **会用梯度下降**：掌握用梯度信息调整参数，让模型越来越准
- **动手写代码**：在PyTorch中实现这些概念，并调试运行
- **解决真实问题**：通过MNIST手写数字识别，看这些技术如何实际应用
```

## 机器学习的基本问题

机器学习的目标是让计算机从数据中自己发现规律，而不是我们一条条写规则。核心挑战是如何自动调整模型参数，让它能更好地拟合数据、做出准确预测。

```{admonition} 机器学习的核心挑战
:class: note

- **模型复杂度**：现代深度学习模型可能有数百万甚至数十亿个参数
- **优化难度**：在如此高维的空间里找到最优解，就像在迷宫里找出口
- **计算效率**：需要又快又省的算法处理海量数据和复杂模型
- **泛化能力**：模型不仅要在训练数据上表现好，还要在没见过的新数据上靠谱
```

## 为什么需要计算图、反向传播和梯度下降？

这三个概念就像深度学习的“基础设施”，缺一不可：

1. **计算图**：把复杂的数学计算变成一张清晰的流程图，一目了然
2. **反向传播**：高效计算每个参数对最终结果影响的算法（告诉你“谁该背锅”）
3. **梯度下降**：利用计算出的梯度信息，一步步调整参数，让模型越来越准

```{warning}
**历史背景**

这些概念并非一夜之间出现，而是经过了几十年的发展：

- 1960s：链式法则在神经网络中的应用首次被提出
- 1970s：反向传播算法的雏形出现
- 1986年：Rumelhart、Hinton和Williams重新发现并推广了反向传播算法
- 1990s至今：这些技术成为所有深度学习框架的核心

了解历史能帮助我们更好地理解这些技术的来龙去脉。
