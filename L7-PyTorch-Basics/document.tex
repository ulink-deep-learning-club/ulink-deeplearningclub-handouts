\input{../Common/DocumentBaseFormat.tex}
\input{../Common/WebpageHeader.tex}
\input{../Common/HeaderPackages.tex}
\input{../Common/DocumentTheme.tex}

\title{\textbf{PyTorch入门教程：从NumPy到深度学习}}
\author{Anson, 深度学习社\quad \small{Cooperated with \texttt{MiniMax M2} \& \texttt{DeepSeek V3.2 Exp}}}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
本文是PyTorch深度学习框架入门教程。即使你只熟悉Python基础，也能轻松理解！我们将通过详细的步骤解释和大量的代码对比，帮助你从零开始掌握PyTorch。文章从NumPy出发，逐步介绍为什么需要深度学习框架；然后详细讲解张量（Tensors）、自动求导（Autograd）、神经网络模块（nn.Module）等核心概念；最后通过构建一个手写数字识别器来展示完整的工作流程。每个概念都配有实际代码和运行结果，让你边学边练！
\end{abstract}

\tableofcontents
\newpage

\section{引言：为什么选择PyTorch？}

\subsection{从NumPy到PyTorch：一场必然的进化}

如果你已经熟悉Python和NumPy，那么恭喜你！你已经掌握了深度学习的重要基础。NumPy是Python中进行科学计算的强大工具，让我们能够高效地处理多维数组。但当你尝试用NumPy构建深度神经网络时，会遇到一些挑战：

\begin{exampleblock}{NumPy训练神经网络的挑战}
\begin{enumerate}
    \item \textbf{手动实现反向传播：} 每当你修改网络结构，都需要重新推导和实现梯度计算
    \item \textbf{缺乏自动优化：} 你需要手动实现SGD、Adam等优化算法
    \item \textbf{无法利用GPU：} NumPy的运算默认在CPU上，无法利用GPU的并行计算能力
    \item \textbf{缺少神经网络层：} 从零实现卷积层、循环层等工作量巨大
\end{enumerate}
\end{exampleblock}

\begin{alertblock}{核心问题}
每次调整网络结构（比如加一层），都需要重新推导数学公式和计算梯度。对于复杂的网络（比如识别猫的图片），手动计算梯度几乎是不可能的！
\end{alertblock}

PyTorch就是来解决这些问题的！它就像NumPy的"智能升级版"，专为深度学习设计，但保留了NumPy的直观性。

\begin{block}{特别说明}
如果你对NumPy还不熟悉，别担心！我们会在教程中详细对比NumPy和PyTorch，让你同时学习两个工具。记住：PyTorch的语法和NumPy非常相似，学会一个，另一个就很容易理解！
\end{block}

\subsection{PyTorch是什么？}

\begin{block}{PyTorch的定义}
PyTorch是一个开源的深度学习框架，由Facebook（现Meta）的研究团队开发。它提供了一种灵活、高效的方式来构建、训练和部署神经网络。

\textbf{核心特点：}
\begin{itemize}
    \item \textbf{语法简单：} 就像NumPy的"升级版"，学习起来很自然
    \item \textbf{调试方便：} 可以像普通Python代码一样调试，哪里出错改哪里
    \item \textbf{GPU加速：} 一行代码就能用显卡加速计算，速度提升10-100倍！
    \item \textbf{社区活跃：} 遇到问题随时可以找到解决方案
    \item \textbf{研究首选：} 大多数AI论文都用PyTorch实现
\end{itemize}
\end{block}

\begin{exampleblock}{PyTorch vs NumPy 对比}
\begin{itemize}
    \item \textbf{NumPy：} 提供基础的数组操作、数学函数和计算功能。你可以实现任何算法，但需要手动处理梯度计算和优化
    \item \textbf{PyTorch：} 提供智能的深度学习工具——自动求导、预定义网络层、预训练模型。它保留了NumPy的所有功能，但让深度学习变得更简单、更可靠
\end{itemize}
\end{exampleblock}

\begin{alertblock}{好消息！}
\begin{itemize}
    \item \textbf{不需要高深数学：} PyTorch会自动计算所有复杂的导数
    \item \textbf{不需要硬件知识：} 一行代码就能用GPU加速
    \item \textbf{不需要从头开始：} 有很多预训练模型可以直接使用
    \item \textbf{学习曲线平缓：} 从简单项目开始，逐步深入
\end{itemize}
\end{alertblock}

\subsection{PyTorch vs TensorFlow：两大框架的对决}

很多初学者会问：应该学PyTorch还是TensorFlow？让我们用简单的方式来对比：

\begin{table}[H]
\centering
\begin{tabular}{|p{0.25\textwidth}|p{0.35\textwidth}|p{0.35\textwidth}|}
\hline
\textbf{对比维度} & \textbf{PyTorch} & \textbf{TensorFlow} \\
\hline
学习难度 & \textbf{简单}，像写Python代码 & 初期较复杂，需要理解概念\\
调试方式 & \textbf{容易}，像普通Python调试 & 调试较困难，需要特殊工具\\
代码风格 & \textbf{直观自然}，所见即所得 & 需要先定义再执行\\
研究使用度 & \textbf{学术界首选}（论文多） & 工业界应用广泛\\
GPU使用 & \textbf{一行代码}切换CPU/GPU & 需要复杂设置\\
适合人群 & \textbf{初学者、研究人员} & 工程师、部署专家\\
\hline
\end{tabular}
\caption{PyTorch vs TensorFlow对比}
\end{table}

\begin{exampleblock}{如何选择？}
\begin{itemize}
    \item \textbf{选择PyTorch如果你是：}
        \begin{enumerate}
            \item \textbf{深度学习初学者}，想快速上手
            \item \textbf{喜欢动手实验}，需要灵活调试
            \item \textbf{想读AI论文}，大多数论文用PyTorch实现
            \item \textbf{参加竞赛}，需要快速迭代模型
        \end{enumerate}
    \item \textbf{选择TensorFlow如果你是：}
        \begin{enumerate}
            \item \textbf{想开发手机App}，需要移动端部署
            \item \textbf{团队项目}，已有TensorFlow基础设施
            \item \textbf{喜欢可视化}，想看训练过程动画
            \item \textbf{生产环境}，需要稳定部署
        \end{enumerate}
\end{itemize}

\textbf{建议：} \textbf{从PyTorch开始！} 理由：
\begin{itemize}
    \item \textbf{学习更简单：} 语法直观，调试方便
    \item \textbf{资源更丰富：} 大多数教程和论文都用PyTorch
    \item \textbf{未来更广阔：} 学术界和工业界都在转向PyTorch
    \item \textbf{转换更容易：} 学会PyTorch后，TensorFlow也不难
\end{itemize}
\end{exampleblock}

\begin{alertblock}{重要提醒}
不要纠结于选择哪个框架！\textbf{掌握深度学习概念比掌握特定框架更重要}。PyTorch只是帮助你实现想法的工具，就像画笔对于画家一样。
\end{alertblock}

\section{张量（Tensors）：PyTorch的核心数据结构}

\subsection{什么是张量？}

张量听起来很复杂，但其实很简单！在深度学习中，我们处理的数据可能是：
\begin{itemize}
    \item \textbf{一个数字}（标量，0维张量）- 比如温度：25℃
    \item \textbf{一个向量}（1维张量）- 比如学生成绩：[85, 92, 78]
    \item \textbf{一个矩阵}（2维张量）- 比如班级成绩表
    \item \textbf{更高维度}（如图像：3维，批量图像：4维）
\end{itemize}

\begin{block}{张量的定义}
张量（Tensor）就是PyTorch中的"多维数组"。它就像NumPy的ndarray，但有两个超能力：

\textbf{超能力1：} 自动计算梯度（后面会详细讲）
\textbf{超能力2：} 可以用GPU加速计算

\textbf{维度对应（实际例子）：}
\begin{itemize}
    \item \textbf{0维张量：} 一个数字，比如考试成绩：95
    \item \textbf{1维张量：} 一行数字，比如一周温度：[20, 22, 19, 21, 23]
    \item \textbf{2维张量：} 一个表格，比如班级成绩表
    \item \textbf{3维张量：} 一张彩色图片（高度×宽度×颜色通道）
    \item \textbf{4维张量：} 一批图片（图片数量×高度×宽度×颜色通道）
\end{itemize}
\end{block}

\begin{alertblock}{记住这个类比！}
\begin{itemize}
    \item \textbf{标量（0维）：} 一个点
    \item \textbf{向量（1维）：} 一条线
    \item \textbf{矩阵（2维）：} 一个平面
    \item \textbf{张量（3维+）：} 一个立体空间
\end{itemize}
就像从点到线，再到面，最后到立体空间一样自然！
\end{alertblock}

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{Assets/scalar-to-tensor.png}
\caption{不同维度的张量可视化\cite{xinyuchentensordiagarm}}
\end{figure}

\subsection{NumPy vs PyTorch：相同的操作，不同的能力}

让我们通过实际代码对比来理解PyTorch张量。你会发现它们非常相似！

\textbf{NumPy代码（你可能已经熟悉）：}
\begin{lstlisting}[language=Python]
import numpy as np

# 创建数组
a = np.array([1, 2, 3])          # 向量 [1, 2, 3]
b = np.zeros((3, 3))             # 3x3的零矩阵
c = np.random.rand(2, 2)         # 2x2的随机矩阵

# 运算
d = a + 1                        # 每个元素加1
e = np.dot(a, a)                 # 点积
\end{lstlisting}

\textbf{PyTorch代码（新的，但很相似）：}
\begin{lstlisting}[language=Python]
import torch

# 创建张量
a = torch.tensor([1, 2, 3])      # 向量 [1, 2, 3]
b = torch.zeros(3, 3)            # 3x3的零矩阵
c = torch.rand(2, 2)             # 2x2的随机矩阵

# 运算
d = a + 1                        # 每个元素加1
e = torch.dot(a, a)              # 点积

# PyTorch的超能力！
device = torch.device('cuda')    # 使用GPU
c_gpu = c.to(device)             # 把张量移到GPU
\end{lstlisting}

\begin{alertblock}{重要发现：语法几乎一样！}
\begin{itemize}
    \item \textbf{创建数组：} \texttt{np.array()} → \texttt{torch.tensor()}
    \item \textbf{零矩阵：} \texttt{np.zeros()} → \texttt{torch.zeros()}
    \item \textbf{随机矩阵：} \texttt{np.random.rand()} → \texttt{torch.rand()}
    \item \textbf{数学运算：} 完全一样的语法！
\end{itemize}

\textbf{最大的区别：} PyTorch支持GPU计算和自动求导，这是NumPy没有的超能力！
\end{alertblock}

\begin{block}{练习建议}
\begin{enumerate}
    \item \textbf{打开Python环境}（Jupyter Notebook或Python终端）
    \item \textbf{先运行NumPy代码}，确保理解每行代码的作用
    \item \textbf{再运行PyTorch代码}，对比输出结果
    \item \textbf{尝试修改：} 改变数字、矩阵大小，观察变化
    \item \textbf{挑战：} 创建一个3x3的矩阵，计算它的转置
\end{enumerate}
\end{block}

\subsection{张量操作详解}

让我们逐一探索常用的张量操作。这些操作在NumPy和PyTorch中非常相似！

\begin{exampleblock}{基础操作对比（边学边练）}

\textbf{1. 查看形状和维度}
\begin{center}
\begin{tabular}{|p{0.4\textwidth}|p{0.4\textwidth}|}
\hline
\textbf{NumPy} & \textbf{PyTorch} \\
\hline
\texttt{a.shape} & \texttt{a.shape} \\
\texttt{a.ndim} & \texttt{a.dim()} \\
\hline
\end{tabular}
\end{center}

\textbf{实际例子：}
\begin{lstlisting}[language=Python]
# NumPy
a = np.array([[1, 2], [3, 4]])
print(a.shape)  # 输出: (2, 2)
print(a.ndim)   # 输出: 2

# PyTorch
a = torch.tensor([[1, 2], [3, 4]])
print(a.shape)  # 输出: torch.Size([2, 2])
print(a.dim())  # 输出: 2
\end{lstlisting}

\textbf{2. 数学运算（完全一样！）}
\begin{center}
\begin{tabular}{|p{0.4\textwidth}|p{0.4\textwidth}|}
\hline
\textbf{NumPy} & \textbf{PyTorch} \\
\hline
\texttt{np.sum(a)} & \texttt{torch.sum(a)} \\
\texttt{np.mean(a)} & \texttt{torch.mean(a)} \\
\texttt{np.max(a)} & \texttt{torch.max(a)} \\
\hline
\end{tabular}
\end{center}

\textbf{3. 矩阵运算（几乎一样）}
\begin{center}
\begin{tabular}{|p{0.4\textwidth}|p{0.4\textwidth}|}
\hline
\textbf{NumPy} & \textbf{PyTorch} \\
\hline
\texttt{a @ b} 或 \texttt{np.dot(a, b)} & \texttt{torch.matmul(a, b)} \\
\texttt{a.T} & \texttt{a.T} 或 \texttt{a.t()} \\
\hline
\end{tabular}
\end{center}

\end{exampleblock}

\subsection{广播（Broadcasting）：让不同形状的张量一起运算}

广播是PyTorch和NumPy中的一个重要概念，它允许我们对不同形状的张量进行数学运算。这就像数学中的"自动扩展"功能！

\begin{block}{什么是广播？}
广播是一种机制，允许PyTorch自动扩展较小张量的形状，使其与较大张量的形状兼容，从而进行逐元素运算。

\textbf{广播规则：}
\begin{enumerate}
    \item \textbf{从尾部对齐：} 从最后一个维度开始比较
    \item \textbf{维度为1：} 维度为1的轴会被扩展以匹配其他张量
    \item \textbf{缺失维度：} 缺失的维度被视为1
    \item \textbf{其他情况：} 如果维度不匹配且都不是1，则报错
\end{enumerate}
\end{block}

\begin{exampleblock}{广播的实际例子}
\begin{lstlisting}[language=Python]
import torch

# 例子1：标量与向量相加
a = torch.tensor([1, 2, 3])  # 形状: (3,)
b = 10                        # 形状: 标量
c = a + b                    # 广播：10 → [10, 10, 10]
print(c)  # 输出: tensor([11, 12, 13])

# 例子2：向量与矩阵相加
matrix = torch.tensor([[1, 2, 3],
                       [4, 5, 6]])  # 形状: (2, 3)
vector = torch.tensor([10, 20, 30])  # 形状: (3,)
result = matrix + vector            # 广播：vector → [[10,20,30],[10,20,30]]
print(result)
# 输出: tensor([[11, 22, 33],
#               [14, 25, 36]])

# 例子3：不同形状的矩阵运算
A = torch.ones(3, 1, 4)  # 形状: (3, 1, 4)
B = torch.ones(2, 4)     # 形状: (2, 4)
C = A + B               # 广播：A → (3, 2, 4), B → (3, 2, 4)
print(C.shape)          # 输出: torch.Size([3, 2, 4])
\end{lstlisting}
\end{exampleblock}

\begin{alertblock}{广播的应用场景}
\begin{itemize}
    \item \textbf{归一化：} 从每个元素减去均值，除以标准差
    \item \textbf{批量运算：} 对批量数据应用相同的操作
    \item \textbf{权重更新：} 用标量学习率更新所有参数
    \item \textbf{特征缩放：} 对特征进行统一的缩放处理
\end{itemize}
\end{alertblock}

\subsection{张量重塑（Reshaping）：改变形状但不改变数据}

在深度学习中，我们经常需要改变张量的形状。PyTorch提供了多种方法来重塑张量：

\begin{block}{常用的重塑方法}
\begin{itemize}
    \item \texttt{view()：} 返回相同数据的新视图（不复制数据）
    \item \texttt{reshape()：} 返回重塑后的张量（可能复制数据）
    \item \texttt{permute()：} 重新排列维度顺序
    \item \texttt{transpose()：} 交换两个维度
    \item \texttt{squeeze()：} 移除长度为1的维度
    \item \texttt{unsqueeze()：} 添加长度为1的维度
\end{itemize}
\end{block}

\begin{exampleblock}{张量重塑的实际例子}
\begin{lstlisting}[language=Python]
import torch

# 创建一个2x3x4的张量
original = torch.randn(2, 3, 4)
print(f"原始形状: {original.shape}")  # torch.Size([2, 3, 4])

# 使用view改变形状（不复制数据）
flattened = original.view(2, -1)  # -1表示自动计算该维度大小
print(f"展平后: {flattened.shape}")  # torch.Size([2, 12])

# 使用reshape改变形状
reshaped = original.reshape(6, 4)
print(f"重塑后: {reshaped.shape}")  # torch.Size([6, 4])

# 交换维度
transposed = original.transpose(0, 1)  # 交换第0维和第1维
print(f"转置后: {transposed.shape}")  # torch.Size([3, 2, 4])

# 重新排列维度顺序
permuted = original.permute(2, 0, 1)  # 新顺序：原第2维→第0维，原第0维→第1维，原第1维→第2维
print(f"重排后: {permuted.shape}")  # torch.Size([4, 2, 3])

# 添加和移除维度
tensor_1d = torch.tensor([1, 2, 3])
tensor_2d = tensor_1d.unsqueeze(0)  # 在第0维添加维度
print(f"添加维度后: {tensor_2d.shape}")  # torch.Size([1, 3])

tensor_1d_again = tensor_2d.squeeze(0)  # 移除第0维
print(f"移除维度后: {tensor_1d_again.shape}")  # torch.Size([3])
\end{lstlisting}
\end{exampleblock}

\begin{alertblock}{view() vs reshape() 区别}
\begin{itemize}
    \item \texttt{view()：} 要求张量在内存中是连续的，不复制数据，更快
    \item \texttt{reshape()：} 总是返回所需形状，必要时会复制数据，更安全
    \item \textbf{建议：} 优先使用 \texttt{reshape()}，除非确定张量是连续的
\end{itemize}
\end{alertblock}

\subsection{索引和切片（Indexing and Slicing）：访问张量的特定部分}

索引和切片是访问和修改张量特定元素的重要操作。PyTorch的索引语法与NumPy非常相似！

\begin{block}{基本的索引操作}
\begin{itemize}
    \item \textbf{整数索引：} 访问特定位置的元素
    \item \textbf{切片索引：} 访问一个范围内的元素
    \item \textbf{布尔索引：} 使用布尔条件选择元素
    \item \textbf{高级索引：} 使用整数数组索引
\end{itemize}
\end{block}

\textbf{索引和切片的实际例子:}
\begin{lstlisting}[language=Python]
import torch

# 创建一个3x4的矩阵
matrix = torch.tensor([[1, 2, 3, 4],
                       [5, 6, 7, 8],
                       [9, 10, 11, 12]])
print(f"原始矩阵:\n{matrix}")

# 整数索引
first_row = matrix[0]           # 第0行
first_element = matrix[0, 0]    # 第0行第0列
print(f"第0行: {first_row}")     # tensor([1, 2, 3, 4])
print(f"第0行第0列: {first_element}")  # tensor(1)

# 切片索引
first_two_rows = matrix[:2]     # 前2行
middle_columns = matrix[:, 1:3] # 所有行的第1-2列
print(f"前2行:\n{first_two_rows}")
print(f"中间列:\n{middle_columns}")

# 布尔索引
mask = matrix > 5               # 创建布尔掩码
selected = matrix[mask]         # 选择大于5的元素
print(f"大于5的元素: {selected}")  # tensor([6, 7, 8, 9, 10, 11, 12])

# 高级索引
rows = torch.tensor([0, 2])     # 选择第0行和第2行
cols = torch.tensor([1, 3])     # 选择第1列和第3列
selected_elements = matrix[rows, cols]
print(f"选择的行列: {selected_elements}")  # tensor([2, 12])

# 修改元素
matrix[0, 0] = 100              # 修改单个元素
matrix[1] = torch.tensor([50, 60, 70, 80])  # 修改整行
print(f"修改后的矩阵:\n{matrix}")
\end{lstlisting}

\begin{alertblock}{索引注意事项}
\begin{itemize}
    \item \textbf{视图 vs 副本：} 大多数索引操作返回视图（不复制数据）
    \item \textbf{原地修改：} 使用索引修改会原地改变原始张量
    \item \textbf{梯度跟踪：} 索引操作会保持梯度跟踪
    \item \textbf{性能：} 避免在循环中使用复杂索引
\end{itemize}
\end{alertblock}

\begin{block}{张量的高级功能（PyTorch的超能力！）}
除了NumPy的所有功能，PyTorch张量还提供：

\begin{itemize}
    \item \textbf{自动求导：} 设置 \texttt{requires\_grad=True} 自动跟踪梯度
    \item \textbf{GPU加速：} 使用 \texttt{.to(device)} 在CPU/GPU间切换
    \item \textbf{梯度累积：} 自动计算梯度并更新参数
    \item \textbf{内存共享：} 与NumPy共享内存（CPU张量）
    \item \textbf{广播机制：} 自动处理不同形状张量的运算
    \item \textbf{灵活重塑：} 多种方法改变张量形状
    \item \textbf{强大索引：} 灵活的索引和切片操作
\end{itemize}

\textbf{实际例子：}
\begin{lstlisting}[language=Python]
# 自动求导示例
x = torch.tensor(3.0, requires_grad=True)
y = x**2 + 2*x + 1
y.backward()
print(x.grad)  # 自动计算导数：2*x + 2 = 8.0
\end{lstlisting}
\end{block}

\begin{alertblock}{学习提示}
\begin{itemize}
    \item \textbf{不要死记硬背：} 大多数操作和NumPy一样，用的时候查文档就行
    \item \textbf{多动手实验：} 在Python环境中尝试不同的操作
    \item \textbf{理解概念：} 重点是理解张量是什么，而不是记住所有函数
    \item \textbf{循序渐进：} 先掌握基础操作，再学习高级功能
\end{itemize}
\end{alertblock}

\begin{exampleblock}{实战：NumPy vs PyTorch图像处理}
假设我们有一张28×28的灰度图像（MNIST数字）：

\textbf{NumPy实现：}
\begin{lstlisting}[language=Python]
import numpy as np

# 加载图像 (28x28)
image = np.load('digit.npy')

# 展平为向量
pixels = image.flatten()  # 784维

# 标准化
normalized = (pixels - 128) / 128

# 添加批次维度
batch = normalized.reshape(1, -1)
\end{lstlisting}

\textbf{PyTorch实现：}
\begin{lstlisting}[language=Python]
import torch
import torchvision.transforms

# 加载图像 (28x28)
image = torch.load('digit.pt')

# 展平为向量
pixels = image.flatten()  # 784维

# 标准化
normalized = (pixels - 0.1307) / 0.3081

# 添加批次维度
batch = normalized.view(1, -1)

# 轻松切换到GPU！
device = torch.device('cuda')
batch = batch.to(device)
\end{lstlisting}

\end{exampleblock}

\begin{alertblock}{PyTorch vs NumPy 核心优势}
虽然NumPy和PyTorch语法相似，但PyTorch提供了深度学习的关键功能：

\begin{itemize}
    \item \textbf{自动求导：} 一行代码 \texttt{loss.backward()} 自动计算所有梯度
    \item \textbf{GPU加速：} GPU上PyTorch比CPU NumPy快10-100倍
    \item \textbf{神经网络模块：} 预定义层、损失函数、优化器
    \item \textbf{动态计算图：} 支持Python控制流，调试方便
\end{itemize}
\end{alertblock}

\subsection{动手练习：你的第一个PyTorch程序}

现在让我们通过一个完整的练习来巩固所学知识！

\begin{exampleblock}{练习：计算二次函数的导数}
\textbf{任务：} 计算函数 $f(x) = x^2 + 3x + 2$ 在 $x=2$ 处的导数

\textbf{步骤：}
\begin{enumerate}
    \item 创建需要梯度的张量
    \item 计算函数值
    \item 自动求导
    \item 查看结果
\end{enumerate}
\end{exampleblock}

\textbf{完整代码：}
\begin{lstlisting}[language=Python]
import torch

# 1. 创建需要梯度的张量
x = torch.tensor(2.0, requires_grad=True)

# 2. 计算函数值
y = x**2 + 3*x + 2

# 3. 自动求导
y.backward()

# 4. 查看结果
print(f"x = {x.item()}")
print(f"y = {y.item()}")
print(f"导数 dy/dx = {x.grad.item()}")

# 手动验证：导数应该是 2*x + 3 = 2*2 + 3 = 7
print(f"手动验证：2*x + 3 = {2*x.item() + 3}")
\end{lstlisting}

\textbf{预期输出：}
\begin{lstlisting}
x = 2.0
y = 12.0
导数 dy/dx = 7.0
手动验证：2*x + 3 = 7.0
\end{lstlisting}

\begin{block}{扩展练习}
尝试修改代码完成以下任务：

\begin{enumerate}
    \item \textbf{简单：} 计算 $f(x) = x^3$ 在 $x=3$ 处的导数
    \item \textbf{中等：} 计算 $f(x) = \sin(x)$ 在 $x=\pi/4$ 处的导数
    \item \textbf{高级：} 计算 $f(x, y) = x^2 + y^2$ 在 $(x=2, y=3)$ 处的偏导数
\end{enumerate}

\textbf{提示：}
\begin{itemize}
    \item 多个变量需要设置多个张量的 \texttt{requires\_grad=True}
    \item 使用 \texttt{torch.sin()} 计算正弦函数
    \item 使用 \texttt{torch.pi} 获取 $\pi$ 的值
\end{itemize}
\end{block}

\begin{alertblock}{学习建议}
\begin{itemize}
    \item \textbf{不要只看不练：} 一定要在电脑上运行这些代码
    \item \textbf{遇到错误别怕：} 错误是最好的学习机会
    \item \textbf{多尝试修改：} 改变数字、函数，观察变化
    \item \textbf{记录结果：} 把运行结果和理解记下来
\end{itemize}
\end{alertblock}

\section{自动求导（Autograd）：让梯度计算自动化}

\subsection{为什么需要自动求导？}

在深度学习中，我们通过梯度下降法更新网络参数。手动计算梯度不仅容易出错，而且非常耗时。想象一下：

\begin{exampleblock}{手动计算梯度的挑战}
对于一个简单的3层网络：
\begin{equation}
y = W_3(W_2(W_1x + b_1) + b_2) + b_3
\end{equation}

要计算 $\frac{\partial y}{\partial W_1}$，你需要：
\begin{enumerate}
    \item 使用链式法则展开
    \item 计算每个中间变量的导数
    \item 手动实现每个步骤
    \item 每次修改网络结构都要重新推导！
\end{enumerate}

对于ResNet、Transformer等复杂网络，这几乎是不可能的！
\end{exampleblock}

\begin{alertblock}{自动求导的价值}
PyTorch的Autograd自动计算所有梯度。你只需要写前向传播代码，反向传播交给Autograd处理！
\end{alertblock}

\subsection{Autograd工作原理}

\begin{block}{计算图（Computational Graph）}
Autograd通过构建计算图来跟踪操作序列。计算图是一个有向无环图（DAG），记录了数据如何通过操作流动。

\textbf{节点：}
\begin{itemize}
    \item 叶子节点：张量（通常是输入和参数）
    \item 中间节点：操作（如加法、乘法、矩阵乘法）
    \item 根节点：最终结果
\end{itemize}

\textbf{边：} 表示数据流向
\end{block}

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.8]
    % 计算图示例：y = (x * w + b) * 2
    \node[circle, draw=blue!50, fill=blue!20] (x) at (0,3) {$x$};
    \node[circle, draw=blue!50, fill=blue!20] (w) at (0,1.5) {$w$};
    \node[circle, draw=blue!50, fill=blue!20] (b) at (0,0) {$b$};
    \node[circle, draw=green!50, fill=green!20] (mul) at (3,2.25) {$\times$};
    \node[circle, draw=green!50, fill=green!20] (add1) at (5.5,1.5) {$+$};
    \node[circle, draw=green!50, fill=green!20] (mul2) at (8,1.5) {$\times$};
    \node[circle, draw=red!50, fill=red!20] (y) at (10.5,1.5) {$y$};

    % 连接线
    \draw[->, thick] (x) -- (mul);
    \draw[->, thick] (w) -- (mul);
    \draw[->, thick] (mul) -- (add1);
    \draw[->, thick] (b) -- (add1);
    \draw[->, thick] (add1) -- (mul2);
    \draw[->, thick] (mul2) -- (y);

    % 标注梯度方向
    \draw[<-, dashed, red, thick] (y) -- (mul2);
    \draw[<-, dashed, red, thick] (mul2) -- (add1);
    \draw[<-, dashed, red, thick] (add1) -- (mul);
    \draw[<-, dashed, red, thick] (mul) -- (x);

    % 说明
    \node[text width=6cm] at (5.5, -1) {
        \small 实线：前向传播\\
        \small 虚线：反向传播（梯度流动）
    };
\end{tikzpicture}
\caption{计算图示意图}
\end{figure}

\subsection{Autograd的使用方法}

\begin{exampleblock}{最简单的例子：$y = x^2$}
让我们从一个简单例子开始：

\begin{lstlisting}[language=Python]
import torch

# 创建需要梯度的张量
x = torch.tensor(2.0, requires_grad=True)

# 前向计算
y = x ** 2

# 反向传播
y.backward()

# 查看梯度
print(f"x = {x.item()}")
print(f"y = {y.item()}")
print(f"dy/dx = {x.grad.item()}")  # 应该是 2*x = 4.0
\end{lstlisting}

\begin{alertblock}{关键函数}
\begin{itemize}
    \item \texttt{requires\_grad=True}：告诉PyTorch需要计算这个张量的梯度
    \item \texttt{.backward()}：从当前张量开始反向传播，计算所有依赖张量的梯度
    \item \texttt{.grad}：存储计算得到的梯度
    \item \texttt{torch.no\_grad()}：上下文管理器，在此范围内的操作不会被跟踪（用于参数更新）
\end{itemize}
\end{alertblock}
\end{exampleblock}

\subsection{链式法则：Autograd的数学基础}

PyTorch使用链式法则自动计算梯度。让我们手动推导一个例子：

\begin{lstlisting}[language=Python]
# 链式法则示例：z = (x + y)^2
x = torch.tensor(1.0, requires_grad=True)
y = torch.tensor(2.0, requires_grad=True)

# 中间变量
u = x + y      # u = 3.0
z = u ** 2     # z = 9.0

# 反向传播
z.backward()

# 手动验证链式法则：
# dz/dx = dz/du * du/dx = 2u * 1 = 2*3 = 6.0
# dz/dy = dz/du * du/dy = 2u * 1 = 2*3 = 6.0

print(f"dz/dx = {x.grad.item()}")  # 应该是 6.0
print(f"dz/dy = {y.grad.item()}")  # 应该是 6.0
\end{lstlisting}

\begin{block}{Autograd优势}
\begin{itemize}
    \item \textbf{自动化：} 无需手动推导梯度公式
    \item \textbf{准确性：} 计算机计算，避免笔误
    \item \textbf{灵活性：} 任何可以用Python表达的计算图都能自动求导
    \item \textbf{高效性：} 使用高效的C++后端计算
    \item \textbf{可组合：} 复杂的网络可以组合简单的操作
\end{itemize}
\end{block}

\subsection{控制流：动态计算图的神奇之处}

PyTorch的动态计算图允许我们使用Python的控制流（if、for、while）：

\begin{lstlisting}[language=Python]
# 动态控制流示例
x = torch.tensor(1.0, requires_grad=True)

# 根据条件选择不同的计算路径
if x > 0:
    y = x ** 2
else:
    y = x ** 3

y.backward()
print(f"x = {x.item()}, y = {y.item()}, dy/dx = {x.grad.item()}")
\end{lstlisting}

\begin{alertblock}{静态图 vs 动态图}
\begin{itemize}
    \item \textbf{静态图（TensorFlow 1.x）：} 先定义整个计算图，再执行
        \begin{itemize}
            \item 优点：可能更高效
            \item 缺点：调试困难，不支持动态控制流
        \end{itemize}
    \item \textbf{动态图（PyTorch）：} 运行时构建计算图
        \begin{itemize}
            \item 优点：灵活、易调试、支持Python控制流
            \item 缺点：某些优化较困难
        \end{itemize}
\end{itemize}

最新版本的TensorFlow（2.x）也支持动态图了！
\end{alertblock}

\section{神经网络模块（nn.Module）：构建网络的利器}

\subsection{从零实现到高级API}

到目前为止，我们都是直接操作张量和自动求导。但构建复杂神经网络时，这样做会很繁琐。PyTorch提供了高级API来简化工作。

\begin{lstlisting}[language=Python]
# 手动实现 vs nn.Module

# 手动实现（繁琐）
class ManualNet:
    def __init__(self, input_size, hidden_size, output_size):
        self.w1 = torch.randn(input_size, hidden_size, requires_grad=True)
        self.b1 = torch.randn(hidden_size, requires_grad=True)
        self.w2 = torch.randn(hidden_size, output_size, requires_grad=True)
        self.b2 = torch.randn(output_size, requires_grad=True)
    
    def forward(self, x):
        h = torch.relu(x @ self.w1 + self.b1)
        return h @ self.w2 + self.b2

# 使用nn.Module（简洁）
import torch.nn as nn

class SimpleNet(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        return self.fc2(x)
\end{lstlisting}

\begin{alertblock}{使用nn.Module的优势}
\begin{itemize}
    \item \textbf{参数管理：} 自动管理权重和偏置
    \item \textbf{设备管理：} 自动处理CPU/GPU切换
    \item \textbf{状态保存：} 一键保存/加载模型
    \item \textbf{多种层：} 预定义卷积层、池化层、RNN等
    \item \textbf{简洁代码：} 少写很多样板代码
\end{itemize}
\end{alertblock}

\subsection{构建你的第一个神经网络}

让我们用nn.Module构建一个简单的全连接网络：

\begin{lstlisting}[language=Python]
import torch.nn as nn
import torch.nn.functional as F

class SimpleNet(nn.Module):
    def __init__(self, input_size=784, hidden_size=128, output_size=10):
        super().__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.dropout = nn.Dropout(0.2)
    
    def forward(self, x):
        # 展平输入 (batch, 784)
        x = x.view(x.size(0), -1)
        
        # 第一层 + ReLU + Dropout
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        
        # 输出层 (不需要softmax，CrossEntropyLoss会处理)
        x = self.fc2(x)
        return x

# 使用模型
model = SimpleNet()
print(f"模型参数数量: {sum(p.numel() for p in model.parameters())}")
\end{lstlisting}

\begin{block}{nn.Module关键方法}
\begin{itemize}
    \item \texttt{\_\_init\_\_(self, ...)}：初始化层和参数
    \item \texttt{forward(self, x)}：定义前向传播
    \item \texttt{parameters()}: 返回所有可学习参数
    \item \texttt{to(device)}: 将模型移动到GPU/CPU
    \item \texttt{state\_dict()}: 获取参数字典（用于保存）
    \item \texttt{load\_state\_dict(state\_dict)}: 加载参数
\end{itemize}
\end{block}


\subsection{常见的网络层类型}

PyTorch提供了丰富的预定义层：

\begin{table}[H]
\centering
\begin{tabular}{ p{0.25\textwidth} p{0.35\textwidth} p{0.35\textwidth} }
\hline
\textbf{层类型} & \textbf{代码示例} & \textbf{用途} \\
\hline
全连接层 & \texttt{nn.Linear(784, 256)} & 图像分类、特征学习 \\
卷积层 & \texttt{nn.Conv2d(1, 32, 3)} & 图像处理、特征提取 \\
池化层 & \texttt{nn.MaxPool2d(2)} & 降采样、减少计算 \\
Dropout & \texttt{nn.Dropout(0.5)} & 正则化、防止过拟合 \\
批归一化 & \texttt{nn.BatchNorm2d(32)} & 加速训练、稳定梯度 \\
循环层 & \texttt{nn.LSTM(100, 64)} & 序列数据、自然语言 \\
嵌入层 & \texttt{nn.Embedding(1000, 50)} & 词向量、离散特征 \\
\hline
\end{tabular}
\caption{常见的PyTorch网络层}
\end{table}


\begin{alertblock}{形状变化规律}
理解张量形状变化是掌握CNN的关键：

\begin{itemize}
    \item 输入：\texttt{(batch, 1, 28, 28)} - 28×28灰度图，批量2
    \item 卷积：\texttt{Conv2d(1, 32, 3, padding=1)} → \texttt{(batch, 32, 28, 28)}
    \item 池化：\texttt{MaxPool2d(2)} → \texttt{(batch, 32, 14, 14)}
    \item 展平：\texttt{view(-1)} → \texttt{(batch, 6272)}
    \item 全连接：\texttt{Linear(6272, 128)} → \texttt{(batch, 128)}
\end{itemize}

卷积层保持空间维度，池化层减半空间维度！
\end{alertblock}

\section{优化器：让网络自己学习}

\subsection{从手工更新到自动优化}

之前我们手动更新参数：
\begin{lstlisting}[language=Python]
with torch.no_grad():
    w -= learning_rate * w.grad
    b -= learning_rate * b.grad
\end{lstlisting}

但深度网络有数十万参数，手动更新不现实！优化器（Optimizer）来帮忙！

\begin{block}{优化器职责}
\begin{itemize}
    \item 存储所有参数
    \item 计算梯度
    \item 根据优化算法更新参数
    \item 记录优化历史（动量、Adam的历史梯度等）
\end{itemize}
\end{block}


\begin{alertblock}{梯度清零的重要性}
PyTorch会累积梯度。如果不清零，梯度会不断累加！

\textbf{错误示范:}
\begin{lstlisting}[language=Python]
for data, target in loader:
    output = model(data)
    loss = criterion(output, target)
    loss.backward()
    optimizer.step()
    # 错误：每次都累积梯度！
\end{lstlisting}

\textbf{正确做法:}
\begin{lstlisting}[language=Python]
for data, target in loader:
    optimizer.zero_grad()  # 清零！
    output = model(data)
    loss = criterion(output, target)
    loss.backward()
    optimizer.step()
\end{lstlisting}
\end{alertblock}

\subsection{常见优化算法对比}

\begin{table}[H]
\centering
\begin{tabular}{|p{0.2\textwidth}|p{0.35\textwidth}|p{0.4\textwidth}|}
\hline
\textbf{算法} & \textbf{代码} & \textbf{特点} \\
\hline
SGD & \texttt{SGD(model.parameters(), lr=0.1)} & 简单但震荡，可能陷入局部最优 \\
SGD + 动量 & \texttt{SGD(model.parameters(), lr=0.1, momentum=0.9)} & 减少震荡，加速收敛 \\
Adam & \texttt{Adam(model.parameters(), lr=0.001)} & 最常用，自适应学习率 \\
RMSprop & \texttt{RMSprop(model.parameters())} & 适合RNN、非平稳目标 \\
AdaGrad & \texttt{Adagrad(model.parameters())} & 稀疏数据效果好 \\
\hline
\end{tabular}
\caption{常见优化算法对比}
\end{table}



\begin{alertblock}{现代实践：AdamW}
AdamW是Adam的改进版，解决了权重衰减的问题：

\begin{lstlisting}[language=Python]
# AdamW = Adam + 改进的权重衰减
optimizer = optim.AdamW(
    model.parameters(),
    lr=0.001,
    weight_decay=0.01,  # 更合理的权重衰减
    betas=(0.9, 0.999)
)
\end{lstlisting}
\end{alertblock}

\section{完整训练流程：从数据到模型}

现在让我们把everything together，训练一个完整的MNIST分类器：

\begin{lstlisting}[language=Python]
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms

# 1. 设置设备
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# 2. 数据加载
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

train_dataset = datasets.MNIST(root='./data', train=True,
                              download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

# 3. 定义模型
model = SimpleNet().to(device)

# 4. 选择损失函数和优化器
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 5. 训练循环
for epoch in range(5):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        
        # 前向传播
        output = model(data)
        loss = criterion(output, target)
        
        # 反向传播
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if batch_idx % 100 == 0:
            print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}')
\end{lstlisting}

\begin{alertblock}{训练流程关键步骤}
\begin{enumerate}
    \item \textbf{设置设备：} 确定使用GPU还是CPU
    \item \textbf{数据加载：} 使用DataLoader批量加载数据
    \item \textbf{定义模型：} 使用nn.Module构建网络
    \item \textbf{选择损失函数：} CrossEntropyLoss适合分类任务
    \item \textbf{选择优化器：} Adam是最安全的选择
    \item \textbf{训练循环：} 前向→清零梯度→反向→更新
    \item \textbf{评估模型：} 在测试集上验证性能
\end{enumerate}
\end{alertblock}

\begin{block}{训练技巧}
\begin{itemize}
    \item \textbf{数据增强：} \texttt{transforms.RandomRotation, transforms.RandomAffine}
    \item \textbf{学习率调度：} \texttt{torch.optim.lr\_scheduler}
    \item \textbf{早停：} 验证损失不再下降时停止训练
    \item \textbf{梯度裁剪：} 防止梯度爆炸
    \item \textbf{混合精度训练：} 使用float16加速训练
    \item \textbf{梯度累积：} 模拟大批量训练
\end{itemize}
\end{block}

\section{调试与可视化：让训练过程透明化}

\subsection{常见错误及解决方案}

作为初学者，你可能会遇到一些常见错误。别担心，这些都很正常！

\begin{exampleblock}{错误1：忘记导入PyTorch}
\textbf{错误信息：} \texttt{NameError: name 'torch' is not defined}

\textbf{解决方案：}
\begin{lstlisting}[language=Python]
# 忘记导入！
x = torch.tensor([1, 2, 3])  # 错误！

# 正确做法：
import torch
x = torch.tensor([1, 2, 3])  # 正确！
\end{lstlisting}
\end{exampleblock}

\begin{exampleblock}{错误2：忘记清零梯度}
\textbf{错误现象：} 梯度越来越大，训练不稳定

\textbf{解决方案：}
\begin{lstlisting}[language=Python]
# 错误示范
for data, target in loader:
    output = model(data)
    loss = criterion(output, target)
    loss.backward()
    optimizer.step()  # 梯度累积！

# 正确做法
for data, target in loader:
    optimizer.zero_grad()  # 清零梯度！
    output = model(data)
    loss = criterion(output, target)
    loss.backward()
    optimizer.step()
\end{lstlisting}
\end{exampleblock}

\begin{exampleblock}{错误3：张量形状不匹配}
\textbf{错误信息：} \texttt{RuntimeError: The size of tensor a must match the size of tensor b}

\textbf{解决方案：}
\begin{lstlisting}[language=Python]
# 检查张量形状
print(f"张量1形状: {tensor1.shape}")
print(f"张量2形状: {tensor2.shape}")

# 常见形状问题：
# - 矩阵乘法：A.shape = (3,4), B.shape = (4,5)  形状匹配!
# - 矩阵乘法：A.shape = (3,4), B.shape = (3,5)  形状不匹配!
\end{lstlisting}
\end{exampleblock}

\subsection{调试工具和技巧}

\begin{block}{调试技巧}
\begin{itemize}
    \item \textbf{打印张量形状：} \texttt{print(tensor.shape)}
    \item \textbf{检查数据类型：} \texttt{print(tensor.dtype)}
    \item \textbf{查看梯度：} \texttt{print(tensor.grad)}
    \item \textbf{检查设备：} \texttt{print(tensor.device)}
    \item \textbf{使用断言：} \texttt{assert tensor.shape == expected\_shape}
\end{itemize}
\end{block}

\begin{exampleblock}{调试示例}
\begin{lstlisting}[language=Python]
import torch

# 创建张量
x = torch.randn(2, 3)
y = torch.randn(3, 4)

# 调试信息
print(f"x形状: {x.shape}")  # torch.Size([2, 3])
print(f"y形状: {y.shape}")  # torch.Size([3, 4])
print(f"x设备: {x.device}")  # cpu
print(f"x数据类型: {x.dtype}")  # torch.float32

# 矩阵乘法
z = torch.matmul(x, y)
print(f"z形状: {z.shape}")  # torch.Size([2, 4])
\end{lstlisting}
\end{exampleblock}

\begin{alertblock}{模型保存最佳实践}
\begin{itemize}
    \item \textbf{生产环境：} 只保存 \texttt{state\_dict}，避免版本兼容问题
    \item \textbf{研究环境：} 保存完整检查点，方便恢复训练
    \item \textbf{模型部署：} 使用 \texttt{torch.jit.script} 转换为TorchScript
    \item \textbf{云端部署：} 考虑ONNX格式（跨框架兼容）
\end{itemize}
\end{alertblock}

\begin{block}{调试建议}
\begin{itemize}
    \item \textbf{从简单开始：} 先运行小例子，确保基础正确
    \item \textbf{逐步构建：} 每加一行代码就测试一次
    \item \textbf{善用打印：} 多用 \texttt{print()} 查看中间结果
    \item \textbf{理解错误：} 不要只看错误信息，要理解为什么出错
    \item \textbf{寻求帮助：} 遇到问题先搜索，再问老师或同学
\end{itemize}
\end{block}

\section{PyTorch生态系统：超越框架本身}

PyTorch不仅仅是一个框架，它是一个完整的生态系统：

\subsection{核心组件}

\begin{table}[H]
\centering
\begin{tabular}{|p{0.25\textwidth}|p{0.7\textwidth}|}
\hline
\textbf{组件} & \textbf{作用} \\
\hline
TorchVision & 计算机视觉模型和工具 \\
TorchText & 自然语言处理 \\
TorchAudio & 音频处理 \\
TorchServe & 模型部署服务 \\
PyTorch Lightning & 简化训练代码 \\
Accelerate & 多GPU/分布式训练 \\
\hline
\end{tabular}
\caption{PyTorch生态系统核心组件}
\end{table}



\begin{block}{学习PyTorch的价值}
\begin{itemize}
    \item \textbf{学术研究：} PyTorch是顶会论文的首选框架，跟上最新研究
    \item \textbf{工业应用：} Meta、OpenAI、特斯拉等公司都在使用
    \item \textbf{学习曲线：} 语法直观，快速上手深度学习
    \item \textbf{调试友好：} 动态图让调试像Python一样自然
    \item \textbf{社区支持：} 活跃的开发者社区，丰富的教程和工具
    \item \textbf{未来发展：} PyTorch 2.0进一步提升性能和易用性
\end{itemize}
\end{block}

\begin{alertblock}{学习路径建议}
建议的学习路径：

\begin{enumerate}
    \item \textbf{基础阶段：} 熟悉NumPy，学习张量操作
    \item \textbf{核心概念：} 理解自动求导，完成练习
    \item \textbf{网络构建：} 使用nn.Module构建网络
    \item \textbf{完整项目：} 完成MNIST分类项目
    \item \textbf{进阶学习：} 学习卷积神经网络，完成图像分类
    \item \textbf{序列处理：} 学习循环神经网络，处理序列数据
    \item \textbf{迁移学习：} 尝试迁移学习，使用预训练模型
    \item \textbf{深入研究：} 参与开源项目，阅读论文
\end{enumerate}
\end{alertblock}

\section{结论：PyTorch开启深度学习之旅}

恭喜你！你已经完成了PyTorch的入门学习。从NumPy出发，我们一步步探索了PyTorch的核心概念和实际应用。现在你已经掌握了：

\begin{itemize}
    \item \textbf{张量（Tensors）：} PyTorch的核心数据结构，就像NumPy数组但有超能力
    \item \textbf{自动求导（Autograd）：} 让计算机自动计算梯度，省去手动推导的麻烦
    \item \textbf{神经网络模块（nn.Module）：} 构建深度学习模型的利器
    \item \textbf{优化器（Optimizer）：} 让网络自己学习参数
    \item \textbf{完整训练流程：} 从数据加载到模型训练的全过程
\end{itemize}

\begin{block}{学习原则总结}
\begin{itemize}
    \item \textbf{循序渐进：} 从NumPy思维到PyTorch思维，一步步来
    \item \textbf{对比学习：} 通过与NumPy、TensorFlow对比理解差异
    \item \textbf{实践导向：} 每个概念都配合实际代码示例，边学边练
    \item \textbf{问题驱动：} 解答"为什么要这样做"的疑问，理解原理
    \item \textbf{生态思维：} 理解PyTorch在整个深度学习工作流中的作用
\end{itemize}
\end{block}

\begin{alertblock}{下一步行动建议}
建议立即开始实践：

\begin{enumerate}
    \item \textbf{复制代码：} 运行本文所有代码，确保能跑通
    \item \textbf{修改参数：} 尝试不同的batch\_size、学习率，观察变化
    \item \textbf{扩展模型：} 在SimpleNet基础上加一层，观察效果
    \item \textbf{尝试CNN：} 用卷积层替换全连接层，体验图像处理
    \item \textbf{实际项目：} 选择感兴趣的数据集进行分类
    \item \textbf{阅读源码：} 查看PyTorch官方文档和示例
    \item \textbf{参与社区：} 在GitHub、Stack Overflow提问和回答
\end{enumerate}

\textbf{深度学习是实践的学科！} 读10遍不如动手做1遍。犯错是学习的一部分，不要害怕尝试！
\end{alertblock}

开始你的PyTorch之旅吧！

\begin{thebibliography}{9}

\bibitem{xinyuchentensordiagarm}
X. Chen, ``Intuitive Understanding of Tensors in Machine Learning,'' \emph{[Online]}. Available: \url{https://medium.com/@xinyu.chen/intuitive-understanding-of-tensors-in-machine-learning-33635c64b596}. [Accessed: Nov. 30, 2025].
\end{thebibliography}

\end{document}
