<!DOCTYPE html>
<html lang="en-US">
<head>
<meta charset="UTF-8" />
<meta name="generator" content="LaTeX Lwarp package" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title> 计算图、反向传播与梯度下降：深度学习核心数学基础 </title>
<link rel="stylesheet" type="text/css" href="lwarp.css" />

<script>
// Lwarp MathJax emulation code
//
// Based on code by Davide P. Cervone.
// Equation numbering: https://github.com/mathjax/MathJax/issues/2427
// Starred and ifnextchar macros: https://github.com/mathjax/MathJax/issues/2428
// \left, \right delimiters: https://github.com/mathjax/MathJax/issues/2535
//
// Modified by Brian Dunn to adjust equation numbering and add subequations.
//
// LaTeX can use \seteqnumber{subequations?}{section}{number} before each equation.
// subequations? is 0 usually, 1 if inside subequations.
// section is a string printed as-is, or empty.
// number is auto-incremented by MathJax between equations.
//
MathJax = {
    subequations: "0",
    section: "",
    loader: {
        load: ['[tex]/tagformat', '[tex]/textmacros'],
    },
    startup: {
        ready() {
            //       These would be replaced by import commands if you wanted to make
            //       a proper extension.
            const Configuration = MathJax._.input.tex.Configuration.Configuration;
            const CommandMap = MathJax._.input.tex.SymbolMap.CommandMap;
            const Macro = MathJax._.input.tex.Symbol.Macro;
            const TexError = MathJax._.input.tex.TexError.default;
            const ParseUtil = MathJax._.input.tex.ParseUtil.default;
            const expandable = MathJax._.util.Options.expandable;


            //       Insert the replacement string into the TeX string, and check
            //       that there haven't been too many maxro substitutions (prevents
            //       infinite loops).
            const useArgument = (parser, text) => {
                parser.string = ParseUtil.addArgs(parser, text, parser.string.slice(parser.i));
                parser.i = 0;
                if (++parser.macroCount > parser.configuration.options.maxMacros) {
                     throw new TexError('MaxMacroSub1',
                     'MathJax maximum macro substitution count exceeded; ' +
                     'is there a recursive macro call?');
                }
            }


            //       Create the command map for:
            //           \ifstar, \ifnextchar, \ifblank, \ifstrequal, \gsub, \seteqnumber
            new CommandMap('Lwarp-macros', {
                ifstar: 'IfstarFunction',
                ifnextchar: 'IfnextcharFunction',
                ifblank: 'IfblankFunction',
                ifstrequal: 'IfstrequalFunction',
                gsubstitute: 'GsubstituteFunction',
                seteqnumber: 'SeteqnumberFunction'
            }, {
                //       This function implements an ifstar macro.
                IfstarFunction(parser, name) {
                     const resultstar = parser.GetArgument(name);
                     const resultnostar = parser.GetArgument(name);
                     const star = parser.GetStar();                      // true if there is a *
                     useArgument(parser, star ? resultstar : resultnostar);
                },


                //       This function implements an ifnextchar macro.
                IfnextcharFunction(parser, name) {
                     let whichchar = parser.GetArgument(name);
                     if (whichchar.match(/^(?:0x[0-9A-F]+|[0-9]+)$/i)) {
                         // $ syntax highlighting
                         whichchar = String.fromCodePoint(parseInt(whichchar));
                     }
                     const resultnextchar = parser.GetArgument(name);
                     const resultnotnextchar = parser.GetArgument(name);
                     const gotchar = (parser.GetNext() === whichchar);
                     useArgument(parser, gotchar ? resultnextchar : resultnotnextchar);
                },


                // This function implements an ifblank macro.
                IfblankFunction(parser, name) {
                     const blankarg = parser.GetArgument(name);
                     const resultblank = parser.GetArgument(name);
                     const resultnotblank = parser.GetArgument(name);
                     const isblank = (blankarg.trim() == "");
                     useArgument(parser, isblank ? resultblank : resultnotblank);
                },


                // This function implements an ifstrequal macro.
                IfstrequalFunction(parser, name) {
                     const strequalfirst = parser.GetArgument(name);
                     const strequalsecond = parser.GetArgument(name);
                     const resultequal = parser.GetArgument(name);
                     const resultnotequal = parser.GetArgument(name);
                     const isequal = (strequalfirst == strequalsecond);
                     useArgument(parser, isequal ? resultequal : resultnotequal);
                },


                // This function implements a gsub macro.
                GsubstituteFunction(parser, name) {
                     const gsubfirst = parser.GetArgument(name);
                     const gsubsecond = parser.GetArgument(name);
                     const gsubthird = parser.GetArgument(name);
                     let gsubresult=gsubfirst.replace(gsubsecond, gsubthird);
                     useArgument(parser, gsubresult);
                },


                //       This function modifies the equation numbers.
                SeteqnumberFunction(parser, name) {
                         // Get the macro parameters
                         const star = parser.GetStar();                    // true if there is a *
                         const optBrackets = parser.GetBrackets(name);     // contents of optional brackets
                         const newsubequations = parser.GetArgument(name);    // the subequations argument
                         const neweqsection = parser.GetArgument(name);    // the eq section argument
                         const neweqnumber = parser.GetArgument(name);     // the eq number argument
                         MathJax.config.subequations=newsubequations ;     // a string with boolean meaning
                         MathJax.config.section=neweqsection ;             // a string with numeric meaning
                         parser.tags.counter = parser.tags.allCounter = neweqnumber ;
                }


            });


            //       Create the Lwarp-macros package
            Configuration.create('Lwarp-macros', {
                handler: {macro: ['Lwarp-macros']}
            });


            MathJax.startup.defaultReady();


            // For forward references:
            MathJax.startup.input[0].preFilters.add(({math}) => {
                if (math.inputData.recompile){
                         MathJax.config.subequations = math.inputData.recompile.subequations;
                         MathJax.config.section = math.inputData.recompile.section;
                }
            });
            MathJax.startup.input[0].postFilters.add(({math}) => {
                if (math.inputData.recompile){
                         math.inputData.recompile.subequations = MathJax.config.subequations;
                         math.inputData.recompile.section = MathJax.config.section;
                }
            });


                // For \left, \right with unicode-math:
                const {DelimiterMap} = MathJax._.input.tex.SymbolMap;
                const {Symbol} = MathJax._.input.tex.Symbol;
                const {MapHandler} = MathJax._.input.tex.MapHandler;
                const delimiter = MapHandler.getMap('delimiter');
                delimiter.add('\\lBrack', new Symbol('\\lBrack', '\u27E6'));
                delimiter.add('\\rBrack', new Symbol('\\rBrack', '\u27E7'));
                delimiter.add('\\lAngle', new Symbol('\\lAngle', '\u27EA'));
                delimiter.add('\\rAngle', new Symbol('\\rAngle', '\u27EB'));
                delimiter.add('\\lbrbrak', new Symbol('\\lbrbrak', '\u2772'));
                delimiter.add('\\rbrbrak', new Symbol('\\rbrbrak', '\u2773'));
                delimiter.add('\\lbag', new Symbol('\\lbag', '\u27C5'));
                delimiter.add('\\rbag', new Symbol('\\rbag', '\u27C6'));
                delimiter.add('\\llparenthesis', new Symbol('\\llparenthesis', '\u2987'));
                delimiter.add('\\rrparenthesis', new Symbol('\\rrparenthesis', '\u2988'));
                delimiter.add('\\llangle', new Symbol('\\llangle', '\u2989'));
                delimiter.add('\\rrangle', new Symbol('\\rrangle', '\u298A'));
                delimiter.add('\\Lbrbrak', new Symbol('\\Lbrbrak', '\u27EC'));
                delimiter.add('\\Rbrbrak', new Symbol('\\Rbrbrak', '\u27ED'));
                delimiter.add('\\lBrace', new Symbol('\\lBrace', '\u2983'));
                delimiter.add('\\rBrace', new Symbol('\\rBrace', '\u2984'));
                delimiter.add('\\lParen', new Symbol('\\lParen', '\u2985'));
                delimiter.add('\\rParen', new Symbol('\\rParen', '\u2986'));
                delimiter.add('\\lbrackubar', new Symbol('\\lbrackubar', '\u298B'));
                delimiter.add('\\rbrackubar', new Symbol('\\rbrackubar', '\u298C'));
                delimiter.add('\\lbrackultick', new Symbol('\\lbrackultick', '\u298D'));
                delimiter.add('\\rbracklrtick', new Symbol('\\rbracklrtick', '\u298E'));
                delimiter.add('\\lbracklltick', new Symbol('\\lbracklltick', '\u298F'));
                delimiter.add('\\rbrackurtick', new Symbol('\\rbrackurtick', '\u2990'));
                delimiter.add('\\langledot', new Symbol('\\langledot', '\u2991'));
                delimiter.add('\\rangledot', new Symbol('\\rangledot', '\u2992'));
                delimiter.add('\\lparenless', new Symbol('\\lparenless', '\u2993'));
                delimiter.add('\\rparengtr', new Symbol('\\rparengtr', '\u2994'));
                delimiter.add('\\Lparengtr', new Symbol('\\Lparengtr', '\u2995'));
                delimiter.add('\\Rparenless', new Symbol('\\Rparenless', '\u2996'));
                delimiter.add('\\lblkbrbrak', new Symbol('\\lblkbrbrak', '\u2997'));
                delimiter.add('\\rblkbrbrak', new Symbol('\\rblkbrbrak', '\u2998'));
                delimiter.add('\\lvzigzag', new Symbol('\\lvzigzag', '\u29D8'));
                delimiter.add('\\rvzigzag', new Symbol('\\rvzigzag', '\u29D9'));
                delimiter.add('\\Lvzigzag', new Symbol('\\Lvzigzag', '\u29DA'));
                delimiter.add('\\Rvzigzag', new Symbol('\\Rvzigzag', '\u29DB'));
                delimiter.add('\\lcurvyangle', new Symbol('\\lcurvyangle', '\u29FC'));
                delimiter.add('\\rcurvyangle', new Symbol('\\rcurvyangle', '\u29FD'));
                delimiter.add('\\Vvert', new Symbol('\\Vvert', '\u2980'));
        }        // ready
    },           // startup


    tex: {
        packages: {'[+]': ['tagformat', 'Lwarp-macros', 'textmacros']},
        tags: "ams",
                tagformat: {
                         number: function (n) {
                              if(MathJax.config.subequations==0)
                                 return(MathJax.config.section + n);
                              else
                                 return(MathJax.config.section + String.fromCharCode(96+n));
                         },
                },
    }
}
</script>


<script
        id="MathJax-script"
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"
></script>


</head>
<body>
<!--|Using lwarp|document.html|-->



<div class="bodywithoutsidetoc">



<main class="bodycontainer">



<section class="textbody">

<a id="document-autofile-0"></a>

<!--MathJax customizations:-->
<div data-nosnippet
         style="display:none"
>

\(\newcommand{\footnotename}{footnote}\)

\(\def \LWRfootnote {1}\)

\(\newcommand {\footnote }[2][\LWRfootnote ]{{}^{\mathrm {#1}}}\)

\(\newcommand {\footnotemark }[1][\LWRfootnote ]{{}^{\mathrm {#1}}}\)

\(\let \LWRorighspace \hspace \)

\(\renewcommand {\hspace }{\ifstar \LWRorighspace \LWRorighspace }\)

\(\newcommand {\TextOrMath }[2]{#2}\)

\(\newcommand {\mathnormal }[1]{{#1}}\)

\(\newcommand \ensuremath [1]{#1}\)

\(\newcommand {\LWRframebox }[2][]{\fbox {#2}} \newcommand {\framebox }[1][]{\LWRframebox } \)

\(\newcommand {\setlength }[2]{}\)

\(\newcommand {\addtolength }[2]{}\)

\(\newcommand {\setcounter }[2]{}\)

\(\newcommand {\addtocounter }[2]{}\)

\(\newcommand {\arabic }[1]{}\)

\(\newcommand {\number }[1]{}\)

\(\newcommand {\noalign }[1]{\text {#1}\notag \\}\)

\(\newcommand {\cline }[1]{}\)

\(\newcommand {\directlua }[1]{\text {(directlua)}}\)

\(\newcommand {\luatexdirectlua }[1]{\text {(directlua)}}\)

\(\newcommand {\protect }{}\)

\(\def \LWRabsorbnumber #1 {}\)

\(\def \LWRabsorbquotenumber &quot;#1 {}\)

\(\newcommand {\LWRabsorboption }[1][]{}\)

\(\newcommand {\LWRabsorbtwooptions }[1][]{\LWRabsorboption }\)

\(\def \mathchar {\ifnextchar &quot;\LWRabsorbquotenumber \LWRabsorbnumber }\)

\(\def \mathcode #1={\mathchar }\)

\(\let \delcode \mathcode \)

\(\let \delimiter \mathchar \)

\(\def \oe {\unicode {x0153}}\)

\(\def \OE {\unicode {x0152}}\)

\(\def \ae {\unicode {x00E6}}\)

\(\def \AE {\unicode {x00C6}}\)

\(\def \aa {\unicode {x00E5}}\)

\(\def \AA {\unicode {x00C5}}\)

\(\def \o {\unicode {x00F8}}\)

\(\def \O {\unicode {x00D8}}\)

\(\def \l {\unicode {x0142}}\)

\(\def \L {\unicode {x0141}}\)

\(\def \ss {\unicode {x00DF}}\)

\(\def \SS {\unicode {x1E9E}}\)

\(\def \dag {\unicode {x2020}}\)

\(\def \ddag {\unicode {x2021}}\)

\(\def \P {\unicode {x00B6}}\)

\(\def \copyright {\unicode {x00A9}}\)

\(\def \pounds {\unicode {x00A3}}\)

\(\let \LWRref \ref \)

\(\renewcommand {\ref }{\ifstar \LWRref \LWRref }\)

\( \newcommand {\multicolumn }[3]{#3}\)

\(\require {textcomp}\)

\(\newcommand {\intertext }[1]{\text {#1}\notag \\}\)

\(\let \Hat \hat \)

\(\let \Check \check \)

\(\let \Tilde \tilde \)

\(\let \Acute \acute \)

\(\let \Grave \grave \)

\(\let \Dot \dot \)

\(\let \Ddot \ddot \)

\(\let \Breve \breve \)

\(\let \Bar \bar \)

\(\let \Vec \vec \)

\(\newcommand {\toprule }[1][]{\hline }\)

\(\let \midrule \toprule \)

\(\let \bottomrule \toprule \)

\(\def \LWRbooktabscmidruleparen (#1)#2{}\)

\(\newcommand {\LWRbooktabscmidrulenoparen }[1]{}\)

\(\newcommand {\cmidrule }[1][]{\ifnextchar (\LWRbooktabscmidruleparen \LWRbooktabscmidrulenoparen }\)

\(\newcommand {\morecmidrules }{}\)

\(\newcommand {\specialrule }[3]{\hline }\)

\(\newcommand {\addlinespace }[1][]{}\)

\(\newcommand {\tcbset }[1]{}\)

\(\newcommand {\tcbsetforeverylayer }[1]{}\)

\(\newcommand {\tcbox }[2][]{\boxed {\text {#2}}}\)

\(\newcommand {\tcboxfit }[2][]{\boxed {#2}}\)

\(\newcommand {\tcblower }{}\)

\(\newcommand {\tcbline }{}\)

\(\newcommand {\tcbtitle }{}\)

\(\newcommand {\tcbsubtitle [2][]{\mathrm {#2}}}\)

\(\newcommand {\tcboxmath }[2][]{\boxed {#2}}\)

\(\newcommand {\tcbhighmath }[2][]{\boxed {#2}}\)

</div>

<a id="document-autopage-1"></a>
<div class="titlepage">

<h1><b> 计算图、反向传播与梯度下降：深度学习核心数学基础 </b></h1>



<div class="author">



<div class="oneauthor">

<p>
Anson, 深度学习社 &#x2003;Cooperated with <kbd>Kimi K2 0905</kbd>
</p>
</div>

</div>



<div class="titledate">

<p>
2025 年 12 月 3 日
</p>
</div>

</div>
<div class="abstract">



<div class="abstracttitle"> 摘要 </div>

<p>
本文深入探讨了深度学习中的三个核心数学基础：计算图表示、反向传播算法和梯度下降优化。通过 MNIST 手写数字识别任务作为贯穿始终的实例，我们从基础概念出发，结合详细的数学推导、直观的图示和完整的 PyTorch 实现，
系统性地阐述了这些技术在深度学习中的重要作用。文章首先介绍计算图如何将复杂的数学表达式可视化为图结构，然后详细推导反向传播算法如何基于链式法则高效计算梯度，最后探讨梯度下降如何利用梯度信息优化模型参数。
通过实际的代码示例，我们展示了现代深度学习框架中的自动微分系统实现，并提供了实用的优化策略和实际应用指导。本文不仅涵盖基本理论，还包含了可运行的 Python 代码，帮助读者深入理解这些核心概念的实际应用。
</p>
</div>
<!--
...... section 目录......
-->
<h4 id="autosec-4">目录</h4>
<a id="document-autopage-4"></a>




<nav class="toc">

</nav>
<!--
...... section 引言......
-->
<h4 id="autosec-5"><span class="sectionnumber">1&#x2003;</span>引言</h4>
<a id="document-autopage-5"></a>
<!--
...... subsection 学习目标与核心概念......
-->
<h5 id="autosec-6"><span class="sectionnumber">1.1&#x2003;</span>学习目标与核心概念</h5>
<a id="document-autopage-6"></a>



<p>
在深入探讨计算图、反向传播和梯度下降之前，让我们明确本章节的学习目标：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b> 学习目标 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b> 理解计算图：</b> 掌握如何将复杂的数学表达式表示为图结构
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 掌握反向传播：</b> 理解基于链式法则的梯度计算机制
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 应用梯度下降：</b> 学会使用梯度信息优化模型参数
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 实践自动微分：</b> 能够在 PyTorch 中实现和调试这些概念
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 解决实际问题：</b> 通过 MNIST 实例理解这些技术的实际应用
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 机器学习的基本问题......
-->
<h5 id="autosec-7"><span class="sectionnumber">1.2&#x2003;</span>机器学习的基本问题</h5>
<a id="document-autopage-7"></a>



<p>
机器学习的目标是让计算机从数据中学习模式，而不需要明确编程每一个规则。这种学习过程的核心在于如何自动调整模型的参数，使其能够更好地拟合数据并做出准确的预测。
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b> 机器学习的核心挑战 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b> 模型复杂度：</b> 现代机器学习模型（特别是深度神经网络）包含数百万甚至数十亿个参数
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 优化难度：</b> 在高维参数空间中寻找最优解是一个极具挑战性的问题
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 计算效率：</b> 需要高效的算法来处理大规模数据和复杂模型
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 泛化能力：</b> 确保模型在未见过的数据上表现良好
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 为什么需要计算图、反向传播和梯度下降？......
-->
<h5 id="autosec-8"><span class="sectionnumber">1.3&#x2003;</span>为什么需要计算图、反向传播和梯度下降？</h5>
<a id="document-autopage-8"></a>



<p>
这三个概念构成了现代深度学习的基础设施：
</p>

<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker">1.</span> <b> 计算图：</b> 将复杂的数学表达式可视化为图结构，便于理解和计算
</p>


</li>
<li>


<p>
<span class="listmarker">2.</span> <b> 反向传播：</b> 高效计算梯度（偏导数）的算法，告诉我们每个参数对最终输出的影响程度
</p>


</li>
<li>


<p>
<span class="listmarker">3.</span> <b> 梯度下降：</b> 利用梯度信息优化参数的迭代算法，逐步减小损失函数
</p>
</li>
</ul>



<div
      class="tcolorbox"
      style=" border: 1px solid #BF0040; background: #FFFFFF; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #BF0040; border-bottom: 1px solid #BF0040; background: #BF0040; color: #FFFFFF; "
>

<p>
<b> 历史背景 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
这些概念的发展历程可以追溯到 20 世纪 60‑80 年代：
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> 1960s：链式法则在神经网络中的应用首次被提出
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 1970s：反向传播算法的雏形出现
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 1986 年：Rumelhart、Hinton 和 Williams 重新发现并推广了反向传播算法
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 1990s 至今：这些技术成为所有深度学习框架的核心
</p>
</li>
</ul>

</div>

</div>
<!--
...... section 计算图基础......
-->
<h4 id="autosec-9"><span class="sectionnumber">2&#x2003;</span>计算图基础</h4>
<a id="document-autopage-9"></a>
<!--
...... subsection 什么是计算图？......
-->
<h5 id="autosec-10"><span class="sectionnumber">2.1&#x2003;</span>什么是计算图？</h5>
<a id="document-autopage-10"></a>



<p>
计算图（Computational Graph）是一种将数学表达式表示为有向图的数据结构。图中的节点代表变量或操作，边代表数据流动的方向。这种表示方法不仅直观，而且为自动微分提供了数学基础。
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b> 计算图的正式定义 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
计算图 \(G = (V, E)\) 是一个有向图，其中：
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> \(V\) 是节点集合，包括：
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">–</span> <b> 输入节点：</b> 表示变量或常数
</p>


</li>
<li>


<p>
<span class="listmarker">–</span> <b> 操作节点：</b> 表示数学运算（加法、乘法、函数等）
</p>


</li>
<li>


<p>
<span class="listmarker">–</span> <b> 输出节点：</b> 表示最终计算结果
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker">‧</span> \(E\) 是边集合，表示数据依赖关系
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 每个节点 \(v \in V\) 都有一个对应的值 \(val(v)\)
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 边 \((u, v) \in E\) 表示节点 \(u\) 的值是节点 \(v\) 的输入
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 计算图的基本元素......
-->
<h5 id="autosec-11"><span class="sectionnumber">2.2&#x2003;</span>计算图的基本元素</h5>
<a id="document-autopage-11"></a>



<figure id="autoid-1" class="figure ">
<div class="center">

<p>
<span
      id="lateximage-document-1"
      class="lateximagesource"
><!--
x       y       y




    +       ×       f (x, y)
--><img
   src="./Assets//image-1.svg"
   alt="(-tikz-&nbsp;diagram)"
   style=""
   class="lateximage"
></span>
</p>



<div class="figurecaption">


图&nbsp;1: 计算图示例：\(f(x,y) = (x+y) \times y\)

</div>

</div>

</figure>
<!--
...... subsubsection 节点类型......
-->
<h6 id="autosec-15"><span class="sectionnumber">2.2.1&#x2003;</span>节点类型</h6>
<a id="document-autopage-15"></a>




<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b> 输入节点（Input Nodes）</b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> 表示模型的输入变量或常数
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 通常是计算图的起点
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 在训练过程中，这些节点的值会被赋予具体数值
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 示例：\(x\), \(y\), \(w\), \(b\) 等
</p>
</li>
</ul>

</div>

</div>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b> 操作节点（Operation Nodes）</b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> 表示数学运算或函数
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 接收一个或多个输入，产生一个输出
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 示例：加法 (+)、乘法 (\(\times \))、sigmoid(\(\sigma \))、ReLU 等
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 每个操作节点都对应一个前向计算和反向传播规则
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsubsection 边的语义......
-->
<h6 id="autosec-16"><span class="sectionnumber">2.2.2&#x2003;</span>边的语义</h6>
<a id="document-autopage-16"></a>



<p>
边在计算图中承载着重要的语义信息：
</p>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b> 数据流：</b> 表示数值从源节点流向目标节点
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 依赖关系：</b> 显示计算过程中的依赖关系
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 梯度传播：</b> 在反向传播中，梯度沿着边反向流动
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 计算顺序：</b> 边的方向决定了计算的拓扑顺序
</p>
</li>
</ul>
<!--
...... subsection 计算图的构建规则......
-->
<h5 id="autosec-17"><span class="sectionnumber">2.3&#x2003;</span>计算图的构建规则</h5>
<a id="document-autopage-17"></a>



<p>
构建计算图需要遵循特定的规则，以确保计算的正确性和效率：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b> 构建原则 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker">1.</span> <b> 无环性：</b> 计算图必须是有向无环图（DAG），避免循环依赖
</p>


</li>
<li>


<p>
<span class="listmarker">2.</span> <b> 完整性：</b> 每个操作节点的所有输入都必须有明确的来源
</p>


</li>
<li>


<p>
<span class="listmarker">3.</span> <b> 一致性：</b> 数据类型和维度必须匹配
</p>


</li>
<li>


<p>
<span class="listmarker">4.</span> <b> 可微分性：</b> 所有操作节点必须支持前向计算和反向传播
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 复杂模型的表示......
-->
<h5 id="autosec-18"><span class="sectionnumber">2.4&#x2003;</span>复杂模型的表示</h5>
<a id="document-autopage-18"></a>



<p>
现代深度学习模型可以表示为极其复杂的计算图。以逻辑回归为例：
</p>

<figure id="autoid-2" class="figure ">
<div class="center">

<p>
<span
      id="lateximage-document-2"
      class="lateximagesource"
><!--
x1

x2   ×
         +   σ   ŷ
w1   ×

w2

b
--><img
      src="./Assets//image-2.svg"
      alt="(-tikz-&nbsp;diagram)"
      style=""
      class="lateximage"
></span>
</p>



<div class="figurecaption">


图&nbsp;2: 逻辑回归模型的计算图表示：\(\hat {y} = \sigma (w_1x_1 + w_2x_2 + b)\)

</div>

</div>

</figure>

<p>
这个计算图清晰地展示了逻辑回归模型的完整计算过程：
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> 输入特征 \(x_1\), \(x_2\) 与权重 \(w_1\), \(w_2\) 相乘
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 加权和加上偏置项 \(b\)
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 通过 sigmoid 函数 \(\sigma \) 得到概率预测 \(\hat {y}\)
</p>
</li>
</ul>
<!--
...... section 前向传播......
-->
<h4 id="autosec-22"><span class="sectionnumber">3&#x2003;</span>前向传播</h4>
<a id="document-autopage-22"></a>
<!--
...... subsection 前向传播的基本概念......
-->
<h5 id="autosec-23"><span class="sectionnumber">3.1&#x2003;</span>前向传播的基本概念</h5>
<a id="document-autopage-23"></a>



<p>
前向传播（Forward Propagation）是指沿着计算图从输入节点到输出节点的计算过程。在这个过程中，每个节点的值都基于其输入节点的值计算得出。
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b> 前向传播的数学定义 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
对于计算图 \(G = (V, E)\)，前向传播算法可以形式化描述为：
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker">1.</span> 对每个输入节点 \(v \in V_{input}\)，设置 \(val(v)\) 为给定的输入值
</p>


</li>
<li>


<p>
<span class="listmarker">2.</span> 按照拓扑顺序遍历所有节点 \(v \in V\)
</p>


</li>
<li>


<p>
<span class="listmarker">3.</span> 对于每个操作节点 \(v\)，计算：
</p>
<p>
\[val(v) = f_v(val(u_1), val(u_2), \dots , val(u_k))\]
</p>
<p>
其中 \(u_1, u_2, \dots , u_k\) 是 \(v\) 的所有前驱节点，\(f_v\) 是节点 \(v\) 对应的操作函数
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 拓扑排序的重要性......
-->
<h5 id="autosec-24"><span class="sectionnumber">3.2&#x2003;</span>拓扑排序的重要性</h5>
<a id="document-autopage-24"></a>



<p>
拓扑排序确保在计算一个节点的值之前，其所有输入节点的值都已经计算完毕。这是前向传播正确性的关键保证。
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #008080; background: #FFFFFF; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #008080; border-bottom: 1px solid #008080; background: #008080; color: #FFFFFF; "
>

<p>
<b> 拓扑排序示例 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
考虑计算图：\(c = a + b\), \(e = c \times d\)
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> 有效拓扑顺序：\([a, b, d, c, e]\) 或 \([b, a, d, c, e]\)
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 无效顺序：\([c, a, b, d, e]\)（计算 \(c\) 时 \(a\) 和 \(b\) 还未计算）
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 前向传播的详细步骤......
-->
<h5 id="autosec-25"><span class="sectionnumber">3.3&#x2003;</span>前向传播的详细步骤</h5>
<a id="document-autopage-25"></a>



<p>
让我们通过具体例子详细分析前向传播的过程：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #008080; background: #FFFFFF; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #008080; border-bottom: 1px solid #008080; background: #008080; color: #FFFFFF; "
>

<p>
<b> 详细计算 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
假设 \(x=3\), \(y=2\)：
</p>
<div class="multicols">

<p>
<b> 计算步骤：</b>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker">1.</span> 初始化输入节点：\(x=3\), \(y=2\)
</p>


</li>
<li>


<p>
<span class="listmarker">2.</span> 计算 \(u = x + y = 3 + 2 = 5\)
</p>


</li>
<li>


<p>
<span class="listmarker">3.</span> 计算 \(f = u \times y = 5 \times 2 = 10\)
</p>


</li>
<li>


<p>
<span class="listmarker">4.</span> 得到最终结果：\(f(3,2) = 10\)
</p>
</li>
</ul>
<div class="center">

<p>
<span
      id="lateximage-document-3"
      class="lateximagesource"
><!--
x=3         y=2              y=2

  3         2                2

                5
      u=5           f = 10

  f (x, y) = (x + y) × y
--><img
   src="./Assets//image-3.svg"
   alt="(-tikz-&nbsp;diagram)"
   style=""
   class="lateximage"
></span>
</p>
</div>

</div>

</div>

</div>
<!--
...... subsection 前向传播的计算复杂度......
-->
<h5 id="autosec-30"><span class="sectionnumber">3.4&#x2003;</span>前向传播的计算复杂度</h5>
<a id="document-autopage-30"></a>



<p>
前向传播的计算复杂度取决于计算图的结构和操作类型：
</p>

<figure id="autoid-3" class="table ">
<div class="center">
<table>

<tr style="display:none"><th>.</th></tr>


<tr class="tbrule">
<td class="tdl"><b> 操作类型 </b></td>
<td class="tdl"><b> 时间复杂度 </b></td>
<td class="tdl"><b> 空间复杂度 </b></td>
</tr>


<tr class="hline">
<td class="tdl"> 基本算术运算 </td>
<td class="tdl">\(O(1)\)</td>
<td class="tdl">\(O(1)\)</td>
</tr>


<tr>
<td class="tdl"> 矩阵乘法 </td>
<td class="tdl">\(O(n^3)\)</td>
<td class="tdl">\(O(n^2)\)</td>
</tr>


<tr>
<td class="tdl"> 卷积操作 </td>
<td class="tdl">\(O(k \cdot n^2)\)</td>
<td class="tdl">\(O(n^2)\)</td>
</tr>


<tr>
<td class="tdl"> 激活函数 </td>
<td class="tdl">\(O(n)\)</td>
<td class="tdl">\(O(n)\)</td>
</tr>


<tr class="tbrule" aria-hidden="true">
<td class="tdl"></td>
<td class="tdl"></td>
<td class="tdl"></td>
</tr>
</table>



<div class="figurecaption">


表&nbsp;1: 常见操作的计算复杂度

</div>

</div>

</figure>

<p>
其中 \(n\) 表示输入尺寸，\(k\) 表示卷积核大小。
</p>
<!--
...... section 反向传播......
-->
<h4 id="autosec-33"><span class="sectionnumber">4&#x2003;</span>反向传播</h4>
<a id="document-autopage-33"></a>
<!--
...... subsection 为什么需要反向传播？......
-->
<h5 id="autosec-34"><span class="sectionnumber">4.1&#x2003;</span>为什么需要反向传播？</h5>
<a id="document-autopage-34"></a>



<p>
在机器学习中，我们不仅需要计算模型的输出，更重要的是要知道如何调整参数来改进模型性能。反向传播（Backpropagation）算法解决了这个核心问题。
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b> 梯度计算的重要性 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b> 参数优化：</b> 梯度告诉我们每个参数应该向哪个方向调整
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 影响分析：</b> 了解每个参数对最终输出的贡献程度
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 训练效率：</b> 高效的梯度计算大大加速了模型训练
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 理论保证：</b> 为各种优化算法提供数学基础
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 链式法则：反向传播的数学基础......
-->
<h5 id="autosec-35"><span class="sectionnumber">4.2&#x2003;</span>链式法则：反向传播的数学基础</h5>
<a id="document-autopage-35"></a>



<p>
链式法则是多元微积分中的基本定理，也是反向传播算法的核心：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b> 链式法则（Chain Rule）</b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
如果 \(z = f(y)\) 且 \(y = g(x)\)，那么：
</p>

<p>
\[\frac {dz}{dx} = \frac {dz}{dy} \times \frac {dy}{dx}\]
</p>

<p>
对于多元函数，如果 \(z = f(y_1, y_2, \dots , y_n)\) 且每个 \(y_i = g_i(x)\)，那么：
</p>

<p>
\[\frac {\partial z}{\partial x} = \sum _{i=1}^n \frac {\partial z}{\partial y_i} \times \frac {\partial y_i}{\partial x}\]
</p>

</div>

</div>
<!--
...... subsection 反向传播的直观理解......
-->
<h5 id="autosec-36"><span class="sectionnumber">4.3&#x2003;</span>反向传播的直观理解</h5>
<a id="document-autopage-36"></a>



<p>
反向传播可以理解为” 责任分配” 的过程：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #008080; background: #FFFFFF; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #008080; border-bottom: 1px solid #008080; background: #008080; color: #FFFFFF; "
>

<p>
<b> 责任分配类比 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
想象一个公司组织：
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b> 前向传播：</b> 信息从基层员工流向总经理
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 反向传播：</b> 当公司业绩不佳时，责任从总经理反向分配到各个部门和个人
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 梯度：</b> 每个部门或个人对最终结果的” 责任程度”
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 参数更新：</b> 根据责任程度调整工作方式
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 反向传播的直观理解......
-->
<h5 id="autosec-37"><span class="sectionnumber">4.4&#x2003;</span>反向传播的直观理解</h5>
<a id="document-autopage-37"></a>



<p>
反向传播可以理解为” 责任分配” 的过程：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #008080; background: #FFFFFF; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #008080; border-bottom: 1px solid #008080; background: #008080; color: #FFFFFF; "
>

<p>
<b> 责任分配类比 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
想象一个公司组织：
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b> 前向传播：</b> 信息从基层员工流向总经理
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 反向传播：</b> 当公司业绩不佳时，责任从总经理反向分配到各个部门和个人
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 梯度：</b> 每个部门或个人对最终结果的” 责任程度”
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 参数更新：</b> 根据责任程度调整工作方式
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 反向传播的算法步骤......
-->
<h5 id="autosec-38"><span class="sectionnumber">4.5&#x2003;</span>反向传播的算法步骤</h5>
<a id="document-autopage-38"></a>



<p>
反向传播算法可以形式化地描述为以下步骤：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b> 反向传播算法 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker">1.</span> <b> 前向传播：</b> 计算所有节点的值并保存中间结果
</p>


</li>
<li>


<p>
<span class="listmarker">2.</span> <b> 初始化梯度：</b> 设置输出节点的梯度为 \(\frac {\partial L}{\partial L} = 1\)
</p>


</li>
<li>


<p>
<span class="listmarker">3.</span> <b> 反向遍历：</b> 按照逆拓扑顺序遍历所有节点
</p>


</li>
<li>


<p>
<span class="listmarker">4.</span> <b> 梯度计算：</b> 对于每个节点 \(v\)，计算其对损失函数的梯度：
</p>
<p>
\[\frac {\partial L}{\partial v} = \sum _{u \in \text {后继}(v)} \frac {\partial L}{\partial u} \times \frac {\partial u}{\partial v}\]
</p>
</li>
<li>


<p>
<span class="listmarker">5.</span> <b> 参数梯度：</b> 对于参数节点，计算 \(\frac {\partial L}{\partial \theta }\) 用于后续优化
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 详细计算示例......
-->
<h5 id="autosec-39"><span class="sectionnumber">4.6&#x2003;</span>详细计算示例</h5>
<a id="document-autopage-39"></a>



<p>
让我们通过具体例子详细分析反向传播的过程：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #008080; background: #FFFFFF; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #008080; border-bottom: 1px solid #008080; background: #008080; color: #FFFFFF; "
>

<p>
<b> 反向传播计算 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
假设 \(x=3\), \(y=2\)，我们要计算 \(\frac {\partial f}{\partial x}\) 和 \(\frac {\partial f}{\partial y}\)：
</p>

<p>
<b> 前向传播结果：</b>
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> \(u = x + y = 3 + 2 = 5\)
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> \(f = u \times y = 5 \times 2 = 10\)
</p>
</li>
</ul>

<p>
<b> 反向传播计算：</b>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker">1.</span> 初始化：\(\frac {\partial f}{\partial f} = 1\)
</p>


</li>
<li>


<p>
<span class="listmarker">2.</span> 计算 \(\frac {\partial f}{\partial u} = \frac {\partial }{\partial u}(u \times y) = y = 2\)
</p>


</li>
<li>


<p>
<span class="listmarker">3.</span> 计算 \(\frac {\partial f}{\partial y} = \frac {\partial }{\partial y}(u \times y) + \frac {\partial f}{\partial u} \times \frac {\partial u}{\partial y} = u + 2 \times 1
= 5 + 2 = 7\)
</p>


</li>
<li>


<p>
<span class="listmarker">4.</span> 计算 \(\frac {\partial f}{\partial x} = \frac {\partial f}{\partial u} \times \frac {\partial u}{\partial x} = 2 \times 1 = 2\)
</p>
</li>
</ul>

<p>
<b> 结果：</b> \(\frac {\partial f}{\partial x} = 2\), \(\frac {\partial f}{\partial y} = 7\)
</p>
</div>

</div>
<!--
...... subsection 计算图可视化：前向传播与反向传播对比......
-->
<h5 id="autosec-40"><span class="sectionnumber">4.7&#x2003;</span>计算图可视化：前向传播与反向传播对比</h5>
<a id="document-autopage-40"></a>



<figure id="autoid-4" class="figure ">
<div class="center">

<p>
<span
      id="lateximage-document-4"
      class="lateximagesource"
><!--
       前向传播值


x=3     y1 = 2      y2 = 2



      u=5     f = 10

 u = x + y1 f = u × y 2
--><img
      src="./Assets//image-4.svg"
      alt="(-tikz-&nbsp;diagram)"
      style=""
      class="lateximage"
></span>
</p>

<p>
<span
      id="lateximage-document-5"
      class="lateximagesource"
><!--
         反向传播梯度



 ∂f              ∂f               ∂f
 ∂x
    =2           ∂y1
                     =2           ∂y2
                                      =5


    ×1




                              ×5
             ×1
         ∂f               ∂f
            =2               =1
         ∂u
                  ×2      ∂f




            ∂x = ∂u · ∂x
反向传播应用链式法则: ∂f   ∂f ∂u
--><img
   src="./Assets//image-5.svg"
   alt="(-tikz-&nbsp;diagram)"
   style=""
   class="lateximage"
></span>
</p>



<div class="figurecaption">


图&nbsp;3: 前向传播与反向传播对比示意图

</div>

</div>

</figure>
<!--
...... subsection 常见操作的梯度计算规则......
-->
<h5 id="autosec-46"><span class="sectionnumber">4.8&#x2003;</span>常见操作的梯度计算规则</h5>
<a id="document-autopage-46"></a>



<p>
在反向传播中，我们需要为每种操作定义梯度计算规则：
</p>

<figure id="autoid-5" class="table ">
<div class="center">
<table>

<tr style="display:none"><th>.</th></tr>


<tr class="tbrule">
<td class="tdl"><b> 操作类型 </b></td>
<td class="tdl"><b> 前向计算 </b></td>
<td class="tdl"><b> 反向梯度 </b></td>
</tr>


<tr class="hline">
<td class="tdl"> 加法 </td>
<td class="tdl">\(z = x + y\)</td>
<td class="tdl">\(\frac {\partial L}{\partial x} = \frac {\partial L}{\partial z}\), \(\frac {\partial L}{\partial y} = \frac {\partial L}{\partial z}\)</td>
</tr>


<tr>
<td class="tdl"> 乘法 </td>
<td class="tdl">\(z = x \times y\)</td>
<td class="tdl">\(\frac {\partial L}{\partial x} = \frac {\partial L}{\partial z} \times y\), \(\frac {\partial L}{\partial y} = \frac {\partial L}{\partial z} \times x\)</td>
</tr>


<tr>
<td class="tdl"> 矩阵乘法 </td>
<td class="tdl">\(Z = X \cdot Y\)</td>
<td class="tdl">\(\frac {\partial L}{\partial X} = \frac {\partial L}{\partial Z} \cdot Y^T\), \(\frac {\partial L}{\partial Y} = X^T \cdot \frac {\partial L}{\partial Z}\)</td>
</tr>


<tr>
<td class="tdl">ReLU</td>
<td class="tdl">\(z = \max (0, x)\)</td>
<td class="tdl">\(\frac {\partial L}{\partial x} = \begin {cases} \frac {\partial L}{\partial z} &amp; \text {if } x &gt; 0 \\ 0 &amp; \text {otherwise} \end {cases}\)</td>
</tr>


<tr>
<td class="tdl">Sigmoid</td>
<td class="tdl">\(z = \sigma (x)\)</td>
<td class="tdl">\(\frac {\partial L}{\partial x} = \frac {\partial L}{\partial z} \times z(1-z)\)</td>
</tr>


<tr class="tbrule" aria-hidden="true">
<td class="tdl"></td>
<td class="tdl"></td>
<td class="tdl"></td>
</tr>
</table>



<div class="figurecaption">


表&nbsp;2: 常见操作的梯度计算规则

</div>

</div>

</figure>
<!--
...... section 梯度下降......
-->
<h4 id="autosec-49"><span class="sectionnumber">5&#x2003;</span>梯度下降</h4>
<a id="document-autopage-49"></a>
<!--
...... subsection 优化问题的数学表述......
-->
<h5 id="autosec-50"><span class="sectionnumber">5.1&#x2003;</span>优化问题的数学表述</h5>
<a id="document-autopage-50"></a>



<p>
在机器学习中，训练模型可以形式化为一个优化问题：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b> 机器学习优化问题 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
给定训练数据集 \(\mathcal {D} = \{(x_i, y_i)\}_{i=1}^N\)，模型参数 \(\theta \)，损失函数 \(\mathcal {L}\)，我们的目标是：
</p>

<p>
\[\min _{\theta } \mathcal {L}(\theta ) = \frac {1}{N} \sum _{i=1}^N \ell (f(x_i; \theta ), y_i)\]
</p>

<p>
其中 \(f(x; \theta )\) 是模型预测，\(\ell \) 是单个样本的损失函数。
</p>
</div>

</div>
<!--
...... subsection 梯度下降的基本思想......
-->
<h5 id="autosec-51"><span class="sectionnumber">5.2&#x2003;</span>梯度下降的基本思想</h5>
<a id="document-autopage-51"></a>



<p>
梯度下降（Gradient Descent）是最基本的优化算法，其核心思想是利用梯度信息来寻找函数的最小值。
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b> 梯度下降的直观理解 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
想象你在山上，想要下到山谷：
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b> 当前位置：</b> 当前参数值 \(\theta \)
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 梯度：</b> 最陡的下山方向 \(-\nabla \mathcal {L}(\theta )\)
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 学习率：</b> 步长大小 \(\alpha \)
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 更新：</b> 沿着最陡方向走一小步 \(\theta \leftarrow \theta - \alpha \nabla \mathcal {L}(\theta )\)
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 梯度下降的数学表达......
-->
<h5 id="autosec-52"><span class="sectionnumber">5.3&#x2003;</span>梯度下降的数学表达</h5>
<a id="document-autopage-52"></a>




<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b> 梯度下降算法 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
对于参数 \(\theta \)，梯度下降的更新规则为：
</p>

<p>
\[\theta _{t+1} = \theta _t - \alpha \nabla \mathcal {L}(\theta _t)\]
</p>

<p>
其中：
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> \(\theta _t\)：第 \(t\) 次迭代的参数值
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> \(\alpha \)：学习率（步长）
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> \(\nabla \mathcal {L}(\theta _t)\)：损失函数在 \(\theta _t\) 处的梯度
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 学习率的重要性......
-->
<h5 id="autosec-53"><span class="sectionnumber">5.4&#x2003;</span>学习率的重要性</h5>
<a id="document-autopage-53"></a>



<p>
学习率是梯度下降中最重要的超参数之一，它控制着参数更新的步长：
</p>
<div class="multicols">



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b> 学习率太小 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> 收敛速度很慢
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 需要很多迭代
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 可能陷入局部最优
</p>
</li>
</ul>
<div class="center">

<p>
<span
      id="lateximage-document-6"
      class="lateximagesource"
><!--
小步移动
--><img
      src="./Assets//image-6.svg"
      alt="(-tikz-&nbsp;diagram)"
      style=""
      class="lateximage"
></span>
</p>
</div>

</div>

</div>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b> 学习率太大 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> 可能错过最优解
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 在最优解附近震荡
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 甚至发散（不收敛）
</p>
</li>
</ul>
<div class="center">

<p>
<span
      id="lateximage-document-7"
      class="lateximagesource"
><!--
来回震荡
--><img
      src="./Assets//image-7.svg"
      alt="(-tikz-&nbsp;diagram)"
      style=""
      class="lateximage"
></span>
</p>
</div>

</div>

</div>

</div>



<div
      class="tcolorbox"
      style=" border: 1px solid #BF0040; background: #FFFFFF; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #BF0040; border-bottom: 1px solid #BF0040; background: #BF0040; color: #FFFFFF; "
>

<p>
<b> 选择合适的学习率 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> 通常从 0.01, 0.001 等值开始尝试
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 可以随着训练逐渐减小（学习率衰减）
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 使用自适应方法（如 Adam, RMSprop）
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 通过验证集性能来选择最佳学习率
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 梯度下降的变体......
-->
<h5 id="autosec-61"><span class="sectionnumber">5.5&#x2003;</span>梯度下降的变体</h5>
<a id="document-autopage-61"></a>



<p>
根据使用数据量的不同，梯度下降有多种变体：
</p>

<figure id="autoid-6" class="table ">
<div class="center">
<table>

<tr style="display:none"><th>.</th></tr>


<tr class="tbrule">
<td class="tdl"><b> 算法类型 </b></td>
<td class="tdl"><b> 更新规则 </b></td>
<td class="tdl"><b> 特点 </b></td>
</tr>


<tr class="hline">
<td class="tdl"> 批量梯度下降 </td>
<td class="tdl">\(\theta \leftarrow \theta - \alpha \nabla \mathcal {L}(\theta )\)</td>
<td class="tdl"> 使用全部数据，稳定但慢 </td>
</tr>


<tr>
<td class="tdl"> 随机梯度下降 </td>
<td class="tdl">\(\theta \leftarrow \theta - \alpha \nabla \ell _i(\theta )\)</td>
<td class="tdl"> 使用单个样本，快但不稳定 </td>
</tr>


<tr>
<td class="tdl"> 小批量梯度下降 </td>
<td class="tdl">\(\theta \leftarrow \theta - \alpha \nabla \mathcal {L}_B(\theta )\)</td>
<td class="tdl"> 使用小批量，平衡稳定性和速度 </td>
</tr>


<tr class="tbrule" aria-hidden="true">
<td class="tdl"></td>
<td class="tdl"></td>
<td class="tdl"></td>
</tr>
</table>



<div class="figurecaption">


表&nbsp;3: 梯度下降算法变体对比

</div>

</div>

</figure>
<!--
...... subsection 梯度下降的收敛性分析......
-->
<h5 id="autosec-64"><span class="sectionnumber">5.6&#x2003;</span>梯度下降的收敛性分析</h5>
<a id="document-autopage-64"></a>



<p>
梯度下降的收敛性可以通过数学分析来理解：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b> 收敛性条件 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
如果损失函数 \(\mathcal {L}\) 是凸函数且 Lipschitz 连续，即存在 \(L &gt; 0\) 使得：
</p>

<p>
\[\|\nabla \mathcal {L}(\theta ) - \nabla \mathcal {L}(\theta &apos;)\| \leq L\|\theta - \theta &apos;\|\]
</p>

<p>
那么当学习率 \(\alpha &lt; \frac {2}{L}\) 时，梯度下降保证收敛到全局最优解。
</p>
</div>

</div>

<p>
对于非凸函数（如神经网络），梯度下降只能保证收敛到局部最优解或鞍点。
</p>
<!--
...... section 综合实例：训练线性回归模型......
-->
<h4 id="autosec-65"><span class="sectionnumber">6&#x2003;</span>综合实例：训练线性回归模型</h4>
<a id="document-autopage-65"></a>
<!--
...... subsection 问题设定......
-->
<h5 id="autosec-66"><span class="sectionnumber">6.1&#x2003;</span>问题设定</h5>
<a id="document-autopage-66"></a>



<p>
让我们通过一个完整的例子来演示计算图、反向传播和梯度下降的协同工作。我们要训练一个简单的线性回归模型：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b> 线性回归模型 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
模型：\(f(x) = wx + b\) 数据点：\((2, 5)\) 损失函数：\(L = (\hat {y} - y)^2\) 目标：找到最优的 \(w\) 和 \(b\)
</p>
</div>

</div>
<!--
...... subsection 计算图表示......
-->
<h5 id="autosec-67"><span class="sectionnumber">6.2&#x2003;</span>计算图表示</h5>
<a id="document-autopage-67"></a>



<figure id="autoid-7" class="figure ">
<div class="center">

<p>
<span
      id="lateximage-document-8"
      class="lateximagesource"
><!--
x
    ×            预测值
w       +   ŷ         L
b                真实值 y = 5
--><img
   src="./Assets//image-8.svg"
   alt="(-tikz-&nbsp;diagram)"
   style=""
   class="lateximage"
></span>
</p>



<div class="figurecaption">


图&nbsp;4: 线性回归模型的计算图

</div>

</div>

</figure>
<!--
...... subsection 详细训练过程......
-->
<h5 id="autosec-71"><span class="sectionnumber">6.3&#x2003;</span>详细训练过程</h5>
<a id="document-autopage-71"></a>




<div
      class="tcolorbox"
      style=" border: 1px solid #008080; background: #FFFFFF; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #008080; border-bottom: 1px solid #008080; background: #008080; color: #FFFFFF; "
>

<p>
<b> 第 1 轮迭代详细计算 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
<b> 初始化：</b> \(w=1\), \(b=0\), 学习率 \(\alpha =0.1\)
</p>

<p>
<b> 前向传播：</b>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker">1.</span> \(\hat {y} = w \times x + b = 1 \times 2 + 0 = 2\)
</p>


</li>
<li>


<p>
<span class="listmarker">2.</span> \(L = (\hat {y} - y)^2 = (2 - 5)^2 = 9\)
</p>
</li>
</ul>

<p>
<b> 反向传播：</b>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker">1.</span> \(\frac {\partial L}{\partial \hat {y}} = 2(\hat {y} - y) = 2(2-5) = -6\)
</p>


</li>
<li>


<p>
<span class="listmarker">2.</span> \(\frac {\partial \hat {y}}{\partial w} = x = 2\), \(\frac {\partial \hat {y}}{\partial b} = 1\)
</p>


</li>
<li>


<p>
<span class="listmarker">3.</span> \(\frac {\partial L}{\partial w} = \frac {\partial L}{\partial \hat {y}} \times \frac {\partial \hat {y}}{\partial w} = -6 \times 2 = -12\)
</p>


</li>
<li>


<p>
<span class="listmarker">4.</span> \(\frac {\partial L}{\partial b} = \frac {\partial L}{\partial \hat {y}} \times \frac {\partial \hat {y}}{\partial b} = -6 \times 1 = -6\)
</p>
</li>
</ul>

<p>
<b> 梯度下降更新：</b>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker">1.</span> \(w_{\text {新}} = w - \alpha \frac {\partial L}{\partial w} = 1 - 0.1 \times (-12) = 2.2\)
</p>


</li>
<li>


<p>
<span class="listmarker">2.</span> \(b_{\text {新}} = b - \alpha \frac {\partial L}{\partial b} = 0 - 0.1 \times (-6) = 0.6\)
</p>
</li>
</ul>

<p>
<b> 结果：</b> 新的参数 \(w=2.2\), \(b=0.6\)，预测值 \(\hat {y} = 2.2 \times 2 + 0.6 = 5.0\)，更接近真实值 \(y=5\)。
</p>
</div>

</div>
<!--
...... subsection 多轮迭代的收敛过程......
-->
<h5 id="autosec-72"><span class="sectionnumber">6.4&#x2003;</span>多轮迭代的收敛过程</h5>
<a id="document-autopage-72"></a>



<p>
通过多轮迭代，模型会逐步收敛到最优解：
</p>

<figure id="autoid-8" class="table ">
<div class="center">
<table>

<tr style="display:none"><th>.</th></tr>


<tr class="tbrule">
<td class="tdc"><b> 迭代轮数 </b></td>
<td class="tdc"><b>\(w\)</b></td>
<td class="tdc"><b>\(b\)</b></td>
<td class="tdc"><b> 预测值 \(\hat {y}\)</b></td>
<td class="tdc"><b> 损失 \(L\)</b></td>
</tr>


<tr class="hline">
<td class="tdc">0</td>
<td class="tdc">1.000</td>
<td class="tdc">0.000</td>
<td class="tdc">2.000</td>
<td class="tdc">9.000</td>
</tr>


<tr>
<td class="tdc">1</td>
<td class="tdc">2.200</td>
<td class="tdc">0.600</td>
<td class="tdc">5.000</td>
<td class="tdc">0.000</td>
</tr>


<tr>
<td class="tdc">2</td>
<td class="tdc">2.200</td>
<td class="tdc">0.600</td>
<td class="tdc">5.000</td>
<td class="tdc">0.000</td>
</tr>


<tr class="tbrule" aria-hidden="true">
<td class="tdc"></td>
<td class="tdc"></td>
<td class="tdc"></td>
<td class="tdc"></td>
<td class="tdc"></td>
</tr>
</table>



<div class="figurecaption">


表&nbsp;4: 线性回归训练过程

</div>

</div>

</figure>

<p>
可以看到，仅经过一轮迭代，模型就找到了完美拟合数据的参数。
</p>
<!--
...... section 现代深度学习框架的实现......
-->
<h4 id="autosec-75"><span class="sectionnumber">7&#x2003;</span>现代深度学习框架的实现</h4>
<a id="document-autopage-75"></a>
<!--
...... subsection 自动微分系统......
-->
<h5 id="autosec-76"><span class="sectionnumber">7.1&#x2003;</span>自动微分系统</h5>
<a id="document-autopage-76"></a>



<p>
现代深度学习框架（如 PyTorch、TensorFlow、JAX）都内置了自动微分系统，这些系统基于计算图和反向传播原理：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b> 自动微分的关键特性 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b> 动态计算图：</b> 在运行时构建计算图，便于调试和动态控制流
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 静态计算图：</b> 预先构建完整的计算图，优化执行效率
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 梯度追踪：</b> 自动记录前向传播的操作序列
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 内存优化：</b> 智能管理中间结果的存储和释放
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection PyTorch 实现：手动构建计算图......
-->
<h5 id="autosec-77"><span class="sectionnumber">7.2&#x2003;</span>PyTorch 实现：手动构建计算图</h5>
<a id="document-autopage-77"></a>



<p>
让我们通过 PyTorch 来手动实现计算图、反向传播和梯度下降，深入理解这些概念的实际应用：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #008080; background: #FFFFFF; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #008080; border-bottom: 1px solid #008080; background: #008080; color: #FFFFFF; "
>

<p>
<b> 手动实现计算图和反向传播 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>
<div class="figurecaption">

</div>
<p>


</p>
<pre class="programlisting">
1    import torch
2    import torch . nn as nn
3    import matplotlib . pyplot as plt
4
5     class ManualComputationalGraph:
6         ”””手动实现计算图和反向传播”””
7
8         def __init__ ( self ) :
9             # 初始化参数
10              self . w = torch . tensor (1.0,    requires_grad =True)
11              self . b = torch . tensor (0.0,    requires_grad =True)
12              self . learning_rate = 0.1
13
14         def forward_pass ( self , x) :
15              ”””前向传播：计算预测值和损失”””
16              # 线性模型：y_pred = w * x + b
17              y_pred = self . w * x + self . b
18
19              # 均方误差损失：L = ( y_pred − y_true )^2
20              loss = ( y_pred − y_true ) ** 2
21
22              return y_pred , loss
23
24         def backward_pass( self , x , y_true , y_pred ) :
25              ”””反向传播：手动计算梯度”””
26              # 清空之前的梯度
27              if   self . w.grad is not None:
28                    self . w.grad . zero_ ()
29              if   self . b. grad is not None:
30                    self . b. grad . zero_ ()
31
32              # 反向传播计算梯度
33              loss = ( y_pred − y_true ) ** 2
34              loss . backward()
35
36              return self . w.grad . item () , self . b. grad . item ()
37
38         def update_parameters ( self ) :
39              ”””梯度下降：更新参数”””
40              with torch . no_grad () :
41                    self . w −= self . learning_rate    * self . w.grad
42                    self . b −= self . learning_rate    * self . b. grad
43
44    # 训练数据：风格的简单线性回归MNIST
45     x_train = torch . tensor (2.0)       # 输入：图像像素值（简化）
46     y_true = torch . tensor (5.0)       # 输出：对应的数字标签（简化）
47
48    # 创建模型
49    model = ManualComputationalGraph()
50
51    # 训练过程可视化
52     losses = []
53    weights = []
54     biases = []
55
56    print (”=== 手动计算图训练过程 ===”)
57    for epoch in range (10) :
58         # 前向传播
59         y_pred , loss = model.forward_pass ( x_train )
60
61         # 记录训练过程
62         losses . append( loss . item () )
63         weights . append(model.w.item () )
64         biases . append(model.b.item () )
65
66         # 反向传播
67         dw, db = model.backward_pass( x_train , y_true , y_pred )
68
69         # 参数更新
70         model.update_parameters ()
71
72         if epoch &percnt; 2 == 0:
73              print ( f”Epoch {epoch}: Loss = { loss . item () :.4 f }, ”
74                      f”w = {model.w.item () :.3 f }, b = {model.b.item () :.3 f }, ”
75                      f”dw = {dw:.3 f }, db = {db :.3 f }”)
76
77    print ( f”\最终预测：n{y_pred . item () :.3 f，真实值： }{ y_true . item () }”)
</pre>


      <div class="figurecaption">
</div>

</div>

</div>
<!--
...... subsection 现代优化器的工作机制与对比......
-->
<h5 id="autosec-79"><span class="sectionnumber">7.3&#x2003;</span>现代优化器的工作机制与对比</h5>
<a id="document-autopage-79"></a>



<p>
现代深度学习框架提供了多种优化算法，每种算法都有其独特的工作机制。让我们通过流程图和数学表达式来深入理解这些优化器的工作原理。
</p>
<!--
...... subsubsection 优化器的基本框架......
-->
<h6 id="autosec-80"><span class="sectionnumber">7.3.1&#x2003;</span>优化器的基本框架</h6>
<a id="document-autopage-80"></a>



<p>
所有优化器都遵循相似的基本框架，但在梯度处理和参数更新策略上有所不同。
</p>

<figure id="autoid-9" class="figure ">
<div class="center">

<p>
<span
      id="lateximage-document-9"
      class="lateximagesource"
><!--
         开始



     初始化参数 θ0



 计算梯度 gt = ∇θ L(θt )
          否

    优化器处理梯度



更新参数 θt+1 = θt − η · gt




         收敛？


        是

         结束
--><img
   src="./Assets//image-9.svg"
   alt="(-tikz-&nbsp;diagram)"
   style=""
   class="lateximage"
></span>
</p>



<div class="figurecaption">


图&nbsp;5: 优化器通用工作流程

</div>

</div>

</figure>
<!--
...... subsubsection 随机梯度下降（SGD）......
-->
<h6 id="autosec-84"><span class="sectionnumber">7.3.2&#x2003;</span>随机梯度下降（SGD）</h6>
<a id="document-autopage-84"></a>



<p>
SGD 是最基础的优化器，直接使用计算出的梯度进行参数更新。
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b>SGD 数学原理 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b> 梯度计算：</b> \(g_t = \nabla _{\theta } \mathcal {L}(\theta _t)\)
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 参数更新：</b> \(\theta _{t+1} = \theta _t - \eta \cdot g_t\)
</p>
</li>
</ul>

<p>
其中：
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> \(g_t\)：第\(t\) 步的梯度（gradient）
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> \(\theta _t\)：第\(t\) 步的参数（parameters）
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> \(\eta \)：学习率（learning rate）
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> \(\mathcal {L}\)：损失函数（loss function）
</p>
</li>
</ul>

</div>

</div>

<figure id="autoid-10" class="figure ">
<div class="center">

<p>
<span
      id="lateximage-document-10"
      class="lateximagesource"
><!--
     计算梯度 gt


直接更新 θt+1 = θt − ηgt
--><img
      src="./Assets//image-10.svg"
      alt="(-tikz-&nbsp;diagram)"
      style=""
      class="lateximage"
></span>
</p>



<div class="figurecaption">


图&nbsp;6: SGD 优化器工作流程

</div>

</div>

</figure>

<p>
<b>SGD 特点：</b>
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> 简单直接，无额外计算
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 可能震荡，收敛不稳定
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 容易陷入局部最优
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 对学习率敏感
</p>
</li>
</ul>
<!--
...... subsubsection 带动量的 SGD（SGD+Momentum）......
-->
<h6 id="autosec-88"><span class="sectionnumber">7.3.3&#x2003;</span>带动量的 SGD（SGD+Momentum）</h6>
<a id="document-autopage-88"></a>



<p>
动量法通过累积之前的梯度信息来加速收敛并减少震荡。
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b> 动量法数学原理 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b> 动量累积：</b> \(m_t = \beta \cdot m_{t-1} + (1-\beta ) \cdot g_t\)
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 参数更新：</b> \(\theta _{t+1} = \theta _t - \eta \cdot m_t\)
</p>
</li>
</ul>

<p>
其中：
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> \(m_t\)：第\(t\) 步的动量（momentum）
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> \(m_{t-1}\)：上一步的动量
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> \(\beta \)：动量系数，通常设为 0.9
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> \(g_t\)：当前梯度
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> \(\eta \)：学习率
</p>
</li>
</ul>

</div>

</div>

<figure id="autoid-11" class="figure ">
<div class="center">

<p>
<span
      id="lateximage-document-11"
      class="lateximagesource"
><!--
        计算梯度 gt               震荡


更新动量 mt = βmt−1 + (1 − β)gt   动量平滑


   参数更新 θt+1 = θt − ηmt
--><img
   src="./Assets//image-11.svg"
   alt="(-tikz-&nbsp;diagram)"
   style=""
   class="lateximage"
></span>
</p>



<div class="figurecaption">


图&nbsp;7: 动量法工作流程与效果

</div>

</div>

</figure>
<!--
...... subsubsection RMSprop 优化器......
-->
<h6 id="autosec-92"><span class="sectionnumber">7.3.4&#x2003;</span>RMSprop 优化器</h6>
<a id="document-autopage-92"></a>



<p>
RMSprop 通过自适应调整每个参数的学习率来解决学习率选择问题。
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b>RMSprop 数学原理 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b> 平方梯度累积：</b> \(v_t = \beta \cdot v_{t-1} + (1-\beta ) \cdot g_t^2\)
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 自适应学习率：</b> \(\eta _t = \frac {\eta }{\sqrt {v_t + \epsilon }}\)
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 参数更新：</b> \(\theta _{t+1} = \theta _t - \eta _t \cdot g_t\)
</p>
</li>
</ul>

<p>
其中：
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> \(v_t\)：第\(t\) 步的平方梯度累积（second moment）
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> \(v_{t-1}\)：上一步的平方梯度累积
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> \(\beta \)：衰减系数，通常设为 0.999
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> \(g_t^2\)：当前梯度的平方（element‑wise）
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> \(\epsilon \)：小常数，防止除零（通常\(10^{-8}\)）
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> \(\eta _t\)：自适应学习率
</p>
</li>
</ul>

</div>

</div>

<figure id="autoid-12" class="figure ">
<div class="center">

<p>
<span
      id="lateximage-document-12"
      class="lateximagesource"
><!--
       计算梯度 gt


       平方梯度 gt2


累积 vt = βvt−1 + (1 − β)gt2


   自适应率 ηt = √vηt +ϵ


        参数更新
--><img
      src="./Assets//image-12.svg"
      alt="(-tikz-&nbsp;diagram)"
      style=""
      class="lateximage"
></span>
</p>



<div class="figurecaption">


图&nbsp;8: RMSprop 优化器工作流程

</div>

</div>

</figure>

<p>
<b>RMSprop 特点：</b>
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> 为每个参数分配不同学习率
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 对频繁更新的参数减小学习率
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 对稀疏更新的参数保持学习率
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 适合处理非平稳目标
</p>
</li>
</ul>
<!--
...... subsubsection Adam 优化器......
-->
<h6 id="autosec-96"><span class="sectionnumber">7.3.5&#x2003;</span>Adam 优化器</h6>
<a id="document-autopage-96"></a>



<p>
Adam（Adaptive Moment Estimation）结合了一阶动量和二阶动量，是目前最流行的优化器之一。
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b>Adam 数学原理 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b> 一阶动量（均值）：</b> \(m_t = \beta _1 \cdot m_{t-1} + (1-\beta _1) \cdot g_t\)
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 二阶动量（方差）：</b> \(v_t = \beta _2 \cdot v_{t-1} + (1-\beta _2) \cdot g_t^2\)
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 偏差校正：</b> \(\hat {m}_t = \frac {m_t}{1-\beta _1^t}\), \(\hat {v}_t = \frac {v_t}{1-\beta _2^t}\)
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 参数更新：</b> \(\theta _{t+1} = \theta _t - \eta \cdot \frac {\hat {m}_t}{\sqrt {\hat {v}_t} + \epsilon }\)
</p>
</li>
</ul>

<p>
其中：
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> \(m_t\)：一阶动量（梯度均值）
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> \(v_t\)：二阶动量（梯度方差）
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> \(\beta _1\)：一阶衰减系数，通常设为 0.9
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> \(\beta _2\)：二阶衰减系数，通常设为 0.999
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> \(\hat {m}_t\), \(\hat {v}_t\)：偏差校正后的动量
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> \(t\)：当前时间步（用于偏差校正）
</p>
</li>
</ul>

</div>

</div>

<figure id="autoid-13" class="figure ">
<div class="center">

<p>
<span
      id="lateximage-document-13"
      class="lateximagesource"
><!--
                      梯度 gt

mt = β1 mt−1 + (1 − βvt1 )g
                         =t β2 vt−1 + (1 − β2 )gt2

               mt                      vt
        m̂t = 1−β t             v̂t = 1−β t
                  1                       2



                      √m̂t
                       v̂t +ϵ


                      参数更新
--><img
      src="./Assets//image-13.svg"
      alt="(-tikz-&nbsp;diagram)"
      style=""
      class="lateximage"
></span>
</p>



<div class="figurecaption">


图&nbsp;9: Adam 优化器完整工作流程

</div>

</div>

</figure>

<p>
<b>Adam 优势：</b>
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> 结合动量法和自适应学习率的优点
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 偏差校正解决冷启动问题
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 对超参数相对不敏感
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 适合大多数深度学习任务
</p>
</li>
</ul>
<!--
...... subsubsection 优化器对比分析......
-->
<h6 id="autosec-100"><span class="sectionnumber">7.3.6&#x2003;</span>优化器对比分析</h6>
<a id="document-autopage-100"></a>



<p>
不同优化器在收敛速度、稳定性和适用场景方面各有特点。
</p>

<figure id="autoid-14" class="table ">
<div class="center">
<table>

<tr style="display:none"><th>.</th></tr>


<tr class="hline">
<td class="tdp tvertbarl tvertbarr" style="border-left: 1px solid black; border-right: 1px solid black">

<p>
<b> 优化器 </b>
</p>
</td>
<td class="tdp tvertbarr" style="border-right: 1px solid black">

<p>
<b> 核心思想 </b>
</p>
</td>
<td class="tdp tvertbarr" style="border-right: 1px solid black">

<p>
<b> 主要优势 </b>
</p>
</td>
<td class="tdp tvertbarr" style="border-right: 1px solid black">

<p>
<b> 适用场景 </b>
</p>
</td>
</tr>


<tr class="hline">
<td class="tdp tvertbarl tvertbarr" style="border-left: 1px solid black; border-right: 1px solid black">

<p>
SGD
</p>
</td>
<td class="tdp tvertbarr" style="border-right: 1px solid black">

<p>
简单直接
</p>
</td>
<td class="tdp tvertbarr" style="border-right: 1px solid black">

<p>
收敛慢
</p>
</td>
<td class="tdp tvertbarr" style="border-right: 1px solid black">

<p>
易实现
</p>
</td>
</tr>


<tr class="hline">
<td class="tdp tvertbarl tvertbarr" style="border-left: 1px solid black; border-right: 1px solid black">

<p>
SGD+Momentum
</p>
</td>
<td class="tdp tvertbarr" style="border-right: 1px solid black">

<p>
加速收敛
</p>
</td>
<td class="tdp tvertbarr" style="border-right: 1px solid black">

<p>
减少震荡
</p>
</td>
<td class="tdp tvertbarr" style="border-right: 1px solid black">

<p>
通用性好
</p>
</td>
</tr>


<tr class="hline">
<td class="tdp tvertbarl tvertbarr" style="border-left: 1px solid black; border-right: 1px solid black">

<p>
RMSprop
</p>
</td>
<td class="tdp tvertbarr" style="border-right: 1px solid black">

<p>
自适应学习率
</p>
</td>
<td class="tdp tvertbarr" style="border-right: 1px solid black">

<p>
适合稀疏数据
</p>
</td>
<td class="tdp tvertbarr" style="border-right: 1px solid black">

<p>
非平稳目标
</p>
</td>
</tr>


<tr class="hline">
<td class="tdp tvertbarl tvertbarr" style="border-left: 1px solid black; border-right: 1px solid black">

<p>
Adam
</p>
</td>
<td class="tdp tvertbarr" style="border-right: 1px solid black">

<p>
结合动量和自适应
</p>
</td>
<td class="tdp tvertbarr" style="border-right: 1px solid black">

<p>
鲁棒性强
</p>
</td>
<td class="tdp tvertbarr" style="border-right: 1px solid black">

<p>
最常用
</p>
</td>
</tr>


<tr class="hline" aria-hidden="true">
<td class="tdp">

</td>
<td class="tdp">

</td>
<td class="tdp">

</td>
<td class="tdp">

</td>
</tr>
</table>



<div class="figurecaption">


表&nbsp;5: 主流优化器特性对比

</div>

</div>

</figure>
<!--
...... subsubsection 优化器选择指导......
-->
<h6 id="autosec-103"><span class="sectionnumber">7.3.7&#x2003;</span>优化器选择指导</h6>
<a id="document-autopage-103"></a>



<p>
选择合适的优化器需要考虑多个因素：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b> 选择优化器的实用建议 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker">1.</span> <b> 对于简单问题：</b> 使用 SGD 或 SGD+Momentum
</p>


</li>
<li>


<p>
<span class="listmarker">2.</span> <b> 对于深度学习：</b> 优先尝试 Adam
</p>


</li>
<li>


<p>
<span class="listmarker">3.</span> <b> 对于稀疏数据：</b> 考虑 RMSprop 或 Adam
</p>


</li>
<li>


<p>
<span class="listmarker">4.</span> <b> 对于非平稳目标：</b> 使用 RMSprop 或 Adam
</p>


</li>
<li>


<p>
<span class="listmarker">5.</span> <b> 对于资源受限场景：</b> 使用 SGD（计算量最小）
</p>
</li>
</ul>

</div>

</div>



<div
      class="tcolorbox"
      style=" border: 1px solid #BF0040; background: #FFFFFF; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #BF0040; border-bottom: 1px solid #BF0040; background: #BF0040; color: #FFFFFF; "
>

<p>
<b> 重要提醒 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
优化器的选择并非一成不变，应该：
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> 根据具体任务和数据特点进行实验对比
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 考虑计算资源和时间成本
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 注意不同优化器对学习率的敏感性差异
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 可以结合学习率调度策略获得更好效果
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 优化器的抽象......
-->
<h5 id="autosec-104"><span class="sectionnumber">7.4&#x2003;</span>优化器的抽象</h5>
<a id="document-autopage-104"></a>



<p>
现代框架提供了各种优化器来简化梯度下降过程：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #008080; background: #FFFFFF; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #008080; border-bottom: 1px solid #008080; background: #008080; color: #FFFFFF; "
>

<p>
<b> 使用优化器 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>
<div class="figurecaption">

</div>
<p>


</p>
<pre class="programlisting">
1    import torch . optim as optim
2
3    # 定义模型和优化器
4    model = torch . nn. Linear (1, 1)   # 线性回归模型
5     optimizer = optim.SGD(model.parameters () , lr =0.1)
6
7    # 训练循环
8     for epoch in range (100) :
9         # 前向传播
10         y_pred = model(x)
11         loss = ( y_pred − y_true ) ** 2
12
13         # 反向传播
14         optimizer . zero_grad ()   # 清空梯度
15         loss . backward()          # 计算梯度
16         optimizer . step ()        # 更新参数
</pre>


           <div class="figurecaption">
</div>

</div>

</div>
<!--
...... subsection 梯度消失与梯度爆炸......
-->
<h5 id="autosec-106"><span class="sectionnumber">7.5&#x2003;</span>梯度消失与梯度爆炸</h5>
<a id="document-autopage-106"></a>



<p>
在深层网络中，反向传播可能面临梯度消失或梯度爆炸问题：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b> 梯度消失（Vanishing Gradient）</b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b> 原因：</b> 连续的小梯度相乘导致最终梯度趋近于零
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 影响：</b> 深层网络的前面层无法有效学习
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 解决方案：</b> ReLU 激活函数、残差连接、批量归一化
</p>
</li>
</ul>

</div>

</div>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b> 梯度爆炸（Exploding Gradient）</b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b> 原因：</b> 连续的大梯度相乘导致梯度数值溢出
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 影响：</b> 训练不稳定，参数更新过大
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 解决方案：</b> 梯度裁剪、权重初始化、学习率调整
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 二阶优化方法......
-->
<h5 id="autosec-107"><span class="sectionnumber">7.6&#x2003;</span>二阶优化方法</h5>
<a id="document-autopage-107"></a>



<p>
除了梯度下降，还有基于二阶导数的优化方法：
</p>

<figure id="autoid-15" class="table ">
<div class="center">
<table>

<tr style="display:none"><th>.</th></tr>


<tr class="tbrule">
<td class="tdl"><b> 方法 </b></td>
<td class="tdl"><b> 原理 </b></td>
<td class="tdl"><b> 特点 </b></td>
</tr>


<tr class="hline">
<td class="tdl"> 牛顿法 </td>
<td class="tdl"> 使用 Hessian 矩阵 </td>
<td class="tdl"> 收敛快但计算昂贵 </td>
</tr>


<tr>
<td class="tdl"> 拟牛顿法 </td>
<td class="tdl"> 近似 Hessian 矩阵 </td>
<td class="tdl"> 平衡收敛速度和计算成本 </td>
</tr>


<tr>
<td class="tdl"> 共轭梯度法 </td>
<td class="tdl"> 利用共轭方向 </td>
<td class="tdl"> 适合大规模问题 </td>
</tr>


<tr>
<td class="tdl"> 自然梯度 </td>
<td class="tdl"> 考虑参数空间的几何结构 </td>
<td class="tdl"> 在信息几何框架下优化 </td>
</tr>


<tr class="tbrule" aria-hidden="true">
<td class="tdl"></td>
<td class="tdl"></td>
<td class="tdl"></td>
</tr>
</table>



<div class="figurecaption">


表&nbsp;6: 二阶优化方法对比

</div>

</div>

</figure>
<!--
...... subsection 自适应优化算法......
-->
<h5 id="autosec-110"><span class="sectionnumber">7.7&#x2003;</span>自适应优化算法</h5>
<a id="document-autopage-110"></a>



<p>
现代深度学习广泛使用自适应优化算法：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b>Adam 优化器 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
Adam（Adaptive Moment Estimation）结合了动量法和 RMSProp 的优点：
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b> 动量：</b> 累积梯度的一阶矩（均值）
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 自适应学习率：</b> 累积梯度的二阶矩（未中心化的方差）
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 偏差校正：</b> 解决初始阶段的偏差问题
</p>
</li>
</ul>

<p>
更新规则：
</p>
<span class="hidden"> \(\seteqnumber{0}{}{0}\)</span>



<!--




                                                                                             mt = β1 mt−1 + (1 − β1 )gt                                                                                 (1)
                                                                                            vt = β2 vt−1 + (1 − β2 )gt2                                                                                 (2)
                                                                                                    mt
                                                                                           m̂t =                                                                                                        (3)
                                                                                                  1 − β1t
                                                                                                    vt
                                                                                            v̂t =                                                                                                       (4)
                                                                                                  1 − β2t
                                                                                                           m̂t
                                                                                          θt+1 = θt − α √                                                                                               (5)
                                                                                                          v̂t + ϵ



-->



<p>


\begin{align}
m_t &amp;= \beta _1 m_{t-1} + (1 - \beta _1) g_t \\ v_t &amp;= \beta _2 v_{t-1} + (1 - \beta _2) g_t^2 \\ \hat {m}_t &amp;= \frac {m_t}{1 - \beta _1^t} \\ \hat {v}_t &amp;= \frac {v_t}{1 - \beta _2^t} \\
\theta _{t+1} &amp;= \theta _t - \alpha \frac {\hat {m}_t}{\sqrt {\hat {v}_t} + \epsilon }
\end{align}


</p>
</div>

</div>
<!--
...... section 结论与实践指导......
-->
<h4 id="autosec-111"><span class="sectionnumber">8&#x2003;</span>结论与实践指导</h4>
<a id="document-autopage-111"></a>
<!--
...... subsection 核心概念总结与学习成果......
-->
<h5 id="autosec-112"><span class="sectionnumber">8.1&#x2003;</span>核心概念总结与学习成果</h5>
<a id="document-autopage-112"></a>



<p>
通过本章节的学习，我们系统性地掌握了深度学习中的三个核心数学基础：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b> 计算图：数学表达式的可视化表示 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
<b> 核心理解：</b>
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> 将复杂计算分解为基本操作的图结构
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 为自动微分提供数学基础
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 便于理解和调试复杂模型
</p>
</li>
</ul>

<p>
<b> 实践技能：</b>
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> 能够手动构建简单模型的计算图
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 理解 PyTorch 中动态计算图的工作原理
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 掌握计算图的拓扑排序和依赖关系分析
</p>
</li>
</ul>

</div>

</div>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b> 反向传播：高效的梯度计算算法 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
<b> 核心理解：</b>
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> 基于链式法则从输出向输入传播梯度
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 避免了重复计算，大大提高了效率
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 是现代深度学习框架的核心
</p>
</li>
</ul>

<p>
<b> 实践技能：</b>
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> 能够手动推导简单函数的梯度
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 理解反向传播中的梯度累积机制
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 掌握常见操作的梯度计算规则
</p>
</li>
</ul>

</div>

</div>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b> 梯度下降：参数优化的基本方法 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
<b> 核心理解：</b>
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> 利用梯度信息寻找函数最小值
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 学习率控制更新步长，影响收敛性
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 有多种变体和改进算法
</p>
</li>
</ul>

<p>
<b> 实践技能：</b>
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> 能够实现基本的梯度下降算法
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 理解不同优化器的优缺点和适用场景
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 掌握学习率调优的基本方法
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 实践应用指南......
-->
<h5 id="autosec-113"><span class="sectionnumber">8.2&#x2003;</span>实践应用指南</h5>
<a id="document-autopage-113"></a>



<p>
基于我们的学习经验，以下是实际应用中的关键指导原则：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b> 模型开发流程 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker">1.</span> <b> 问题分析：</b> 明确任务类型（分类、回归等）和数据特征
</p>


</li>
<li>


<p>
<span class="listmarker">2.</span> <b> 架构设计：</b> 根据问题复杂度选择合适的网络结构
</p>


</li>
<li>


<p>
<span class="listmarker">3.</span> <b> 计算图构建：</b> 设计前向传播的计算流程
</p>


</li>
<li>


<p>
<span class="listmarker">4.</span> <b> 损失函数选择：</b> 根据任务类型选择合适的损失函数
</p>


</li>
<li>


<p>
<span class="listmarker">5.</span> <b> 优化器配置：</b> 选择适当的优化算法和超参数
</p>


</li>
<li>


<p>
<span class="listmarker">6.</span> <b> 训练监控：</b> 实时监控损失和性能指标
</p>


</li>
<li>


<p>
<span class="listmarker">7.</span> <b> 调试优化：</b> 分析梯度流，调整网络结构和超参数
</p>
</li>
</ul>

</div>

</div>



<div
      class="tcolorbox"
      style=" border: 1px solid #BF0040; background: #FFFFFF; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #BF0040; border-bottom: 1px solid #BF0040; background: #BF0040; color: #FFFFFF; "
>

<p>
<b> 常见陷阱与解决方案 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b> 梯度消失：</b> 使用 ReLU 激活函数、残差连接、适当的权重初始化
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 梯度爆炸：</b> 实施梯度裁剪、减小学习率、使用批量归一化
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 过拟合：</b> 应用 Dropout、L2 正则化、数据增强、早停法
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 欠拟合：</b> 增加模型复杂度、延长训练时间、调整学习率
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 收敛缓慢：</b> 尝试自适应优化器（Adam、RMSprop）、调整批量大小
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 从理论到实践的进阶路径......
-->
<h5 id="autosec-114"><span class="sectionnumber">8.3&#x2003;</span>从理论到实践的进阶路径</h5>
<a id="document-autopage-114"></a>



<p>
掌握这些基础概念后，读者可以沿着以下路径继续深入学习：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b> 进阶学习建议 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker">1.</span> <b> 深入数学理论：</b> 学习更优化理论、凸优化、数值分析
</p>


</li>
<li>


<p>
<span class="listmarker">2.</span> <b> 复杂架构设计：</b> 研究 CNN、RNN、Transformer 等不同架构的计算图特点
</p>


</li>
<li>


<p>
<span class="listmarker">3.</span> <b> 高级优化技术：</b> 探索二阶优化方法、元学习、自适应学习率调度
</p>


</li>
<li>


<p>
<span class="listmarker">4.</span> <b> 分布式训练：</b> 理解数据并行、模型并行的梯度聚合机制
</p>


</li>
<li>


<p>
<span class="listmarker">5.</span> <b> 实际项目应用：</b> 在计算机视觉、自然语言处理等领域实践这些概念
</p>
</li>
</ul>

</div>

</div>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
<b> 技术发展历程与前沿趋势 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
这些概念的发展体现了深度学习领域的演进：
</p>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b>1960s‑1970s：</b> 理论基础建立，反向传播思想萌芽
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b>1980s：</b> 反向传播算法被重新发现和推广
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b>1990s：</b> 计算图概念在自动微分中系统化
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b>2000s：</b> 深度学习框架开始集成这些技术
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b>2010s 至今：</b> 成为所有现代 AI 系统的标准组件
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 当前前沿：</b> 神经架构搜索、自动机器学习、量子机器学习
</p>
</li>
</ul>

</div>

</div>



<div
      class="tcolorbox"
      style=" border: 1px solid #BF0040; background: #FFFFFF; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #BF0040; border-bottom: 1px solid #BF0040; background: #BF0040; color: #FFFFFF; "
>

<p>
<b> 重要启示 </b>
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
计算图、反向传播和梯度下降不仅是技术工具，更是理解深度学习本质的钥匙。掌握这些基础概念，能够帮助我们：
</p>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b> 设计更好的模型：</b> 理解如何构建高效的神经网络架构
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 调试训练问题：</b> 快速定位和解决训练过程中的各种问题
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 推动技术创新：</b> 基于这些原理开发新的算法和技术
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 跨领域应用：</b> 将这些概念应用到不同的机器学习任务中
</p>
</li>
</ul>

<p>
正如我们在 MNIST 实例中看到的，这些看似抽象的数学概念，实际上是解决实际问题的强大工具。
</p>
</div>

</div>
<!--
...... section 参考文献......
-->
<h4 id="autosec-115">参考文献</h4>
<a id="document-autopage-115"></a>



<ul class="list" style="list-style-type:none">


<li>
<p>
<span class="listmarker">[1]&#x2003;</span> Rumelhart, D. E., Hinton, G. E., &amp; Williams, R. J. (1986). Learning representations by back‑propagating errors. <i>Nature</i>, 323(6088), 533‑536.
</p>
</li>
<li>


<p>
<span class="listmarker">[2]&#x2003;</span> LeCun, Y., Bengio, Y., &amp; Hinton, G. (2015). Deep learning. <i>Nature</i>, 521(7553), 436‑444.
</p>
</li>
<li>


<p>
<span class="listmarker">[3]&#x2003;</span> Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). <i>Deep Learning</i>. MIT Press.
</p>
</li>
<li>


<p>
<span class="listmarker">[4]&#x2003;</span> Baydin, A. G., Pearlmutter, B. A., Radul, A. A., &amp; Siskind, J. M. (2018). Automatic differentiation in machine learning: a survey. <i>Journal of Machine Learning
Research</i>, 18(1), 5595‑5637.
</p>
</li>
<li>


<p>
<span class="listmarker">[5]&#x2003;</span> Kingma, D. P., &amp; Ba, J. (2014). Adam: A method for stochastic optimization. <i>arXiv preprint arXiv:1412.6980</i>.
</p>
</li>
<li>


<p>
<span class="listmarker">[6]&#x2003;</span> Bottou, L., Curtis, F. E., &amp; Nocedal, J. (2018). Optimization methods for large‑scale machine learning. <i>SIAM Review</i>, 60(2), 223‑311.
</p>
</li>
<li>


<p>
<span class="listmarker">[7]&#x2003;</span> Pascanu, R., Mikolov, T., &amp; Bengio, Y. (2013). On the difficulty of training recurrent neural networks. In <i>International conference on machine learning</i>
(pp. 1310‑1318).
</p>
</li>
<li>


<p>
<span class="listmarker">[8]&#x2003;</span> Sutskever, I., Martens, J., Dahl, G., &amp; Hinton, G. (2013). On the importance of initialization and momentum in deep learning. In <i>International conference on
machine learning</i> (pp. 1139‑1147).
</p>
<p>


</p>
</li>
</ul>

<a id="document-autofile-last"></a>
</section>

</main>

</div>

</body>
</html>
