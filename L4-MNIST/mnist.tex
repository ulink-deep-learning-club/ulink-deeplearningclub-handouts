\documentclass[hidelinks,12pt,a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage[UTF8]{ctex}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{float}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{xcolor}
\usepackage[
            pdfauthor={Anson, Kimi},
            pdftitle={MNIST数字识别：从全连接网络到卷积神经网络},
            ]{hyperref}
\usepackage{booktabs}
\usepackage{tcolorbox}

\usepackage{fontspec}
\setmainfont{Noto Serif CJK SC}
\setsansfont{Noto Sans CJK SC}

\newenvironment{block}[1]{%
    \begin{tcolorbox}[title=#1]%
}{%
    \end{tcolorbox}%
}

\newenvironment{exampleblock}[1]{%
    \begin{tcolorbox}[colback=white, colframe=teal, title=#1]%
}{%
    \end{tcolorbox}%
}

\newenvironment{alertblock}[1]{%
    \begin{tcolorbox}[colback=white, colframe=purple, title=#1]%
}{%
    \end{tcolorbox}%
}

% 定义颜色
\definecolor{myblue}{RGB}{0, 102, 204}
\definecolor{mygreen}{RGB}{0, 153, 0}
\definecolor{myred}{RGB}{204, 0, 0}
\definecolor{lightgray}{RGB}{240, 240, 240}

% 代码 listings 设置
\lstset{
    backgroundcolor=\color{lightgray},
    basicstyle=\ttfamily\small,
    breaklines=true,
    captionpos=b,
    frame=single,
    numbers=left,
    numberstyle=\tiny,
    keywordstyle=\color{myblue}\bfseries,
    commentstyle=\color{mygreen},
    stringstyle=\color{myred},
    showstringspaces=false
}

\title{\textbf{MNIST数字识别：从全连接网络到卷积神经网络}}
\author{Anson, 深度学习社\quad \small{Cooperated with Kimi}}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
本文深入探讨了MNIST手写数字识别任务中的两种主要神经网络架构：经典全连接神经网络（Fully Connected Networks）和卷积神经网络（Convolutional Neural Networks, CNN）。文章首先系统介绍了神经网络训练的基础概念和术语，包括损失函数、优化算法、过拟合与正则化技术等核心内容。通过详细分析LeNet架构，我们展示了如何将卷积层用于特征提取，以及如何将全连接层用于分类。文章包含完整的数学推导、PyTorch实现代码，并深入对比了两种架构的优缺点。此外，我们还探讨了神经网络中的缩放定律，为理解现代深度学习的发展提供了理论基础。
\end{abstract}

\tableofcontents
\newpage

\section{引言}

\subsection{MNIST数据集简介}

MNIST（Modified National Institute of Standards and Technology）数据集是机器学习领域最经典的数据集之一，由Yann LeCun等人创建。该数据集包含：

\begin{itemize}
    \item 训练集：60,000张手写数字图像
    \item 测试集：10,000张手写数字图像
    \item 图像尺寸：28×28像素，灰度图像
    \item 类别：0-9共10个数字类别
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/mnist.png}
\caption{MNIST图像数据集}
\end{figure}

MNIST数据集之所以成为 “Hello World” 级别的基准测试，是因为：

\begin{enumerate}
    \item \textbf{规模适中}：足够大以展示机器学习的效果，又足够小以便快速实验
    \item \textbf{预处理完善}：图像已经过标准化处理，可直接用于训练
    \item \textbf{评估标准明确}：分类准确率的计算简单直观
    \item \textbf{历史意义}：见证了从传统机器学习到深度学习的发展历程
\end{enumerate}

\subsection{LeNet的历史意义}

LeNet由Yann LeCun在1989年提出~\cite{lecun1989backpropagation}，是最早的卷积神经网络之一。其历史意义在于：

\begin{itemize}
    \item \textbf{开创性}：首次将卷积操作引入神经网络
    \item \textbf{实用性}：成功应用于银行支票的手写数字识别
    \item \textbf{理论基础}：奠定了现代CNN架构的基础
    \item \textbf{持久影响}：其设计思想至今仍在使用
\end{itemize}

\section{神经网络训练基础}

\subsection{基本训练术语}

在深入探讨神经网络架构之前，我们需要明确一些基本的训练概念和术语：

\begin{block}{核心训练术语}
\begin{itemize}
    \item \textbf{Epoch（轮次）}：完整遍历整个训练数据集一次的过程
    \item \textbf{Batch（批次）}：一次训练迭代中使用的样本集合
    \item \textbf{Batch Size（批大小）}：每个批次中的样本数量，影响训练稳定性和内存使用
    \item \textbf{Iteration（迭代）}：完成一个批次的训练步骤
    \item \textbf{Learning Rate（学习率）}：控制模型参数更新步长的超参数
    \item \textbf{Loss Function（损失函数）}：衡量模型预测与真实标签差异的函数
\end{itemize}
\end{block}

\subsection{损失函数的选择}

损失函数是神经网络训练的核心，不同的任务需要不同的损失函数：

\begin{table}[H]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{任务类型} & \textbf{损失函数} & \textbf{数学表达式} \\ \midrule
回归任务 & 均方误差（MSE） & $\displaystyle \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2$ \\
\\
分类任务 & 交叉熵损失 & $\displaystyle -\sum_{i=1}^n y_i \log(\hat{y}_i)$ \\
\\
二分类任务 & 二元交叉熵 & $\displaystyle -\frac{1}{n}\sum_{i=1}^n [y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)]$ \\ \bottomrule
\end{tabular}
\caption{常见损失函数对比}
\end{table}

\begin{exampleblock}{MNIST分类的损失函数}
对于MNIST手写数字识别这样的10类分类任务，我们使用\textbf{分类交叉熵损失函数}：

\begin{equation}
\mathcal{L} = -\sum_{c=1}^{10} y_c \log(p_c)
\end{equation}

其中：
\begin{itemize}
    \item $y_c$：真实标签的one-hot编码（0或1）
    \item $p_c$：模型预测属于类别$c$的概率
    \item 求和遍历所有10个数字类别
\end{itemize}

\textbf{为什么选择交叉熵而不是MSE？} 交叉熵对概率分布的差异更敏感，且在分类任务中梯度更稳定。
\end{exampleblock}

\subsection{训练过程与验证}

\subsubsection{训练集与测试集的划分}

在机器学习中，我们将数据划分为不同的集合：

\begin{itemize}
    \item \textbf{训练集（Training Set）}：用于模型参数的学习
    \item \textbf{验证集（Validation Set）}：用于超参数调优和模型选择
    \item \textbf{测试集（Test Set）}：用于最终模型性能评估
\end{itemize}

\begin{alertblock}{数据划分的重要性}
\begin{itemize}
    \item 避免过拟合：确保模型在未见过的数据上表现良好
    \item 公平评估：测试集只能在最终评估时使用一次
    \item 模型选择：验证集帮助我们选择最佳超参数
\end{itemize}
\end{alertblock}

\subsubsection{训练过程的监控}

有效的训练需要监控多个指标：

\begin{block}{关键监控指标}
\begin{itemize}
    \item \textbf{训练损失}：模型在训练集上的表现，应该逐渐下降
    \item \textbf{验证损失}：模型在验证集上的表现，反映泛化能力
    \item \textbf{训练准确率}：模型在训练集上的分类正确率
    \item \textbf{验证准确率}：模型在验证集上的分类正确率
\end{itemize}
\end{block}

\subsection{过拟合与欠拟合}

\subsubsection{欠拟合（Underfitting）}

当模型过于简单，无法捕捉数据中的基本模式时发生：

\begin{itemize}
    \item 训练损失和验证损失都很高
    \item 模型在训练集上表现不佳
    \item 解决方法：增加模型复杂度、减少正则化、训练更长时间
\end{itemize}

\subsubsection{过拟合（Overfitting）}

想象你在学习一门新课程：

\begin{itemize}
    \item \textbf{正常学习}：理解基本概念，能够解决类似问题
    \item \textbf{死记硬背}：记住所有例题，但遇到新题型就不会
\end{itemize}

当模型过于复杂，记忆了训练数据中的噪声而非真实模式时发生：

\begin{itemize}
    \item 训练损失很低，但验证损失很高
    \item 模型在训练集上表现很好，但在新数据上表现差
    \item 解决方法：增加训练数据、使用正则化、早停法、降低模型复杂度
\end{itemize}

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.8]
    % Underfitting region
    \fill[yellow!20] (0,0) rectangle (2.5,5);
    \node at (1.5,-0.5) {欠拟合};
    
    % Good fit region
    \fill[green!20] (2.5,0) rectangle (5.5,5);
    \node at (4,-0.5) {良好拟合};
    
    % Overfitting region
    \fill[orange!20] (5.5,0) rectangle (7.5,5);
    \node at (6.5,-0.5) {过拟合};

    % Model complexity axis
    \draw[->] (0,0) -- (8,0) node[right] {模型复杂度};
    \draw[->] (0,0) -- (0,5) node[left] {损失};
    
    % Training loss curve
    \draw[thick, blue, domain=0:7.5, smooth] plot ({\x}, {4 - 0.6*\x + 0.04*\x*\x});
    
    % Validation loss curve
    \draw[thick, red, domain=0:7.5, smooth] plot ({\x}, {3 - 0.4*\x + 0.08*\x*\x});
    
    % Labels
    \node[blue] at (2.5,3.4) {训练损失};
    \node[red] at (6.5,2.9) {验证损失};
\end{tikzpicture}
\caption{模型复杂度与损失的关系}
\end{figure}

\begin{block}{有点牵强的总结：}
    学而不思则过拟合，思而不学则欠拟合。
\end{block}

\subsection{正则化技术：防止过拟合的实用方法}

\subsubsection{什么是正则化？}

在神经网络中，正则化就是帮助我们训练出“聪明”而不是“死记硬背”的模型的技术。

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.8]
    % Student learning analogy
    \node at (0,3) {\textbf{学生学习}};
    \draw[->] (-1,2.3) -- (1,2.3);
    \node at (0,1.7) {理解概念};
    \node at (0,1) {举一反三};
    
    \node at (6,3) {\textbf{神经网络训练}};
    \draw[->] (5,2.3) -- (7,2.3);
    \node at (6,1.7) {学习通用特征};
    \node at (6,1) {泛化到新数据};
    
    \draw[->, thick, blue!70] (2,2.5) -- (4,2.5);
    \node[blue!70] at (3,2.8) {正则化};
\end{tikzpicture}
\caption{正则化的直观理解}
\end{figure}

\subsubsection{L1和L2正则化：最简单的正则化方法}

\textbf{核心思想：} 让模型的权重不要变得太大

正则化通过在原始损失函数中添加参数惩罚项来实现：

\begin{block}{L1正则化（Lasso回归）}
\begin{equation}
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{original}} + \lambda \sum_{i=1}^{n} |w_i|
\end{equation}

\textbf{特点：}
\begin{itemize}
    \item 产生稀疏解，许多参数会变为0
    \item 具有特征选择功能，自动选择重要特征
    \item 对异常值相对鲁棒
    \item 不可导，需要特殊优化方法
\end{itemize}
\end{block}


\begin{block}{L2正则化（Ridge回归）}
\begin{equation}
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{original}} + \frac{\lambda}{2} \sum_{i=1}^{n} w_i^2
\end{equation}

\textbf{特点：}
\begin{itemize}
    \item 参数趋向于小而非零的值
    \item 数学处理更简单，可导
    \item 对异常值敏感
    \item 是最常用的正则化形式
\end{itemize}
\end{block}

\begin{exampleblock}{类比}
想象你在调音：
\begin{itemize}
    \item \textbf{L1正则化}：直接告诉某些旋钮“完全不要动”（变成0）
    \item \textbf{L2正则化}：告诉所有旋钮“不要调得太大”（保持小值）
\end{itemize}
\end{exampleblock}

\begin{alertblock}{为什么有效？}
\begin{itemize}
    \item 权重太大 → 模型过于敏感 → 容易记住训练数据
    \item 权重较小 → 模型更加平滑 → 能够泛化到新数据
    \item 就像用粗笔画画，不会画出过于复杂的细节
\end{itemize}
\end{alertblock}

\begin{exampleblock}{PyTorch中的L2正则化}
在PyTorch中，L2正则化被称为“权重衰减”（weight decay）：
\begin{lstlisting}[language=Python]
optimizer = torch.optim.Adam(
    model.parameters(),
    lr=0.001,
    weight_decay=0.0001  # L2正则化
)
\end{lstlisting}
\textbf{实践建议：} 从0.0001开始尝试，根据验证效果调整
\end{exampleblock}

\subsubsection{Dropout：随机“失忆”技术}

\textbf{核心思想：} 训练时随机让一些神经元“罢工”，迫使网络学习冗余的特征

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.8]
    % Input layer
    \foreach \i in {1,2,3,4}
        \node[circle, draw=blue!50, fill=blue!20, minimum size=0.6cm] (in\i) at (0,\i) {};
    
    % Hidden layer with dropout
    \foreach \i in {1,2,3,4,5}
        \node[circle, draw=red!50, minimum size=0.6cm] (hid\i) at (2,\i-0.5) {};
    
    % Some nodes crossed out (dropout)
    \foreach \i in {3,5}
        \draw[thick, red!70] (1.5,\i-1) -- (2.5,\i);
    \foreach \i in {3,5}
        \draw[thick, red!70] (1.5,\i) -- (2.5,\i-1);
    
    % Output layer
    \foreach \i in {1,2,3}
        \node[circle, draw=green!40!black, fill=green!20, minimum size=0.6cm] (out\i) at (4,\i+0.5) {};
    
    % Connections
    \foreach \i in {1,2,3,4}
        \foreach \j in {1,3,5}
            \draw[->, gray!50] (in\i) -- (hid\j);
    
    \foreach \j in {1,3,5}
        \foreach \k in {1,2,3}
            \draw[->, gray!50] (hid\j) -- (out\k);
    
    \node at (2,-0.5) {Dropout：随机让部分神经元“罢工”};
\end{tikzpicture}
\caption{Dropout机制示意图}
\end{figure}

\begin{exampleblock}{直观的理解}
\textbf{训练阶段：} “同学们，今天随机抽一半同学回答问题，其他人休息”
\begin{itemize}
    \item 每个人都要准备好，因为不知道会不会被抽到
    \item 不能依赖某个“学霸”同学，必须自己理解
\end{itemize}

\textbf{测试阶段：} “现在全班一起回答问题，把大家的答案平均一下”
\begin{itemize}
    \item 相当于多个“子班级”的集体智慧
    \item 结果更加稳定可靠
\end{itemize}
\end{exampleblock}

\begin{exampleblock}{Dropout的PyTorch实现}
\begin{lstlisting}[language=Python]
class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(784, 256)
        self.dropout = nn.Dropout(0.5)  # 50%的dropout率
        self.fc2 = nn.Linear(256, 10)
        
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout(x)  # 只在训练时随机“丢弃”
        x = self.fc2(x)
        return x
\end{lstlisting}
\textbf{参数选择：}
\begin{itemize}
    \item 输入层：通常不用dropout（0\%）
    \item 隐藏层：0.3-0.5（30\%-50\%）
    \item 输出层：不用dropout
\end{itemize}
\end{exampleblock}

\subsubsection{早停法：聪明的“刹车”技术}

\textbf{核心思想：} 看到验证效果开始变差时就停止训练

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.9]
    % Axes
    \draw[->] (0,0) -- (8,0) node[right] {训练时间};
    \draw[->] (0,0) -- (0,5) node[left] {准确率};
    
    % Training accuracy curve (improving)
    \draw[thick, blue, domain=0:7, smooth] plot ({\x}, {0.2 + 0.2*\x + 0.03*\x*\x});
    
    % Validation accuracy curve (plateau then drop)
    \draw[thick, red, domain=0:7, smooth] plot ({\x}, {0.3 + 0.4*\x - 0.05*\x*\x});
    
    % Optimal stopping point
    \draw[dashed, green!70!black] (4,0) -- (4,5);
    
    % Labels
    \node[blue] at (1.5,3.5) {训练准确率};
    \node[red] at (5.5,3.5) {验证准确率};
    \node[green!70!black] at (4,-0.3) {最佳停止点};
    
    % Overfitting indication
    \node[text width=3cm, align=center] at (8.5,2) {开始过拟合\\验证效果下降};
\end{tikzpicture}
\caption{早停法示意图}
\end{figure}

\begin{alertblock}{什么时候该停止？}
\begin{itemize}
    \item 验证损失连续几个epoch没有改善
    \item 验证准确率开始下降
    \item 训练损失还在下降，但验证损失开始上升
\end{itemize}
\textbf{就像考试前：} 发现模拟考试成绩开始下降，就应该停止“熬夜突击”，保持当前水平
\end{alertblock}

\subsubsection{数据增强：免费的“新数据”}

\textbf{核心思想：} 通过对现有数据进行合理变换，创造“新”的训练样本

\begin{exampleblock}{MNIST数据增强实例}
原始图像：手写数字“3”
\begin{itemize}
    \item \textbf{轻微旋转：} 顺时针转5度，还是“3”
    \item \textbf{平移：} 向左移动2像素，还是“3”
    \item \textbf{轻微缩放：} 放大到1.1倍，还是“3”
    \item \textbf{加噪声：} 加一点“雪花点”，还是“3”
\end{itemize}
\end{exampleblock}

\begin{block}{数据增强的注意事项}
\begin{itemize}
    \item \textbf{保持类别不变：} 增强后的数据应该还是同一个数字
    \item \textbf{避免过度增强：} 不要把“6”转得看起来像“9”
    \item \textbf{任务相关：} 手写数字识别不需要颜色变换
    \item \textbf{渐进式：} 从轻微变换开始，逐步增加强度
\end{itemize}
\end{block}

\begin{exampleblock}{PyTorch中的MNIST数据增强}
\begin{lstlisting}[language=Python]
from torchvision import transforms

# 定义数据增强变换
train_transform = transforms.Compose([
    transforms.RandomRotation(10),      # 随机旋转±10度
    transforms.RandomAffine(0, translate=(0.1, 0.1)),  # 随机平移10%
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))  # MNIST数据的标准化
])

# 应用到训练数据
train_dataset = MNIST(root='./data', train=True,
                     transform=train_transform, download=True)
\end{lstlisting}
\end{exampleblock}

\subsubsection{如何选择正则化方法？}

\begin{table}[H]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{方法} & \textbf{什么时候用} \\ \midrule
L2正则化 & 几乎所有情况 \\
Dropout & 深层网络，参数量大 \\
早停法 & 总是使用 \\
数据增强 & 图像、语音等数据 \\
L1正则化 & 需要特征选择时 \\ \bottomrule
\end{tabular}
\caption{正则化方法选择指南}
\end{table}

\begin{alertblock}{初学者建议（从简单到复杂）}
\begin{enumerate}
    \item \textbf{第一步：} 只用早停法（最简单，零成本）
    \item \textbf{第二步：} 加上L2正则化（weight\_decay=0.0001）
    \item \textbf{第三步：} 在隐藏层加Dropout（0.3-0.5）
    \item \textbf{第四步：} 尝试数据增强（如果适用）
    \item \textbf{高级：} 组合多种方法，交叉验证调参
\end{enumerate}
\end{alertblock}

\subsection{批量大小与学习率调度}

\subsubsection{批量大小的影响}

不同的批量大小对训练有不同影响：

\begin{table}[H]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{批量大小} & \textbf{优点} & \textbf{缺点} \\ \midrule
小批量（如32） & 泛化性好，内存需求低 & 训练不稳定，梯度噪声大 \\
大批量（如256） & 训练稳定，并行度高 & 内存需求大，可能陷入局部最优 \\
全批量（所有数据） & 梯度准确，收敛稳定 & 内存需求极大，不适合大数据集 \\ \bottomrule
\end{tabular}
\caption{不同批量大小的对比}
\end{table}

\subsubsection{学习率调度}

学习率不必固定不变，可以采用不同的调度策略：

\begin{block}{常见学习率调度策略}
\begin{itemize}
    \item \textbf{步长衰减}：每隔一定轮次将学习率乘以固定因子
    \item \textbf{指数衰减}：学习率按指数函数逐渐减小
    \item \textbf{余弦退火}：学习率按余弦函数周期性变化
    \item \textbf{自适应方法}：根据训练进展自动调整学习率
\end{itemize}
\end{block}

\subsection{评价指标}

对于分类任务，我们需要多个评价指标来全面评估模型性能：

\begin{block}{分类任务的核心指标}
\begin{itemize}
    \item \textbf{准确率（Accuracy）}：正确预测的样本比例
    \[\text{Accuracy} = \frac{\text{正确预测数}}{\text{总预测数}}\]
    
    \item \textbf{精确率（Precision）}：预测为正类中真正为正类的比例
    \[\text{Precision} = \frac{TP}{TP + FP}\]
    
    \item \textbf{召回率（Recall）}：真正为正类中被正确预测的比例
    \[\text{Recall} = \frac{TP}{TP + FN}\]
    
    \item \textbf{F1分数}：精确率和召回率的调和平均
    \[\text{F1} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}\]
\end{itemize}
\end{block}

\begin{exampleblock}{MNIST评价指标选择}
对于MNIST这样的平衡多分类任务，\textbf{准确率}是最直观和常用的指标，因为：
\begin{itemize}
    \item 10个类别的样本数量大致相等
    \item 每个类别的重要性相同
    \item 易于理解和解释
\end{itemize}
\end{exampleblock}

\section{经典神经网络：全连接层}

\subsection{全连接层的基本原理}

全连接层（Fully Connected Layer），也称为线性层或密集层，是神经网络中最基本的构建块。其核心思想是每个输入节点都与每个输出节点相连接。

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.8]
    % 输入层
    \foreach \i in {1,2,3,4,5}
        \node[circle, draw=blue!50, fill=blue!20, minimum size=0.6cm] (in\i) at (0,\i) {};
    
    % 输出层
    \foreach \i in {1,2,3}
        \node[circle, draw=red!50, fill=red!20, minimum size=0.6cm] (out\i) at (4,\i+1) {};
    
    % 全连接
    \foreach \i in {1,2,3,4,5}
        \foreach \j in {1,2,3}
            \draw[->, gray!50] (in\i) -- (out\j);
    
    \node at (-1, -0.5) {输入层 (5个神经元)};
    \node at (5, -0.5) {输出层 (3个神经元)};
    \node at (2, -1.5) {每个输入都连接到每个输出};
\end{tikzpicture}
\caption{全连接层结构示意图}
\end{figure}

\begin{block}{全连接层的数学表达}
对于输入向量 $\mathbf{x} \in \mathbb{R}^n$ 和输出向量 $\mathbf{y} \in \mathbb{R}^m$，全连接层的变换为：
\begin{equation}
\mathbf{y} = \mathbf{W}\mathbf{x} + \mathbf{b}
\end{equation}
其中 $\mathbf{W} \in \mathbb{R}^{m \times n}$ 是权重矩阵，$\mathbf{b} \in \mathbb{R}^m$ 是偏置向量。
\end{block}

\subsection{参数数量分析}

对于从 $n$ 维到 $m$ 维的全连接层，参数总数为：
\begin{equation}
\text{Parameters} = m \times n + m = m(n + 1)
\end{equation}

以MNIST为例，如果将28×28=784像素的图像直接输入到全连接层：

\begin{itemize}
    \item 第一层：假设有256个神经元，参数数量为 $256 \times 784 + 256 = 200,960$
    \item 第二层：从256到128个神经元，参数数量为 $128 \times 256 + 128 = 32,896$
    \item 输出层：从128到10个类别，参数数量为 $10 \times 128 + 10 = 1,290$
\end{itemize}

\textbf{总参数数量：} 235,146个参数

\subsection{PyTorch实现}

\begin{lstlisting}[language=Python, caption=全连接神经网络PyTorch实现]
import torch
import torch.nn as nn
import torch.nn.functional as F

class FullyConnectedNet(nn.Module):
    def __init__(self, input_size=784, hidden_size=256, num_classes=10):
        super(FullyConnectedNet, self).__init__()
        # 将28x28图像展平为784维向量
        self.flatten = nn.Flatten()
        
        # 全连接层堆叠
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)
        self.fc3 = nn.Linear(hidden_size // 2, num_classes)
        
        # Dropout层用于防止过拟合
        self.dropout = nn.Dropout(0.5)
        
    def forward(self, x):
        # 展平输入
        x = self.flatten(x)
        
        # 第一层：784 -> 256
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        # 第二层：256 -> 128
        x = self.fc2(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        # 输出层：128 -> 10
        x = self.fc3(x)
        
        return x

# 模型实例化
model = FullyConnectedNet()
print(f“模型总参数数量: {sum(p.numel() for p in model.parameters()):,}”)
\end{lstlisting}

\subsection{全连接层的优缺点}

\begin{table}[H]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{优点} & \textbf{缺点} \\ \midrule
实现简单直观 & 忽略空间结构信息 \\
理论成熟完善 & 参数数量巨大 \\
易于理解和调试 & 容易过拟合 \\
计算效率高（小模型） & 对平移不具备鲁棒性 \\
适用于非结构化数据 & 需要大量训练数据 \\ \bottomrule
\end{tabular}
\caption{全连接层优缺点对比}
\end{table}

\section{卷积神经网络：CNN}

\subsection{卷积操作的基本原理}

卷积操作是CNN的核心，它通过滑动窗口的方式在输入图像上应用滤波器（卷积核）来提取特征。这种思想最早由LeCun等人~\cite{lecun1989backpropagation}在1989年提出，并在后续的LeNet-5~\cite{lecun1998gradient}工作中得到完善。

\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{figures/conv-process.png}
\caption{卷积操作示意图}
\end{figure}

\begin{block}{卷积操作的数学定义}
\begin{equation}
(f * g)[m,n] = \sum_{i=-\infty}^{\infty}\sum_{j=-\infty}^{\infty} f[i,j] \cdot g[m-i, n-j]
\end{equation}

在离散图像处理中，卷积操作可以表示为：
\begin{equation}
Y[i,j] = \sum_{u=0}^{k-1}\sum_{v=0}^{k-1} X[i+u, j+v] \cdot K[u,v] + b
\end{equation}

其中：
\begin{itemize}
    \item $X$：输入特征图
    \item $K$：卷积核（滤波器）
    \item $b$：偏置项
    \item $k$：卷积核大小
\end{itemize}
\end{block}

\subsection{特征图尺寸计算}

对于输入尺寸为 $W \times H$，卷积核大小为 $K \times K$，步长为 $S$，填充为 $P$ 的情况，输出特征图的尺寸为：

\begin{equation}
W_{out} = \left\lfloor \frac{W - K + 2P}{S} \right\rfloor + 1
\end{equation}

\begin{equation}
H_{out} = \left\lfloor \frac{H - K + 2P}{S} \right\rfloor + 1
\end{equation}

\begin{exampleblock}{MNIST实例计算}
对于28×28的MNIST图像，使用3×3卷积核，步长S=1，填充P=1：
\begin{itemize}
    \item 输出宽度：$W_{out} = \lfloor \frac{28 - 3 + 2 \times 1}{1} \rfloor + 1 = 28$
    \item 输出高度：$H_{out} = \lfloor \frac{28 - 3 + 2 \times 1}{1} \rfloor + 1 = 28$
    \item 结论：输出特征图尺寸保持28×28不变
\end{itemize}
如果使用2×2池化，步长S=2，无填充P=0：
\begin{itemize}
    \item 输出尺寸：$W_{out} = \lfloor \frac{28 - 2 + 0}{2} \rfloor + 1 = 14$
    \item 结论：池化将特征图尺寸减半
\end{itemize}
\end{exampleblock}

\subsection{参数共享机制}

卷积层的一个重要特性是参数共享。同一个卷积核在图像的不同位置重复使用，这大大减少了参数数量。

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{figures/conv-param-share.png}
\caption{参数共享机制示意图}
\end{figure}

对于 $C_{\text{out}}$ 个输出通道，每个通道使用独立的卷积核：

\begin{equation}
\text{Parameters} = C_{\text{out}} \times (K \times K \times C_{in} + 1)
\end{equation}

其中 $C_{in}$ 是输入通道数。

以MNIST为例（单通道输入，32个3×3卷积核）：
\begin{equation}
\text{Parameters} = 32 \times (3 \times 3 \times 1 + 1) = 32 \times 10 = 320
\end{equation}

相比全连接层的数万参数，卷积层的参数数量显著减少了99\%以上。

\subsection{池化层}

池化层用于降采样，减少特征图的空间尺寸：

\begin{itemize}
    \item \textbf{最大池化（Max Pooling）}：取局部区域的最大值
    \item \textbf{平均池化（Average Pooling）}：取局部区域的平均值
\end{itemize}

池化操作不提供可学习参数，但能有效减少计算量和过拟合风险。

\subsection{PyTorch实现CNN}

\begin{lstlisting}[language=Python, caption=基础CNN PyTorch实现]
import torch
import torch.nn as nn
import torch.nn.functional as F

class SimpleCNN(nn.Module):
    def __init__(self, num_classes=10):
        super(SimpleCNN, self).__init__()
        
        # 第一个卷积块：1 -> 32通道
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        
        # 第二个卷积块：32 -> 64通道
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        
        # 池化层
        self.pool = nn.MaxPool2d(2, 2)
        
        # 全连接层
        self.fc1 = nn.Linear(64 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, num_classes)
        
        self.dropout = nn.Dropout(0.5)
        
    def forward(self, x):
        # 第一个卷积块：28x28 -> 14x14
        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.pool(x)
        
        # 第二个卷积块：14x14 -> 7x7
        x = self.conv2(x)
        x = self.bn2(x)
        x = F.relu(x)
        x = self.pool(x)
        
        # 展平
        x = x.view(x.size(0), -1)
        
        # 全连接层
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        
        return x

# 模型实例化
model = SimpleCNN()
print(f“CNN模型总参数数量: {sum(p.numel() for p in model.parameters()):,}”)
\end{lstlisting}

\subsection{CNN的优缺点}

\begin{table}[H]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{优点} & \textbf{缺点} \\ \midrule
保留空间结构信息 & 实现相对复杂 \\
参数数量少 & 超参数选择敏感 \\
平移不变性 & 计算复杂度高 \\
局部连接 & 需要更多内存 \\
分层特征提取 & 训练时间较长 \\ \bottomrule
\end{tabular}
\caption{卷积神经网络优缺点对比}
\end{table}

\section{LeNet架构详解}

\subsection{LeNet-5架构概述}

LeNet-5由Yann LeCun等人于1998年提出~\cite{lecun1998gradient}，是卷积神经网络发展史上的里程碑工作。

\begin{block}{LeNet-5架构}

\begin{equation}
\text{INPUT} \rightarrow \text{CONV} \rightarrow \text{POOL} \rightarrow \text{CONV} \rightarrow \text{POOL} \rightarrow \text{FC} \rightarrow \text{FC} \rightarrow \text{OUTPUT}
\end{equation}
\end{block}

具体参数配置：

\begin{table}[H]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{层类型} & \textbf{输出尺寸} & \textbf{核大小/参数} & \textbf{激活函数} \\ \midrule
输入层 & 32×32×1 & - & - \\
卷积层C1 & 28×28×6 & 5×5, 6个滤波器 & Tanh \\
池化层S2 & 14×14×6 & 2×2, 平均池化 & - \\
卷积层C3 & 10×10×16 & 5×5, 16个滤波器 & Tanh \\
池化层S4 & 5×5×16 & 2×2, 平均池化 & - \\
全连接层C5 & 120 & 5×5×16 → 120 & Tanh \\
全连接层F6 & 84 & 120 → 84 & Tanh \\
输出层 & 10 & 84 → 10 & Softmax \\ \bottomrule
\end{tabular}
\caption{LeNet-5架构详细配置}
\end{table}

\subsection{LeNet的PyTorch实现}

\begin{lstlisting}[language=Python, caption=LeNet-5完整PyTorch实现]
import torch
import torch.nn as nn
import torch.nn.functional as F

class LeNet5(nn.Module):
    def __init__(self, num_classes=10):
        super(LeNet5, self).__init__()
        
        # 第一个卷积块：C1 + S2
        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, padding=0)
        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)
        
        # 第二个卷积块：C3 + S4
        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, padding=0)
        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)
        
        # 全连接层
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, num_classes)
        
    def forward(self, x):
        # 输入：1x28x28
        
        # C1：卷积层，1x28x28 -> 6x24x24
        x = self.conv1(x)
        x = torch.tanh(x)
        
        # S2：平均池化，6x24x24 -> 6x12x12
        x = self.pool1(x)
        
        # C3：卷积层，6x12x12 -> 16x8x8
        x = self.conv2(x)
        x = torch.tanh(x)
        
        # S4：平均池化，16x8x8 -> 16x4x4
        # 注意：这里需要调整以适应MNIST的28x28输入
        # 实际实现中，我们通常将输入填充到32x32
        x = self.pool2(x)
        
        # 展平
        x = x.view(x.size(0), -1)
        
        # 全连接层
        x = self.fc1(x)
        x = torch.tanh(x)
        
        x = self.fc2(x)
        x = torch.tanh(x)
        
        x = self.fc3(x)
        
        return x

# 适配MNIST的LeNet实现
class LeNetMNIST(nn.Module):
    def __init__(self, num_classes=10):
        super(LeNetMNIST, self).__init__()
        
        # 为适配28x28输入，我们使用padding=2将输入变为32x32
        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, padding=2)
        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)
        
        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, padding=0)
        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)
        
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, num_classes)
        
    def forward(self, x):
        # C1：1x28x28 -> 6x28x28 (with padding)
        x = self.conv1(x)
        x = torch.tanh(x)
        
        # S2：6x28x28 -> 6x14x14
        x = self.pool1(x)
        
        # C3：6x14x14 -> 16x10x10
        x = self.conv2(x)
        x = torch.tanh(x)
        
        # S4：16x10x10 -> 16x5x5
        x = self.pool2(x)
        
        # 展平
        x = x.view(x.size(0), -1)
        
        # 全连接层
        x = torch.tanh(self.fc1(x))
        x = torch.tanh(self.fc2(x))
        x = self.fc3(x)
        
        return x

# 模型实例化
model = LeNetMNIST()
print(f“LeNet模型总参数数量: {sum(p.numel() for p in model.parameters()):,}”)
\end{lstlisting}

\subsection{LeNet的参数分析}

LeNet的参数分布：

\begin{itemize}
    \item \textbf{卷积层C1：} $6 \times (5 \times 5 \times 1 + 1) = 156$ 参数
    \item \textbf{卷积层C3：} $16 \times (5 \times 5 \times 6 + 1) = 2,416$ 参数
    \item \textbf{全连接层C5：} $120 \times (16 \times 5 \times 5 + 1) = 48,120$ 参数
    \item \textbf{全连接层F6：} $84 \times (120 + 1) = 10,164$ 参数
    \item \textbf{输出层：} $10 \times (84 + 1) = 850$ 参数
\end{itemize}

\textbf{总参数数量：} 61,706个参数

相比全连接网络，LeNet的参数数量显著减少，但性能却大幅提升。

\subsection{特征图的语义演化：从低层到高层}

卷积神经网络的一个关键特性是特征图随着网络深度的增加而变得越来越具有语义意义。让我们详细分析LeNet中各层特征图的语义内容：

\begin{block}{低层特征（C1层）：边缘和纹理检测}
在第一个卷积层（C1），6个特征图主要检测图像中的基本视觉元素：
\begin{itemize}
    \item \textbf{边缘检测：} 识别数字的轮廓和边界
    \item \textbf{纹理特征：} 捕捉笔画的方向和粗细
    \item \textbf{对比度变化：} 检测明暗交替区域
\end{itemize}

这些特征具有高度的局部性，每个特征图只关注图像的很小一部分区域（5×5感受野）。
\end{block}

\begin{block}{中层特征（C3层）：形状和部件组合}
在第二个卷积层（C3），16个特征图开始组合低层特征，形成更复杂的形状：
\begin{itemize}
    \item \textbf{角点检测：} 识别数字的拐角和交叉点
    \item \textbf{曲线特征：} 检测数字的弯曲部分（如数字“3”的曲线）
    \item \textbf{直线组合：} 识别数字的直线段及其组合
\end{itemize}

这一层的感受野扩大到了14×14，能够捕捉数字的局部结构模式。
\end{block}

\begin{block}{高层特征（全连接层）：语义概念}
全连接层（C5和F6）将中层特征进一步抽象为高级语义概念：
\begin{itemize}
    \item \textbf{数字部件组合：} 识别完整的数字形状特征
    \item \textbf{类别特异性：} 区分不同数字的独特特征
    \item \textbf{不变性表示：} 对位置、大小、旋转具有一定的不变性
\end{itemize}
\end{block}

\begin{exampleblock}{语义演化的数学解释}
特征图的语义演化可以通过特征复杂度来量化：

\begin{equation}
\text{特征复杂度} = \frac{\text{高层特征响应}}{\text{低层特征响应}} \times \text{空间不变性程度}
\end{equation}

随着网络深度增加：
\begin{itemize}
    \item 低层：高空间分辨率，低语义复杂度
    \item 中层：中等空间分辨率，中等语义复杂度
    \item 高层：低空间分辨率，高语义复杂度
\end{itemize}
\end{exampleblock}

\begin{alertblock}{为什么这种分层特征提取有效？}
这种从低层到高层的语义演化之所以有效，是因为：

\begin{enumerate}
    \item \textbf{层次化组合：} 复杂特征可以由简单特征层次化组合而成
    \item \textbf{参数效率：} 共享的低层特征可以被重复使用
    \item \textbf{泛化能力：} 学习通用特征而不是记忆特定样本
    \item \textbf{生物学启发：} 类似于人类视觉系统的信息处理机制，先局部后整体
\end{enumerate}
\end{alertblock}

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.8]
    % 输入图像
    \node at (-0.5, 2.5) {\includegraphics[width=2cm]{figures/mnist.png}};
    \node at (-0.5, 1.5) {\small 输入图像};
    
    % C1层特征图
    \draw[step=0.15cm, green!70!black, very thin] (2.39,0.29) grid (3.6,3.6);
    \node at (3.1, 4.2) {\scriptsize C1: 边缘特征};
    \node at (3.1, -0.4) {\scriptsize 6个特征图};
    
    % C3层特征图
    \draw[step=0.15cm, blue!50, very thin] (5.39,0.59) grid (6.3,3.3);
    \node at (5.9, 4.2) {\scriptsize C3: 形状特征};
    \node at (5.9, -0.4) {\scriptsize 16个特征图};
    
    % 全连接层
    \foreach \i in {1,2,3,4,5}
        \node[circle, draw=orange!70, fill=orange!20, minimum size=0.2cm] (fc1\i) at (8.5, 3.2-0.4*\i) {};
    
    \foreach \i in {1,2,3}
        \node[circle, draw=red!70, fill=red!20, minimum size=0.2cm] (fc2\i) at (10.5, 2.4-0.4*\i) {};
    
    \node at (9.5, 4.2) {\scriptsize FC: 语义特征};
    \node at (9.5, -0.4) {\scriptsize 120 → 84 → 10};
    
    % 箭头连接
    \draw[->, thick] (1.2, 2.5) -- (2.3, 2.5);
    \draw[->, thick] (4, 2.5) -- (5.3, 2.5);
    \draw[->, thick] (6.5, 2.5) -- (8.2, 2.5);
    
    % 语义复杂度标注
    \node[text width=2cm, align=center] at (-0.5, -1.5) {\scriptsize 像素级\\高空间分辨率};
    \node[text width=2cm, align=center] at (3.1, -1.5) {\scriptsize 边缘纹理\\中等分辨率};
    \node[text width=2cm, align=center] at (5.9, -1.5) {\scriptsize 形状部件\\较低分辨率};
    \node[text width=2cm, align=center] at (9.5, -1.5) {\scriptsize 语义概念\\最低分辨率};
\end{tikzpicture}
\caption{LeNet中特征图的语义演化过程}
\end{figure}

这种从具体到抽象、从局部到整体的特征演化过程，使得卷积神经网络能够有效地理解图像内容，并在MNIST等视觉任务上取得优异的性能。

\section{架构对比分析：理论与实践}

\subsection{为什么需要对比分析？}

在深度学习的发展历程中，理解不同架构的优劣对于选择合适的模型至关重要。MNIST数据集为我们提供了一个理想的实验平台，因为它足够简单，可以让我们清晰地看到不同架构的本质差异。

\begin{alertblock}{核心问题}
\begin{itemize}
    \item 为什么CNN在图像任务上表现更好？
    \item 全连接网络的局限性在哪里？
    \item 参数数量与性能之间的关系是什么？
    \item 如何根据任务需求选择合适的架构？
\end{itemize}
\end{alertblock}

\subsection{信息处理方式的对比}

\subsubsection{全连接网络的信息处理}

全连接网络将图像视为一个长向量，完全忽略了像素之间的空间关系：

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.8]
    % 输入图像
    \draw[step=0.4cm, gray, very thin] (0,0) grid (2.8,2.8);
    \node at (1.4, -0.5) {28×28图像};
    
    % 展平过程
    \draw[->, thick, red] (3,1.4) -- (5,1.4);
    \node[red] at (4, 2) {Flatten};
    
    % 展平后的向量
    \draw[->, thick, blue] (5.5,1.4) -- (8.5,1.4);
    \node[blue] at (7, 1) {784维向量};
    
    % 信息丢失示意
    \node[red, text width=5cm] at (3, -3.2) {\textbf{问题：}\\空间结构信息完全丢失\\相邻像素关系被破坏\\需要从头学习所有模式};
\end{tikzpicture}
\caption{全连接网络的信息处理方式}
\end{figure}

\subsubsection{CNN的信息处理}

CNN通过局部感受野和参数共享，保留了图像的空间结构：

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.8]
    % 原始图像
    \draw[step=0.4cm, gray, very thin] (0,0) grid (2.8,2.8);
    \node at (1.4, -0.5) {原始图像};
    
    % 第一层特征图
    \draw[step=0.4cm, green!70!black, very thin] (3.99,0.39) grid (6,2.4);
    \node[green!70!black] at (5, -0.2) {边缘特征};
    
    % 第二层特征图
    \draw[step=0.4cm, blue!50, very thin] (7.99,0.79) grid (9.6,2.4);
    \node[blue!70] at (8.8, 0.2) {形状特征};
    
    % 第三层特征图
    \draw[step=0.4cm, orange!50, very thin] (11.19,0.79) grid (12.41,2);
    \node[orange!70] at (11.6, 0.2) {高级特征};
    
    % 箭头连接
    \draw[->, thick] (2.9,1.4) -- (3.9,1.4);
    \draw[->, thick] (6.1,1.4) -- (7.9,1.4);
    \draw[->, thick] (9.7,1.4) -- (10.9,1.4);
    
    % 层次说明
    \node at (3.5, 3.2) {Conv1};
    \node at (7, 3.2) {Conv2};
    \node at (10.5, 3.2) {Conv3};
    
    % 优势说明
    \node[green!70!black, text width=3cm] at (2, -3) {\textbf{优势：}\\分层特征提取\\空间结构保留\\参数共享};
\end{tikzpicture}
\caption{CNN的分层特征提取}
\end{figure}

\subsection{参数效率的数学分析}

让我们通过一个具体的例子来比较两种架构的参数效率：

\begin{exampleblock}{参数数量对比实例}
假设我们要处理28×28的MNIST图像，目标输出为10个类别：

\textbf{全连接网络方案：}
\begin{itemize}
    \item 输入层：784个神经元（28×28）
    \item 隐藏层1：256个神经元
    \item 隐藏层2：128个神经元
    \item 输出层：10个神经元
\end{itemize}

\textbf{参数计算：}
\begin{align}
\text{Layer 1} &: 784 \times 256 + 256 = 200,960 \\
\text{Layer 2} &: 256 \times 128 + 128 = 32,896 \\
\text{Layer 3} &: 128 \times 10 + 10 = 1,290 \\
\text{Total} &: 235,146 \text{ 参数}
\end{align}
\end{exampleblock}

\begin{exampleblock}{LeNet方案：}
\begin{itemize}
    \item C1：6个5×5卷积核 → 156参数
    \item C3：16个5×5卷积核 → 2,416参数
    \item FC1：16×5×5 → 120 → 48,120参数
    \item FC2：120 → 84 → 10,164参数
    \item FC3：84 → 10 → 850参数
\end{itemize}

\textbf{总参数：} 61,706参数
\end{exampleblock}

\begin{alertblock}{关键发现}
LeNet的参数数量仅为全连接网络的\textbf{26\%}，但在准确率上表现出色。这证明了：
\begin{itemize}
    \item 参数共享的有效性
    \item 归纳偏置的价值
    \item 架构设计的重要性
\end{itemize}
\end{alertblock}

\subsection{什么是归纳偏置？}

在深入比较两种架构的归纳偏置之前，我们需要先理解什么是归纳偏置以及为什么它如此重要。

\begin{block}{归纳偏置的定义}
归纳偏置（Inductive Bias）是指学习算法对可能解空间所做的假设集合。换句话说，它是模型在没有任何数据之前，对学习问题所做的先验假设。

\textbf{关键特征：}
\begin{itemize}
    \item \textbf{先验性：} 在学习开始之前就存在的假设
    \item \textbf{限制性：} 限制了模型可以考虑的解空间
    \item \textbf{引导性：} 帮助模型从有限数据中推广到未见情况
    \item \textbf{权衡性：} 更强的偏置意味着需要更少的数据，但可能错过真实解
\end{itemize}
\end{block}

\begin{exampleblock}{直观的类比}
想象你在学习识别动物：

\textbf{没有归纳偏置的情况：} 你需要看到所有可能的狗的照片才能识别狗，包括各种角度、光照、品种等。

\textbf{有归纳偏置的情况：} 你假设“有四条腿、有尾巴、有特定面部特征的动物可能是狗”。这个假设帮助你从少量例子中学习识别狗。

神经网络的归纳偏置就是类似的假设，只是它们体现在网络结构和连接方式中。
\end{exampleblock}

\begin{alertblock}{为什么归纳偏置至关重要？}
\begin{enumerate}
    \item \textbf{解决欠定问题：} 从有限数据中学习需要额外的约束
    \item \textbf{提高泛化能力：} 帮助模型在未见数据上表现良好
    \item \textbf{减少样本复杂度：} 需要更少的训练数据
    \item \textbf{加速收敛：} 让学习过程更加高效
\end{enumerate}

\textbf{没有归纳偏置的机器学习方法往往：}
\begin{itemize}
    \item 需要海量数据才能学习
    \item 容易过拟合训练数据
    \item 泛化能力差
    \item 训练时间长
\end{itemize}
\end{alertblock}

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.8]
    % 解空间大圆
    \draw[thick, blue!50] (0,0) circle (3);
    \node[blue!70] at (0,3.5) {所有可能的函数空间};
    
    % 归纳偏置限制的空间
    \fill[green!20, opacity=0.5] (0,0) circle (1.5);
    \draw[thick, green!70!black] (0,0) circle (1.5);
    \node[green!70!black] at (0,1.8) {归纳偏置假设的解空间};
    
    % 训练数据点
    \fill[red] (0.5,0.3) circle (2pt);
    \fill[red] (-0.3,0.8) circle (2pt);
    \fill[red] (0.8,-0.5) circle (2pt);
    \node[red] at (1.5,0.8) {训练数据};
    
    % 没有偏置的解
    \draw[dashed, red!70, thick] (0.5,0.3) .. controls (1,1) and (-1,0.5) .. (0.8,-0.5);
    \draw[dashed, red!70, thick] (0.5,0.3) .. controls (-0.5,-1) and (1,-0.5) .. (-0.3,0.8);
    
    % 说明文字
    \node[text width=5cm, align=center] at (0,-4) {
        \small 归纳偏置通过限制解空间，\\
        \small 帮助模型找到更好的泛化解
    };
\end{tikzpicture}
\caption{归纳偏置的作用机理}
\end{figure}

\subsection{归纳偏置的深层分析}

\subsubsection{全连接网络的归纳偏置}

全连接网络的基本假设是：所有输入特征都是同等重要的，且可以任意组合。这种假设：

\begin{itemize}
    \item \textbf{优点：} 通用性强，不依赖于特定的数据结构
    \item \textbf{缺点：} 对于图像等具有空间结构的数据，需要从零开始学习所有空间关系
\end{itemize}

\begin{block}{数学表达}
对于图像分类任务，全连接网络需要学习映射：
\begin{equation}
f: \mathbb{R}^{784} \rightarrow \mathbb{R}^{10}
\end{equation}
其中所有784个像素都被视为独立的特征，没有利用像素间的空间相关性。
\end{block}

\subsubsection{CNN的归纳偏置}

CNN引入了三个关键的归纳偏置：

\begin{enumerate}
    \item \textbf{局部性（Locality）：} 附近的像素更可能相关
    \item \textbf{平移不变性（Translation Invariance）：} 相同的特征可以在不同位置出现
    \item \textbf{组合性（Compositionality）：} 复杂特征可以由简单特征组合而成
\end{enumerate}

\begin{block}{数学表达}
CNN通过卷积操作实现这些偏置：
\begin{equation}
Y[i,j] = \sum_{u=-k}^{k}\sum_{v=-k}^{k} X[i+u, j+v] \cdot K[u,v] + b
\end{equation}
其中$k$定义了局部感受野的大小，相同的核$K$在整个图像上共享。
\end{block}

\subsection{实际性能对比实验}

让我们通过一个具体的实验来对比两种架构的性能。这种对比分析方法受到了现代深度学习研究~\cite{krizhevsky2012imagenet,simonyan2014very,he2016deep}的启发：

\begin{exampleblock}{实验设置}
\begin{itemize}
    \item 训练轮数：10 epochs
    \item 优化器：AdamW，学习率0.001
    \item 批大小：64
    \item 硬件：单个GPU （CPU也行）
    \item 数据增强：无
\end{itemize}
\end{exampleblock}

\subsection{何时选择哪种架构？}

基于我们的分析，选择架构时应考虑：

\begin{table}[H]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{选择全连接网络} & \textbf{选择CNN} \\ \midrule
数据量较小 & 数据量充足 \\
特征间无明显空间关系 & 数据具有空间结构 \\
需要快速原型开发 & 追求最高性能 \\
计算资源极其有限 & 可以接受更长的训练时间 \\
简单的分类任务 & 复杂的视觉任务 \\ \bottomrule
\end{tabular}
\caption{架构选择指南}
\end{table}

这些原则不仅适用于计算机视觉，也适用于其他深度学习领域。

\begin{block}{归纳偏置总结}


\textbf{全连接网络的归纳偏置：}
\begin{itemize}
    \item 所有像素同等重要
    \item 不考虑空间局部性
    \item 需要从头学习所有关系
\end{itemize}

\textbf{卷积网络的归纳偏置：}
\begin{itemize}
    \item 局部连接：附近的像素相关性强
    \item 平移不变性：特征位置不重要
    \item 参数共享：相同特征可在不同位置复用
\end{itemize}

\end{block}

\section{神经网络的缩放定律}

\subsection{缩放定律的基本概念}

缩放定律（Scaling Laws）描述了神经网络性能如何随着模型规模、数据集规模和计算资源的增加而变化。Kaplan等人~\cite{kaplan2020scaling}的研究表明，这些关系通常遵循幂律分布：

\begin{equation}
L(N, D, C) \propto N^{-\alpha} \cdot D^{-\beta} \cdot C^{-\gamma}
\end{equation}

其中：
\begin{itemize}
    \item $L$：损失函数值
    \item $N$：模型参数数量
    \item $D$：数据集大小
    \item $C$：计算资源（FLOPs）
    \item $\alpha, \beta, \gamma$：缩放指数
\end{itemize}

\subsection{模型规模缩放}

对于固定数据集，模型性能与参数数量的关系：

\begin{equation}
L(N) = L_{\infty} + A \cdot N^{-\alpha}
\end{equation}

其中 $L_{\infty}$ 是理论最优损失，$A$ 是幅度系数，$\alpha \approx 0.076$（对于语言模型）。

\subsection{数据集规模缩放}

类似地，数据集规模的影响：

\begin{equation}
L(D) = L_{\infty} + B \cdot D^{-\beta}
\end{equation}

其中 $\beta \approx 0.095$（对于语言模型）。

\subsection{计算最优缩放}

在实际应用中，我们需要在模型规模、数据集规模和计算资源之间找到最优平衡。研究表明：

\begin{equation}
N_{opt} \propto C^{\frac{\alpha}{\alpha + \beta}}, \quad D_{opt} \propto C^{\frac{\beta}{\alpha + \beta}}
\end{equation}

这意味着随着计算资源的增加，模型规模和数据集规模应该按照特定比例同步增长。

\subsection{MNIST与LeNet的缩放分析}

对于MNIST这样的简单任务，缩放定律表现出一些特殊性质：

\begin{itemize}
    \item \textbf{饱和效应：} 当模型达到一定规模后，性能提升趋于饱和
    \item \textbf{数据限制：} MNIST数据集相对较小，限制了大规模模型的效果
    \item \textbf{任务复杂度：} 对于简单的10类分类任务，过大的模型反而容易过拟合
\end{itemize}

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.8]
    \fill[orange!20] (0,3.7) rectangle (12,6);
    \node at (6, 5) {性能饱和区};
    \draw[dashed] (0,3.7) -- (12,3.7);

    % 绘制性能随模型规模变化的曲线
    \draw[->] (0,0) -- (12,0) node[below] {模型规模（参数数量）};
    \draw[->] (0,0) -- (0,6) node[above] {测试准确率};
    
    % 绘制饱和曲线
    \draw[thick, blue, domain=0.5:12, smooth] plot ({\x}, {1 + 3 * (1 - exp(-\x/4))});
    
    % 添加关键点
    \fill[red] (1,1.7) circle (3pt);
    \fill[red] (6,3.32) circle (3pt);
    \fill[red] (11,3.83) circle (3pt);
    
    % 添加解释文字
    \node[text width=4cm, align=left] at (2.7, 1.7) {\small 小模型:\\快速训练，参数效率高};
    
    \node[text width=4cm, align=left] at (7.5, 3.3) {\small 适中模型:\\计算效率平衡};
    
    \node[text width=4cm, align=left] at (12.7, 3.8) {\small 大模型:\\性能饱和，容易过拟合};
\end{tikzpicture}
\caption{MNIST任务上的性能饱和现象}
\end{figure}

\begin{alertblock}{重要启示}
MNIST任务告诉我们~\cite{kaplan2020scaling}：
\begin{enumerate}
    \item 对于简单任务，过大的模型并不能带来性能提升
    \item 数据集的复杂度限制了模型的有效规模
    \item 需要在模型复杂度和任务需求之间找到平衡
    \item LeNet的61K参数已经足以达到99\%+的准确率，好的归纳偏置，比暴力堆参更有效
\end{enumerate}
\end{alertblock}

\section{现代发展与应用}

\subsection{深度学习的现代发展}

自LeNet以来，深度学习领域取得了显著进展。Krizhevsky等人~\cite{krizhevsky2012imagenet}在2012年提出了AlexNet，在ImageNet竞赛中取得了突破性成果，重新点燃了人们对卷积神经网络的兴趣。随后，Simonyan和Zisserman~\cite{simonyan2014very}提出了VGG网络，通过使用更小的卷积核和更深的网络结构进一步提升了性能。He等人~\cite{he2016deep}提出的ResNet通过引入残差连接，解决了深层网络的训练问题，使得网络可以达到前所未有的深度。

\subsection{在实际应用中的选择}

在选择神经网络架构时，应考虑：

\begin{enumerate}
    \item \textbf{任务复杂度：} 简单任务可用小模型，复杂任务需要大模型
    \item \textbf{数据规模：} 小数据集适合简单模型，防止过拟合
    \item \textbf{计算资源：} 考虑训练和推理的计算成本
    \item \textbf{实时性要求：} 移动端应用需要轻量级模型
    \item \textbf{准确率要求：} 医疗等关键应用需要最高准确率
\end{enumerate}

\section{结论}

本文通过MNIST手写数字识别任务，深入比较了全连接神经网络和卷积神经网络的原理、实现和性能。正如LeCun等人在其开创性工作~\cite{lecun1998gradient,lecun1989backpropagation}中所展示的，以及后续研究者~\cite{krizhevsky2012imagenet,simonyan2014very,he2016deep}所发展的，主要结论如下：

\begin{enumerate}
    \item \textbf{架构选择的重要性：} 合适的架构能显著提升性能并减少参数数量
    \item \textbf{归纳偏置的价值：} CNN的空间归纳偏置使其在图像任务上具有天然优势
    \item \textbf{参数效率：} LeNet通过参数共享实现了更高的参数效率
    \item \textbf{缩放定律：} 理解模型规模、数据规模和性能之间的关系对实际应用至关重要
\end{enumerate}

我们看到了从简单的全连接网络到复杂的CNN架构的演进。然而，基本的原理和思想——如参数共享、分层特征提取和适当的归纳偏置——至今仍然适用。理解这些基础概念，对于设计和应用现代神经网络具有重要意义。

\newpage

\begin{thebibliography}{9}

\bibitem{lecun1998gradient}
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, ``Gradient-based learning applied to document recognition,'' \textit{Proceedings of the IEEE}, vol. 86, no. 11, pp. 2278--2324, Nov. 1998.

\bibitem{lecun1989backpropagation}
Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel, ``Backpropagation applied to handwritten zip code recognition,'' \textit{Neural Computation}, vol. 1, no. 4, pp. 541--551, Winter 1989.

\bibitem{krizhevsky2012imagenet}
A. Krizhevsky, I. Sutskever, and G. E. Hinton, ``ImageNet classification with deep convolutional neural networks,'' in \textit{Advances in Neural Information Processing Systems 25}, F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, Eds. Curran Associates, Inc., 2012, pp. 1097--1105.

\bibitem{simonyan2014very}
K. Simonyan and A. Zisserman, ``Very deep convolutional networks for large-scale image recognition,'' \textit{arXiv preprint arXiv:1409.1556}, Sep. 2014.

\bibitem{he2016deep}
K. He, X. Zhang, S. Ren, and J. Sun, ``Deep residual learning for image recognition,'' in \textit{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, Las Vegas, NV, USA, Jun. 2016, pp. 770--778.

\bibitem{kaplan2020scaling}
J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, ``Scaling laws for neural language models,'' \textit{arXiv preprint arXiv:2001.08361}, Jan. 2020.

\end{thebibliography}

\end{document}