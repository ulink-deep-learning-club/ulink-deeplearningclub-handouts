\input{../Common/DocumentBaseFormat.tex}
\input{../Common/WebpageHeader.tex}
\input{../Common/HeaderPackages.tex}
\input{../Common/DocumentTheme.tex}

\title{\textbf{注意力机制在卷积神经网络中的应用：从SE-Net到CBAM}}
\author{深度学习社 \\\small{Cooperated with \texttt{MiniMax M2} \& \texttt{DeepSeek V3.2 Exp}}}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
本文深入探讨了注意力机制在卷积神经网络（CNN）中的应用。首先解释了什么是"注意力"以及为什么需要在CNN中引入注意力机制。随后详细介绍了三类主要的注意力机制：通道注意力（以Squeeze-and-Excitation Networks为代表）、空间注意力，以及结合两者的Convolutional Block Attention Module（CBAM）。文章包含完整的数学推导、PyTorch实现代码，并深入分析了各种注意力机制的优势、局限性和适用场景。通过对比不同方法的参数效率和计算复杂度，我们揭示了注意力机制如何帮助神经网络更好地聚焦于重要特征，从而提升模型性能。
\end{abstract}

\tableofcontents
\newpage

\section{引言：什么是注意力？}

\subsection{人类视觉系统中的注意力}

想象你在一个拥挤的咖啡厅里寻找朋友：

\begin{exampleblock}{日常生活中的注意力}
\begin{itemize}
    \item \textbf{扫视整个场景}：你的眼睛会快速浏览整个房间
    \item \textbf{聚焦关键区域}：当看到熟悉的面孔或颜色时，你会自动聚焦
    \item \textbf{过滤无关信息}：你会忽略背景中的其他人，将注意力集中在目标上
\end{itemize}
\end{exampleblock}

这就是人类视觉系统中的注意力机制——我们不会平等地处理视野中的所有信息，而是有选择地关注最重要的部分。

\subsection{关键术语定义}

在深入讨论注意力机制之前，我们先定义一些核心概念：

\begin{block}{重要术语解释}
\begin{itemize}
    \item \textbf{特征图（Feature Map）}：卷积神经网络中经过卷积操作输出的多维数组，包含空间位置和通道维度的信息
    \item \textbf{通道（Channel）}：特征图的深度维度，每个通道通常对应某种特定的视觉模式或特征
    \item \textbf{空间维度（Spatial Dimension）}：特征图的高度和宽度维度，对应输入图像的空间位置
    \item \textbf{注意力权重（Attention Weight）}：表示不同特征重要性的数值，通常通过归一化处理（如Softmax）得到
    \item \textbf{全局池化（Global Pooling）}：将整个特征图的空间维度压缩为1×1的操作，用于获取全局统计信息
\end{itemize}
\end{block}

\subsection{为什么CNN需要注意力？}

传统的CNN平等对待所有特征，但并非所有特征都同样重要。考虑一个图像分类任务：

\begin{block}{CNN的局限性}
\begin{itemize}
    \item \textbf{特征平等性}：CNN对所有通道和空间位置应用相同的处理
    \item \textbf{噪声敏感性}：无关特征可能会干扰分类决策
    \item \textbf{资源浪费}：计算资源平均分配给所有特征，包括不重要的
\end{itemize}
\end{block}

\begin{exampleblock}{图像分类实例}
当识别一只猫时：
\begin{itemize}
    \item \textbf{重要特征}：猫的脸、耳朵、眼睛
    \item \textbf{次要特征}：背景、阴影、纹理
    \item \textbf{干扰特征}：其他物体、部分遮挡
\end{itemize}

如果没有注意力机制，CNN会同等处理所有这些特征，这显然是低效的。
\end{exampleblock}

\subsection{注意力机制的核心思想}

注意力机制的灵感来自于人类的认知过程，其核心目标是：

\begin{block}{注意力机制的核心思想}
让神经网络能够\textbf{自适应地}决定：
\begin{enumerate}
    \item \textbf{关注什么}：哪些特征/区域最重要
    \item \textbf{忽略什么}：哪些特征可以抑制
    \item \textbf{动态调整}：根据输入内容动态调整关注度
\end{enumerate}
\end{block}

数学上，注意力机制通过学习一组\textbf{权重}来实现这个目标，这些权重决定了每个特征的重要性。

\subsection{注意力机制的类型}

在CNN中，注意力机制主要分为三类：

\begin{table}[H]
\centering
\begin{tabular}{lll}
\toprule
\textbf{类型} & \textbf{关注维度} & \textbf{代表方法} \\
\midrule
通道注意力 & 特征通道的重要性 & SE-Net \\
空间注意力 & 空间位置的重要性 & Spatial Attention \\
混合注意力 & 通道+空间 & CBAM \\
\bottomrule
\end{tabular}
\caption{注意力机制的分类}
\end{table}

接下来的章节，我们将逐一深入探讨这些注意力机制。

\section{通道注意力：Squeeze-and-Excitation Networks (SE-Net)}

\subsection{SE-Net的动机}

在卷积神经网络中，特征图的每个通道（channel）通常代表某种特定的视觉模式（如边缘、纹理、特定对象部分等）。SE-Net的核心思想是：\textbf{不是所有通道都同等重要}。

\subsection{SE块的结构}

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{figures/se-block.png}
\caption{SE块的结构}
\label{fig:se_block}
\end{figure}

SE块~\cite{hu2017squeeze}由两个关键操作组成：

\begin{block}{Squeeze操作（压缩）}
\textbf{目的}：将空间维度$H \times W$压缩为$1 \times 1$，获得全局信息

使用\textbf{全局平均池化}：
\begin{equation}
z_c = F_{sq}(u_c) = \frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W} u_c(i,j)
\end{equation}

其中：
\begin{itemize}
    \item $u_c \in \mathbb{R}^{H \times W}$：第$c$个通道的特征图
    \item $z_c$：第$c$个通道的全局描述符
    \item $F_{sq}$：压缩函数
    \item $H, W$：特征图的高度和宽度
    \item $\mathbb{R}$：实数空间，表示特征图包含实数值
\end{itemize}
\end{block}

\begin{alertblock}{为什么选择全局平均池化？}
全局平均池化具有以下优势：
\begin{itemize}
    \item \textbf{全局信息捕获}：整合整个特征图的空间信息
    \item \textbf{平移不变性}：对输入图像的平移具有鲁棒性
    \item \textbf{防止过拟合}：相比全连接层，参数更少
    \item \textbf{计算高效}：只需简单的平均操作
\end{itemize}

数学上，全局平均池化可以看作是对特征图进行\textbf{空间维度的期望估计}：
\begin{equation}
z_c = \mathbb{E}_{(i,j) \sim \text{Uniform}}[u_c(i,j)]
\end{equation}
其中$\mathbb{E}$表示数学期望，$\text{Uniform}$表示均匀分布。这为每个通道提供了一个全局的统计描述。

\textbf{术语解释}：
\begin{itemize}
    \item \textbf{平移不变性（Translation Invariance）}：模型对输入图像中目标位置的平移不敏感
    \item \textbf{过拟合（Overfitting）}：模型在训练数据上表现很好，但在新数据上表现差的现象
    \item \textbf{全连接层（Fully Connected Layer）}：神经网络中每个神经元都与前一层所有神经元相连的层
\end{itemize}
\end{alertblock}


\begin{block}{Excitation操作（激励）}
\textbf{目的}：学习通道间的非线性关系，生成通道权重

使用两层的全连接网络：
\begin{equation}
\mathbf{s} = F_{ex}(\mathbf{z}, \mathbf{W}) = \sigma(\mathbf{W}_2 \delta(\mathbf{W}_1 \mathbf{z}))
\end{equation}

其中：
\begin{itemize}
    \item $\mathbf{z} \in \mathbb{R}^C$：压缩后的特征向量
    \item $\mathbf{W}_1 \in \mathbb{R}^{\frac{C}{r} \times C}$：第一层权重（降维）
    \item $\mathbf{W}_2 \in \mathbb{R}^{C \times \frac{C}{r}}$：第二层权重（升维）
    \item $\delta$：ReLU激活函数
    \item $\sigma$：Sigmoid激活函数
    \item $r$：降维比率（reduction ratio，通常取16）
\end{itemize}

\textbf{降维比率$r$的数学定义}：
\begin{equation}
r = \frac{C}{\text{隐藏层维度}}
\end{equation}
其中$r$控制着信息压缩的程度，$r$越大表示压缩程度越高。

最终输出：
\begin{equation}
\tilde{u}_c = F_{scale}(u_c, s_c) = s_c \cdot u_c
\end{equation}
其中$s_c$是第$c$个通道的注意力权重，取值范围为$(0,1)$。
\end{block}

\begin{alertblock}{为什么使用两层全连接网络？}
这种设计具有重要的数学意义：
\begin{itemize}
    \item \textbf{降维-升维结构}：$\mathbf{W}_1$将$C$维特征压缩到$\frac{C}{r}$维，$\mathbf{W}_2$再恢复到$C$维
    \item \textbf{非线性建模}：ReLU引入非线性，学习通道间的复杂关系
    \item \textbf{参数效率}：相比直接使用$C \times C$的全连接层，参数数量从$C^2$减少到$\frac{2C^2}{r}$
    \item \textbf{信息瓶颈}：降维操作迫使网络学习最重要的通道关系
\end{itemize}

数学上，这个过程可以看作是一个\textbf{自编码器}结构，学习如何重新加权通道的重要性。
\end{alertblock}

\begin{alertblock}{降维比率$r$的数学意义}
$r$控制了模型的复杂度和性能权衡，其数学影响如下：

\begin{itemize}
    \item \textbf{参数数量分析}：
    \[
    \begin{aligned}
        \text{参数数量} &= \underbrace{C \times \frac{C}{r}}_{\text{第一层}} + \underbrace{\frac{C}{r} \times C}_{\text{第二层}} \\
        &= \frac{2C^2}{r}
    \end{aligned}
    \]
    
    \item \textbf{计算复杂度}：$r$越大，计算量越小，但表达能力可能受限
    
    \item \textbf{信息压缩比}：$r$决定了信息压缩的程度，$r=16$意味着将通道信息压缩到原来的$\frac{1}{16}$
    
    \item \textbf{经验选择}：通常选择$r=16$作为平衡点，因为：
    \begin{itemize}
        \item $r=4$：参数过多，容易过拟合
        \item $r=32$：信息损失过多，性能下降
        \item $r=16$：在性能和效率之间取得良好平衡
    \end{itemize}
\end{itemize}

在实际应用中，可以根据具体任务和计算资源调整$r$值。
\end{alertblock}

\subsection{SE块的PyTorch实现}

\begin{exampleblock}{SE块的核心实现}
SE块的核心结构包括两个步骤：Squeeze（压缩）和Excitation（激励）。

\textbf{关键代码结构：}
\begin{enumerate}
    \item 全局平均池化：将$H \times W$的特征图压缩为$1 \times 1$
    \item 两层全连接网络：学习通道间的非线性关系
    \item Sigmoid激活：生成通道注意力权重（0-1之间）
    \item 特征重标定：用注意力权重缩放输入特征
\end{enumerate}

% 完整的实现请参见 \texttt{code/se_block.py} 文件。
\end{exampleblock}

\subsection{SE-ResNet架构}

SE块可以插入到现有CNN架构中，形成SENet~\cite{hu2017squeeze}：

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/se-resnet.png}
\caption{SE块集成到残差网络中}
\end{figure}

\subsection{参数复杂度分析}

SE块引入的额外参数主要来自两个全连接层：

\begin{block}{额外参数计算}
对于有$C$个通道的特征图：
\begin{align}
\text{参数数量} &= \underbrace{C \times \frac{C}{r}}_{\text{第一层}} + \underbrace{\frac{C}{r} \times C}_{\text{第二层}} \\
&= 2 \times \frac{C^2}{r}
\end{align}

以ResNet-50为例：
\begin{itemize}
    \item 总参数：$\approx 25.6$百万
    \item SE块引入：$\approx 2.5$百万（增加约10\%）
    \item 计算量增加：$\approx 0.26\%$
\end{itemize}
\end{block}

\begin{alertblock}{为什么SE块如此高效？}
尽管引入了额外参数，但SE块的性能提升非常显著：
\begin{itemize}
    \item SE-ResNet-50的性能接近ResNet-101
    \item 计算量几乎不变
    \item 只在最后几个阶段引入SE块，可进一步减少参数
\end{itemize}
\end{alertblock}

\subsection{实验结果}

SE-Net在ImageNet上取得了突破性成果~\cite{hu2017squeeze}：

\begin{table}[H]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{模型} & \textbf{Top-1错误率} & \textbf{Top-5错误率} \\
\midrule
ResNet-50 & 24.80\% & 7.48\% \\
SE-ResNet-50 & \textbf{23.29\%} & \textbf{6.62\%} \\
ResNet-101 & 23.17\% & 6.52\% \\
SE-ResNet-101 & \textbf{22.38\%} & \textbf{6.07\%} \\
\bottomrule
\end{tabular}
\caption{SE-Net在ImageNet上的性能}
\end{table}

SENet最终以25\%的相对改进赢得了ILSVRC 2017分类竞赛~\cite{hu2017squeeze}。

\section{空间注意力：聚焦于重要位置}

\subsection{空间注意力的动机}

通道注意力关注"什么特征重要"，而空间注意力关注"哪里重要"。对于图像中的关键对象，其位置往往比背景更重要。

\begin{exampleblock}{目标检测实例}
在检测行人时：
\begin{itemize}
    \item \textbf{关注区域}：行人的身体、头部、四肢
    \item \textbf{忽略区域}：路面、天空、其他车辆
    \item \textbf{空间权重}：不同位置有不同的重要性权重
\end{itemize}
\end{exampleblock}

\subsection{空间注意力的计算}

空间注意力通常通过以下步骤计算：

\begin{block}{空间注意力的一般形式}
对于输入特征图$F \in \mathbb{R}^{C \times H \times W}$，空间注意力图$M_s \in \mathbb{R}^{H \times W}$的计算：

\begin{equation}
M_s = \sigma(f^{7 \times 7}([\mathrm{AvgPool}(F); \mathrm{MaxPool}(F)]))
\end{equation}

其中：
\begin{itemize}
    \item $\mathrm{AvgPool}(F)$：沿通道维度的平均池化，输出$\mathbb{R}^{1 \times H \times W}$
    \item $\mathrm{MaxPool}(F)$：沿通道维度的最大池化，输出$\mathbb{R}^{1 \times H \times W}$
    \item $[\mathrm{AvgPool}(F); \mathrm{MaxPool}(F)]$：concatenation，输出$\mathbb{R}^{2 \times H \times W}$
    \item $f^{7 \times 7}$：$7 \times 7$卷积层
    \item $\sigma$：Sigmoid激活函数
\end{itemize}

最终输出：
\begin{equation}
F' = M_s \odot F
\end{equation}
其中$\odot$表示逐元素相乘。
\end{block}

\begin{alertblock}{为什么使用$7 \times 7$卷积核？}
$7 \times 7$卷积核的选择具有重要考虑：
\begin{itemize}
    \item \textbf{感受野大小}：$7 \times 7$提供足够大的感受野来捕获局部上下文
    \item \textbf{计算效率}：相比更大的卷积核，计算量适中
    \item \textbf{经验验证}：实验表明$7 \times 7$在性能和效率之间取得最佳平衡
    \item \textbf{空间平滑性}：较大的卷积核产生更平滑的注意力分布
\end{itemize}

在深层网络中，可以使用$3 \times 3$卷积核来减少计算量。
\end{alertblock}

\begin{alertblock}{为什么同时使用平均池化和最大池化？}
这种设计的数学和直觉解释：

\textbf{平均池化的作用}：
\begin{itemize}
    \item \textbf{全局统计信息}：捕捉每个空间位置在所有通道上的平均响应
    \item \textbf{稳定性}：对噪声和异常值具有鲁棒性
    \item \textbf{平滑性}：提供平滑的空间注意力分布
\end{itemize}

\textbf{最大池化的作用}：
\begin{itemize}
    \item \textbf{突出特征}：捕捉每个空间位置最显著的特征响应
    \item \textbf{边界敏感}：对物体边界和细节更敏感
    \item \textbf{稀疏性}：倾向于产生更稀疏的注意力分布
\end{itemize}

\textbf{结合使用的优势}：
\begin{itemize}
    \item \textbf{互补信息}：平均池化提供全局上下文，最大池化突出局部细节
    \item \textbf{鲁棒性增强}：减少单一池化方法可能带来的偏差
    \item \textbf{信息完整性}：同时考虑平均响应和峰值响应
\end{itemize}

数学上，这相当于：
\begin{equation}
M_s = \sigma\left(f^{7 \times 7}\left(\left[\mathbb{E}_c[F_{c,:,:}]; \max_c[F_{c,:,:}]\right]\right)\right)
\end{equation}
其中$\mathbb{E}_c$表示通道维度上的期望，$\max_c$表示通道维度上的最大值。
\end{alertblock}

\subsection{空间注意力的PyTorch实现}

\begin{exampleblock}{空间注意力模块核心代码}


完整实现请参见 \texttt{code/spatial\_attention.py} 文件。
\end{exampleblock}

\subsection{空间注意力的应用}

空间注意力在以下任务中特别有效：

\begin{itemize}
    \item \textbf{语义分割}：关注前景对象区域
    \item \textbf{目标检测}：关注可能包含目标的区域
    \item \textbf{图像分类}：关注主要对象的位置
    \item \textbf{图像分割}：精确分割对象边界
\end{itemize}

\section{混合注意力：Convolutional Block Attention Module (CBAM)}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/cbam-block.png}
\caption{CBAM模块}
\end{figure}

\subsection{CBAM的整体思路}

CBAM~\cite{woo2018cbam}是结合了通道注意力和空间注意力的模块。其核心思想是：\textbf{重要特征不仅需要正确的通道权重，还需要出现在正确的位置}。

\begin{block}{CBAM的流水线}
CBAM采用串行连接的方式：
\begin{equation}
F' = M_s(F) \odot M_c(F) \odot F
\end{equation}

或者并行连接：
\begin{equation}
F' = \alpha \cdot M_s(F) \odot F + \beta \cdot M_c(F) \odot F
\end{equation}

其中$\alpha + \beta = 1$是可学习的融合权重。
\end{block}

\begin{alertblock}{串行 vs 并行：为什么串行连接效果更好？}
\textbf{串行连接的优势分析}：

\begin{itemize}
    \item \textbf{信息处理顺序}：先决定"什么特征重要"，再决定"这些重要特征在哪里"
    \item \textbf{计算效率}：空间注意力在通道注意力之后计算，可以利用已经筛选的特征
    \item \textbf{梯度传播}：串行连接提供更清晰的梯度传播路径
    \item \textbf{特征协同}：通道注意力为空间注意力提供更好的输入特征
\end{itemize}

\textbf{数学解释}：
串行连接：
\begin{equation}
F' = M_s(M_c(F) \odot F) \odot (M_c(F) \odot F)
\end{equation}

并行连接：
\begin{equation}
F' = \alpha \cdot M_s(F) \odot F + \beta \cdot M_c(F) \odot F
\end{equation}

\textbf{串行连接的优势}：
\begin{itemize}
    \item \textbf{级联效应}：通道注意力先筛选特征，空间注意力在筛选后的特征上工作
    \item \textbf{减少干扰}：空间注意力不会受到不重要通道的干扰
    \item \textbf{计算协同}：两种注意力机制相互增强，而不是简单相加
    \item \textbf{实验验证}：CBAM原论文中串行连接比并行连接性能更好
\end{itemize}

\textbf{直觉理解}：
想象你在人群中找人：
\begin{itemize}
    \item \textbf{通道注意力}：先确定要找的人的特征（穿什么颜色衣服、身高多少）
    \item \textbf{空间注意力}：然后在人群中定位符合这些特征的人
    \item 如果同时进行，可能会被无关特征干扰
\end{itemize}
\end{alertblock}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/cbam-channel-spatial-module.png}
\caption{CBAM子模块}
\end{figure}

\subsection{CBAM的通道注意力子模块}

CBAM的通道注意力与SE块类似，但有一些改进：

\begin{block}{CBAM通道注意力}
使用平均池化和最大池化两种方式：

\begin{equation}
M_c(F) = \sigma(\mathrm{MLP}(\mathrm{AvgPool}(F)) + \mathrm{MLP}(\mathrm{MaxPool}(F)))
\end{equation}

其中：
\begin{itemize}
    \item $\mathrm{AvgPool}(F)$：全局平均池化，输出$\mathbb{R}^{C \times 1 \times 1}$
    \item $\mathrm{MaxPool}(F)$：全局最大池化，输出$\mathbb{R}^{C \times 1 \times 1}$
    \item $\mathrm{MLP}$：两层的全连接网络（共享权重）
    \item $\sigma$：Sigmoid激活函数
\end{itemize}

计算公式：
\begin{align}
M_c(F) &= \sigma(W_2 \delta(W_1 \mathrm{AvgPool}(F)) + W_2 \delta(W_1 \mathrm{MaxPool}(F))) \\
&= \sigma(W_2 \delta(W_1 F_{avg}^c) + W_2 \delta(W_1 F_{max}^c))
\end{align}
\end{block}

\begin{exampleblock}{为什么同时使用平均池化和最大池化？}
\begin{itemize}
    \item \textbf{平均池化}：捕捉通道的全局统计信息
    \item \textbf{最大池化}：捕捉通道的突出特征
    \item \textbf{结合使用}：提供更丰富的通道描述
\end{itemize}
\end{exampleblock}

\subsection{CBAM的空间注意力子模块}

在通道注意力之后，应用空间注意力：

\begin{block}{CBAM空间注意力}
使用通道维度上的池化：

\begin{equation}
M_s(F') = \sigma(f^{7 \times 7}([\mathrm{AvgPool}(F'); \mathrm{MaxPool}(F')]))
\end{equation}

其中：
\begin{itemize}
    \item $\mathrm{AvgPool}(F')$：沿通道维度的平均池化，输出$\mathbb{R}^{1 \times H \times W}$
    \item $\mathrm{MaxPool}(F')$：沿通道维度的最大池化，输出$\mathbb{R}^{1 \times H \times W}$
    \item $f^{7 \times 7}$：$7 \times 7$卷积，输出单通道空间注意力图
    \item $\sigma$：Sigmoid激活函数
\end{itemize}

计算过程：
\begin{align}
F'_{avg} &= \mathrm{AvgPool}_{c}(F') \in \mathbb{R}^{1 \times H \times W} \\
F'_{max} &= \mathrm{MaxPool}_{c}(F') \in \mathbb{R}^{1 \times H \times W} \\
M_s(F') &= \sigma(f^{7 \times 7}([F'_{avg}; F'_{max}]))
\end{align}

最终输出：
\begin{equation}
F'' = M_s(F') \odot F'
\end{equation}
\end{block}

\begin{alertblock}{注意力的计算顺序}
CBAM先计算通道注意力，再计算空间注意力的原因：
\begin{enumerate}
    \item \textbf{先筛选特征}：通道注意力先决定"什么特征重要"
    \item \textbf{再定位位置}：空间注意力决定"这些重要特征在哪里"
    \item \textbf{级联效应}：两种注意力相互补充，提升整体性能
\end{enumerate}
\end{alertblock}

\subsection{CBAM的完整实现}

\begin{exampleblock}{CBAM模块核心结构}


完整实现请参见 \texttt{code/cbam.py} 文件。
\end{exampleblock}

\subsection{CBAM集成到CNN}

CBAM是轻量级模块，可以插入到CNN的任意位置：

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/cbam-resnet.png}
\caption{CBAM在残差网络中的集成}
\end{figure}

\subsection{CBAM的性能分析}

CBAM的性能提升~\cite{woo2018cbam}：

\begin{table}[H]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{模型} & \textbf{Top-1错误率} & \textbf{Top-5错误率} \\
\midrule
ResNet-50 & 24.80\% & 7.48\% \\
SE-ResNet-50 & 23.29\% & 6.62\% \\
CBAM-ResNet-50 & \textbf{22.99\%} & \textbf{6.38\%} \\
\midrule
ResNet-101 & 23.17\% & 6.52\% \\
SE-ResNet-101 & 22.38\% & 6.07\% \\
CBAM-ResNet-101 & \textbf{22.16\%} & \textbf{5.92\%} \\
\bottomrule
\end{tabular}
\caption{CBAM vs SE-Net性能对比}
\end{table}

CBAM相比SE-Net进一步提升了性能：
\begin{itemize}
    \item ResNet-50：+0.30\% Top-1准确率提升
    \item ResNet-101：+0.22\% Top-1准确率提升
    \item 计算开销极小（<1\%）
\end{itemize}

\section{注意力机制的理论分析}

\subsection{数学视角：注意力作为加权平均}

从数学角度看，注意力机制本质上是加权平均操作：

\begin{block}{注意力的通用形式}
给定查询$\mathbf{q}$、键$\{\mathbf{k}_i\}$和值$\{\mathbf{v}_i\}$，注意力输出为：

\begin{equation}
\text{Attention}(\mathbf{q}, \{\mathbf{k}_i\}, \{\mathbf{v}_i\}) = \sum_{i} \alpha_i \mathbf{v}_i
\end{equation}

其中权重$\alpha_i$通过以下方式计算：
\begin{equation}
\alpha_i = \frac{\exp(\text{score}(\mathbf{q}, \mathbf{k}_i))}{\sum_j \exp(\text{score}(\mathbf{q}, \mathbf{k}_j))}
\end{equation}

\textbf{术语解释}：
\begin{itemize}
    \item $\mathbf{q}$：查询向量，表示当前需要关注的内容
    \item $\mathbf{k}_i$：键向量，表示可被关注的内容
    \item $\mathbf{v}_i$：值向量，包含实际的信息内容
    \item $\alpha_i$：注意力权重，表示第$i$个元素的重要性
    \item $\exp$：指数函数，用于将分数转换为正数
\end{itemize}

常见打分函数：
\begin{itemize}
    \item \textbf{点积}：$\text{score}(\mathbf{q}, \mathbf{k}_i) = \mathbf{q}^T \mathbf{k}_i$
    \item \textbf{加性}：$\text{score}(\mathbf{q}, \mathbf{k}_i) = \mathbf{v}^T \tanh(\mathbf{W}_q \mathbf{q} + \mathbf{W}_k \mathbf{k}_i)$
    \item \textbf{缩放点积}：$\text{score}(\mathbf{q}, \mathbf{k}_i) = \frac{\mathbf{q}^T \mathbf{k}_i}{\sqrt{d}}$
\end{itemize}
\end{block}

\subsection{线性代数视角：特征空间的重新加权}

从线性代数角度，注意力机制可以看作是在特征空间上的线性变换：

\begin{block}{通道注意力的矩阵表示}
设输入特征$F \in \mathbb{R}^{C \times HW}$（$HW = H \times W$），则通道注意力可以表示为：

\begin{align}
\mathbf{z} &= \text{GlobalPool}(F) = \frac{1}{HW} F \mathbf{1} \in \mathbb{R}^C \\
\mathbf{s} &= \sigma(\mathbf{W}_2 \delta(\mathbf{W}_1 \mathbf{z})) \in \mathbb{R}^C \\
F' &= \text{diag}(\mathbf{s}) F
\end{align}

其中：
\begin{itemize}
    \item $\mathbf{1} \in \mathbb{R}^{HW}$：全1向量
    \item $\text{diag}(\mathbf{s})$：以$\mathbf{s}$为对角线的对角矩阵
\end{itemize}

这相当于：对每个通道$c$，乘以注意力权重$s_c$，即$F'_c = s_c \cdot F_c$。
\end{block}

\begin{alertblock}{线性代数视角的深入理解}
从线性代数角度看，注意力机制具有以下数学性质：

\textbf{特征空间变换}：
\begin{itemize}
    \item \textbf{对角变换}：$\text{diag}(\mathbf{s})$是对角矩阵，相当于对每个通道进行独立的缩放
    \item \textbf{保结构变换}：变换保持特征空间的线性结构，但重新加权不同维度的重要性
    \item \textbf{可逆性}：当所有$s_c > 0$时，变换是可逆的
\end{itemize}

\textbf{几何解释}：
\begin{itemize}
    \item \textbf{特征缩放}：每个特征通道被独立缩放，相当于在特征空间中沿坐标轴方向拉伸或压缩
    \item \textbf{重要性排序}：注意力权重$\mathbf{s}$定义了特征通道的重要性顺序
    \item \textbf{子空间选择}：权重接近0的通道被抑制，相当于选择重要的特征子空间
\end{itemize}

\textbf{矩阵分析}：
注意力变换可以分解为：
\begin{equation}
F' = \underbrace{\text{diag}(\mathbf{s})}_{\text{注意力矩阵}} \cdot \underbrace{F}_{\text{原始特征}}
\end{equation}
其中注意力矩阵是对角矩阵，计算复杂度为$O(C \cdot HW)$，相比全连接层的$O(C^2 \cdot HW)$要高效得多。
\end{alertblock}

\subsection{信息论视角：条件概率建模}

从信息论角度，注意力机制可以看作是在建模条件概率：

\begin{block}{注意力的概率解释}
假设我们想预测输出$y$，给定输入特征$\{x_i\}$。注意力机制学习一个条件分布：

\begin{equation}
p(y | \{x_i\}, \{a_i\}) = \sum_i p(y | x_i) p(x_i | \{a_i\})
\end{equation}

其中注意力权重$a_i = \frac{\exp(f(x_i))}{\sum_j \exp(f(x_j))}$，$f$是注意力函数。

\textbf{术语解释}：
\begin{itemize}
    \item $p(y | \{x_i\}, \{a_i\})$：给定输入特征和注意力权重的条件概率
    \item $p(y | x_i)$：给定单个特征的条件概率
    \item $p(x_i | \{a_i\})$：特征在注意力权重下的条件概率
    \item $f(x_i)$：注意力打分函数
\end{itemize}

这相当于：选择性地关注某些特征，忽略其他特征，从而降低模型的不确定性。

\textbf{信息论概念}：
\begin{itemize}
    \item \textbf{条件概率（Conditional Probability）}：在已知某些事件发生的条件下，其他事件发生的概率
    \item \textbf{不确定性（Uncertainty）}：在信息论中，表示系统状态的不确定程度
    \item \textbf{条件分布（Conditional Distribution）}：给定某些条件下，随机变量的概率分布
\end{itemize}
\end{block}

\begin{alertblock}{信息论视角的深入分析}
从信息论角度看，注意力机制具有以下重要性质：

\textbf{信息瓶颈理论}：
\begin{itemize}
    \item \textbf{信息压缩}：注意力机制通过权重分配实现信息压缩
    \item \textbf{相关特征选择}：选择与目标任务最相关的特征子集
    \item \textbf{互信息最大化}：注意力权重最大化输入特征与输出之间的互信息
\end{itemize}

\textbf{熵与不确定性}：
\begin{itemize}
    \item \textbf{条件熵减少}：$H(y|\{x_i\}, \{a_i\}) \leq H(y|\{x_i\})$
    \item \textbf{不确定性降低}：注意力机制通过聚焦重要特征降低预测不确定性
    \item \textbf{信息增益}：注意力权重提供了关于哪些特征对预测最有用的信息
\end{itemize}

\textbf{信息论术语解释}：
\begin{itemize}
    \item \textbf{信息瓶颈理论（Information Bottleneck Theory）}：一种理论框架，描述神经网络如何通过压缩输入信息来学习有用表示
    \item \textbf{互信息（Mutual Information）}：衡量两个随机变量之间相互依赖程度的量
    \item \textbf{熵（Entropy）}：衡量随机变量不确定性的度量
    \item \textbf{条件熵（Conditional Entropy）}：在已知某些条件下，随机变量的不确定性
    \item $H(y|\{x_i\})$：给定输入特征$\{x_i\}$时输出$y$的条件熵
\end{itemize}

\end{alertblock}
\begin{alertblock}{概率图模型视角}
注意力机制可以看作是一个概率图模型：
\begin{equation}
y \leftarrow \{x_i\} \leftarrow \{a_i\}
\end{equation}
其中注意力权重$\{a_i\}$是隐变量，决定了哪些输入特征$x_i$对输出$y$有贡献。

\textbf{变分推断视角}：
注意力机制可以看作是在进行变分推断，通过参数化分布$q(a|x)$来近似真实后验分布$p(a|x,y)$。

\textbf{概率图模型术语}：
\begin{itemize}
    \item \textbf{隐变量（Latent Variable）}：模型中不可直接观测的变量
    \item \textbf{后验分布（Posterior Distribution）}：给定观测数据时，模型参数或隐变量的概率分布
    \item \textbf{变分推断（Variational Inference）}：一种近似推断方法，通过优化来近似复杂分布
\end{itemize}
\end{alertblock}

\subsection{梯度流分析}

注意力机制的梯度传播：

\begin{block}{SE块的梯度计算}
对于通道注意力，梯度传播到通道$c$的权重为：

\begin{align}
\frac{\partial \mathcal{L}}{\partial s_c} &= \frac{\partial \mathcal{L}}{\partial \tilde{u}_c} \cdot u_c \\
\frac{\partial \mathcal{L}}{\partial u_c} &= s_c \cdot \frac{\partial \mathcal{L}}{\partial \tilde{u}_c}
\end{align}

其中$\mathcal{L}$是损失函数。

这意味着：
\begin{itemize}
    \item \textbf{梯度缩放}：通道权重$s_c$缩放了特征$u_c$的梯度
    \item \textbf{动态调整}：重要特征的梯度更大，更新更快
    \item \textbf{抑制传播}：不重要的特征梯度较小，更新较慢
\end{itemize}
\end{block}

\begin{alertblock}{梯度动态调整机制}
注意力机制通过智能的梯度分配改善优化过程：

\begin{itemize}
    \item \textbf{梯度集中}：重要特征的梯度被放大，加速收敛
    \item \textbf{噪声抑制}：不重要特征的梯度被衰减，减少干扰
    \item \textbf{自适应学习率}：不同特征有不同的有效学习率
    \item \textbf{稀疏性}：注意力权重趋向于稀疏（某些权重接近0）
\end{itemize}

这种机制让模型能够专注于真正重要的特征，类似于人类认知中的"选择性注意"过程。
\end{alertblock}

\begin{alertblock}{优化理论优势}
从优化理论角度，注意力机制带来多重好处：

\begin{itemize}
    \item \textbf{条件数改善}：注意力机制可以改善损失函数的条件数
    \item \textbf{收敛速度}：梯度集中在重要特征上可以加速收敛
    \item \textbf{局部最小值避免}：动态的注意力权重有助于跳出局部最小值
    \item \textbf{泛化能力}：注意力机制通过特征选择提高模型泛化能力
\end{itemize}

\textbf{优化理论术语解释}：
\begin{itemize}
    \item \textbf{条件数（Condition Number）}：衡量函数优化难易程度的指标，条件数越小越容易优化
    \item \textbf{收敛速度（Convergence Rate）}：优化算法达到最优解的速度
    \item \textbf{局部最小值（Local Minimum）}：损失函数在某个小区域内的最小值，但不一定是全局最小值
    \item \textbf{泛化能力（Generalization Ability）}：模型在未见过的数据上的表现能力
\end{itemize}
\end{alertblock}

\begin{alertblock}{数学分析：梯度传播机制}
考虑损失函数$\mathcal{L}$，注意力机制引入的梯度变化：
\begin{align}
\frac{\partial \mathcal{L}}{\partial \theta} &= \sum_c s_c \cdot \frac{\partial \mathcal{L}}{\partial F_c} \cdot \frac{\partial F_c}{\partial \theta} \\
&= \sum_c \underbrace{s_c}_{\text{注意力权重}} \cdot \underbrace{\frac{\partial \mathcal{L}}{\partial F_c}}_{\text{特征梯度}} \cdot \underbrace{\frac{\partial F_c}{\partial \theta}}_{\text{参数梯度}}
\end{align}

\textbf{术语解释}：
\begin{itemize}
    \item $\theta$：模型参数
    \item $\frac{\partial \mathcal{L}}{\partial \theta}$：损失函数对模型参数的梯度
    \item $\frac{\partial F_c}{\partial \theta}$：特征对模型参数的梯度
    \item $\sum_c$：对所有通道求和
\end{itemize}

这意味着：
\begin{itemize}
    \item 当$s_c \approx 1$时，特征$c$的梯度被完全保留
    \item 当$s_c \approx 0$时，特征$c$的梯度被抑制
    \item 梯度传播变得更有针对性
\end{itemize}
\end{alertblock}

\begin{alertblock}{优化稳定性保障}
注意力机制还提供了重要的稳定性保障：

\begin{itemize}
    \item \textbf{梯度裁剪}：注意力权重自然限制了梯度大小
    \item \textbf{数值稳定性}：Sigmoid激活函数提供数值稳定性
    \item \textbf{训练平滑性}：注意力权重变化相对平滑，避免训练震荡
\end{itemize}

这些特性共同确保了训练过程的稳定性和可靠性。
\end{alertblock}

\begin{alertblock}{核心优化优势总结}
注意力机制通过智能地重新分配计算资源，让模型能够：

\begin{itemize}
    \item \textbf{专注于真正重要的特征}
    \item \textbf{抑制噪声和无关特征的干扰}
    \item \textbf{实现更高效的梯度传播}
    \item \textbf{提升整体优化效率和模型性能}
\end{itemize}

这种机制本质上模拟了人类认知中的"选择性注意"过程，在深度学习优化中起到了类似"特征重要性指导"的作用。
\end{alertblock}

\section{其他注意力机制}

除了SE-Net和CBAM，还有许多其他注意力机制~\cite{zhang2019self,wang2017residual,fu2019dual}：

\subsection{非局部注意力（Non-Local Attention）}

非局部注意力~\cite{wang2018non}用于捕获长距离依赖：

\begin{block}{非局部注意力公式}
\begin{equation}
\mathbf{y}_i = \frac{1}{\mathcal{C}(\mathbf{x})} \sum_{\forall j} f(\mathbf{x}_i, \mathbf{x}_j) g(\mathbf{x}_j)
\end{equation}

其中：
\begin{itemize}
    \item $f(\mathbf{x}_i, \mathbf{x}_j)$：位置$i$和$j$之间的关系
    \item $g(\mathbf{x}_j)$：位置$j$的特征嵌入
    \item $\mathcal{C}(\mathbf{x})$：归一化因子
\end{itemize}
\end{block}

\subsection{自注意力（Self-Attention）}

自注意力机制~\cite{vaswani2017attention}允许模型关注自身特征的不同位置：

\begin{block}{多头自注意力}
\begin{equation}
\text{Head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\end{equation}

其中$Q = K = V = X$（输入特征）。

多头注意力通过多个注意力头捕获不同类型的关系。
\end{block}

\subsection{协调注意力（Coordinate Attention）}

协调注意力~\cite{hou2021coordinate}将空间位置信息编码到通道注意力中：

\begin{block}{协调注意力}
分为两个步骤：
\begin{enumerate}
    \item \textbf{坐标信息嵌入}：
    \begin{align}
        z_c^h &= \frac{1}{W} \sum_{i=1}^{W} x_c(i, j) \\
        z_c^w &= \frac{1}{H} \sum_{j=1}^{H} x_c(i, j)
    \end{align}
    \item \textbf{坐标注意力生成}：
    \begin{align}
        \alpha &= \sigma(F_1(z_c^h)) \\
        \beta &= \sigma(F_2(z_c^w)) \\
        y_c(i,j) &= \alpha \cdot \beta \cdot x_c(i,j)
    \end{align}
\end{enumerate}
\end{block}

\subsection{不同注意力机制的对比}

\begin{table}[H]
\centering
\begin{tabular}{lccccccc}
\toprule
\textbf{机制} & \textbf{参数增加} & \textbf{计算开销} & \textbf{内存需求} & \textbf{性能提升} & \textbf{易集成} & \textbf{适用场景} & \textbf{主要优势} \\
\midrule
SE-Net & 中等 & 很低 & 低 & 高 & 容易 & 通用分类 & 通道选择 \\
CBAM & 中等 & 低 & 中等 & 很高 & 容易 & 多任务 & 通道+空间 \\
Non-local & 高 & 高 & 高 & 很高 & 困难 & 长距离依赖 & 全局上下文 \\
Self-Attention & 高 & 高 & 高 & 很高 & 困难 & 序列建模 & 自相关性 \\
Coordinate Attention & 低 & 很低 & 低 & 高 & 容易 & 移动端 & 位置感知 \\
\bottomrule
\end{tabular}
\caption{不同注意力机制的特性对比}
\end{table}

\begin{alertblock}{选择注意力机制的详细准则}
\textbf{根据计算资源选择}：
\begin{itemize}
    \item \textbf{计算资源有限}：选择SE-Net或Coordinate Attention（参数增加少，计算开销低）
    \item \textbf{追求最佳性能}：选择CBAM或Non-Local（性能提升显著，但计算成本高）
    \item \textbf{平衡性能与效率}：选择SE-Net或CBAM（在性能和效率间取得良好平衡）
\end{itemize}

\textbf{根据任务类型选择}：
\begin{itemize}
    \item \textbf{图像分类}：SE-Net或CBAM（关注通道重要性）
    \item \textbf{目标检测}：CBAM或Coordinate Attention（需要空间位置信息）
    \item \textbf{语义分割}：Non-Local或CBAM（需要长距离上下文）
    \item \textbf{移动端应用}：Coordinate Attention或SE-Net（轻量级设计）
\end{itemize}

\textbf{根据实现复杂度选择}：
\begin{itemize}
    \item \textbf{容易实现}：SE-Net或CBAM（结构简单，易于集成）
    \item \textbf{中等复杂度}：Coordinate Attention（需要坐标编码）
    \item \textbf{复杂实现}：Non-Local或Self-Attention（计算复杂度高）
\end{itemize}

\textbf{性能与效率权衡}：
\begin{itemize}
    \item \textbf{SE-Net}：性能提升约1-2\%，参数增加约10\%
    \item \textbf{CBAM}：性能提升约1.5-2.5\%，参数增加约10-15\%
    \item \textbf{Non-Local}：性能提升约2-3\%，参数增加约20-30\%
    \item \textbf{Coordinate Attention}：性能提升约1-1.5\%，参数增加约5-8\%
\end{itemize}
\end{alertblock}

\section{实际应用案例}

注意力机制在实际任务中取得了显著成果：

\subsection{图像分类}

在ImageNet分类任务~\cite{hu2017squeeze}中：
\begin{itemize}
    \item ResNet-50 + CBAM：Top-1准确率提升0.81\%
    \item EfficientNet + SE：显著减少参数量
    \item Vision Transformer中使用自注意力机制
\end{itemize}

\subsection{目标检测}

在COCO数据集~\cite{woo2018cbam}上：
\begin{itemize}
    \item Faster R-CNN + SE-ResNet-50：AP提升2.4\%
    \item RetinaNet + CBAM：改进对小目标的检测
    \item YOLO系列整合SE模块提升检测精度
\end{itemize}

\subsection{语义分割}

在Cityscapes数据集~\cite{fu2019dual}中：
\begin{itemize}
    \item DeepLab v3+ + CBAM：mIoU提升1.2\%
    \item PSPNet + 注意力机制：改进上下文建模
    \item 注意力引导的分割网络：提升边界精度
\end{itemize}

\section{优势、局限与未来方向}

\subsection{注意力机制的优势}

\begin{block}{主要优势}
\begin{enumerate}
    \item \textbf{性能提升显著}：在多项任务上取得SOTA结果
    \item \textbf{计算开销小}：相比模型规模增加，性能提升巨大
    \item \textbf{易于集成}：可插入现有CNN架构
    \item \textbf{可解释性强}：注意力权重可视化帮助理解模型
    \item \textbf{通用性强}：适用于各种视觉任务
\end{enumerate}
\end{block}

\subsection{注意力机制的局限}

\begin{block}{主要局限}
\begin{enumerate}
    \item \textbf{手工设计}：注意力机制通常是经验设计的，缺乏理论指导
    \item \textbf{计算复杂度}：某些注意力机制（如Non-Local）计算开销大
    \item \textbf{内存占用}：部分注意力机制需要额外内存存储注意力图
    \item \textbf{优化困难}：某些注意力机制可能引入优化不稳定性
    \item \textbf{任务依赖}：不同任务可能需要不同的注意力机制
\end{enumerate}
\end{block}

\begin{alertblock}{优化中的问题}
在训练带有注意力机制的网络时：
\begin{itemize}
    \item \textbf{梯度不稳定}：Sigmoid激活函数在饱和区梯度接近0
    \item \textbf{注意力崩溃}：所有注意力权重趋向于相等
    \item \textbf{过拟合}：注意力机制可能学习到数据中的噪声
\end{itemize}

解决方法：
\begin{itemize}
    \item \textbf{梯度裁剪}：防止梯度爆炸
    \item \textbf{L2正则化}：防止注意力权重过大
    \item \textbf{Dropout}：在注意力计算中应用Dropout
    \item \textbf{学习率调度}：使用适当的学习率和调度策略
\end{itemize}
\end{alertblock}

\subsection{未来研究方向}

\begin{block}{未来可能的方向}
\begin{enumerate}
    \item \textbf{自动化设计}：使用神经架构搜索（NAS）自动设计注意力机制
    \item \textbf{多模态注意力}：结合视觉、语言、音频的注意力机制
    \item \textbf{高效注意力}：设计更轻量、更快速的注意力机制
    \item \textbf{可解释注意力}：提高注意力机制的可解释性和可信度
    \item \textbf{理论基础}：建立注意力机制的统一理论框架
\end{enumerate}
\end{block}

\begin{exampleblock}{最新研究进展（2024-2025）}
\begin{itemize}
    \item \textbf{轻量级注意力}：MobileViT Attention、Efficient Attention
    \item \textbf{动态注意力}：根据输入自适应调整注意力结构
    \textbf{跨尺度注意力}：处理多尺度特征图的注意力机制
    \item \textbf{注意力蒸馏}：将大模型的注意力知识迁移到小模型
\end{itemize}
\end{exampleblock}

\section{实践指南：如何在项目中使用注意力机制}

\subsection{选择合适的注意力机制}

根据项目需求选择：

\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{场景} & \textbf{推荐方案} \\
\midrule
图像分类（通用） & CBAM或SE-Net \\
目标检测（实时性要求高） & SE-Net（轻量） \\
语义分割（高精度要求） & CBAM + Non-Local \\
移动端应用 & Coordinate Attention或SE（reduction ratio大） \\
医学图像分析 & CBAM（准确率优先） \\
遥感图像处理 & Dual Attention \\
\bottomrule
\end{tabular}
\caption{注意力机制选择指南}
\end{table}

\subsection{超参数调优}

关键超参数及其调优：

\begin{block}{关键超参数}
\begin{enumerate}
    \item \textbf{降维比率$r$}（SE-Net, CBAM）
        \begin{itemize}
            \item 小$r$（4-8）：性能更好，计算量大
            \item 大$r$（32-64）：计算高效，可能损失性能
            \item 推荐：16（默认），或针对不同层使用不同$r$
        \end{itemize}
    \item \textbf{卷积核大小}（空间注意力）
        \begin{itemize}
            \item 3×3：计算快，感受野小
            \item 7×7：计算稍慢，感受野大
            \item 推荐：7×7（第一层），3×3（深层）
        \end{itemize}
    \item \textbf{插入位置}（CBAM）
        \begin{itemize}
            \item 残差块前：增强特征表示
            \item 残差块后：直接调整输出
            \item 推荐：残差块后（CBAM原论文）
        \end{itemize}
\end{enumerate}
\end{block}

\subsection{常见问题与解决方案}

\begin{alertblock}{常见问题}
\begin{enumerate}
    \item \textbf{内存不足}
        \begin{itemize}
            \item 减少batch size
            \item 使用gradient checkpointing
            \item 选择更轻量的注意力机制
        \end{itemize}
    \item \textbf{训练不稳定}
        \begin{itemize}
            \item 降低学习率
            \item 使用梯度裁剪
            \item 检查初始化
        \end{itemize}
    \item \textbf{没有性能提升}
        \begin{itemize}
            \item 调整$r$值
            \item 检查实现是否正确
            \item 确保训练充分
            \item 可视化注意力权重
        \end{itemize}
    \item \textbf{推理速度慢}
        \begin{itemize}
            \item 使用TensorRT优化
            \item 减少注意力模块数量
            \item 使用INT8量化
        \end{itemize}
\end{enumerate}
\end{alertblock}

\section{总结}

本文全面介绍了注意力机制在CNN中的应用，从基础的通道注意力（SE-Net）到混合注意力（CBAM），再到其他先进的注意力机制。关键要点：

\begin{block}{核心要点}
\begin{enumerate}
    \item \textbf{注意力机制的本质}：让神经网络能够动态地关注重要特征，忽略无关信息
    \item \textbf{通道注意力}：通过全局池化和全连接网络学习通道权重
    \item \textbf{空间注意力}：通过空间位置编码关注重要区域
    \item \textbf{混合注意力}：结合通道和空间注意力，实现更全面的特征选择
    \item \textbf{性能提升}：在多种任务上取得显著性能提升，同时保持低计算开销
    \item \textbf{实际应用}：已广泛应用于图像分类、目标检测、语义分割等任务
\end{enumerate}
\end{block}

注意力机制是深度学习发展的重要里程碑，它不仅提升了模型性能，更重要的是为模型提供了更强的可解释性。随着研究的深入，我们期待看到更多创新性的注意力机制被提出，推动人工智能技术的发展。

\begin{alertblock}{给学习者的建议}
\begin{enumerate}
    \item \textbf{动手实践}：亲自实现SE-Net和CBAM，理解每一步的计算
    \item \textbf{可视化分析}：使用注意力可视化工具观察模型的注意力分布
    \item \textbf{调参实验}：尝试不同的$r$值、插入位置等超参数
    \item \textbf{阅读论文}：关注最新的注意力机制研究进展
    \item \textbf{实际应用}：将注意力机制应用到自己的项目中
\end{enumerate}
\end{alertblock}

\begin{thebibliography}{9}

\bibitem{hu2017squeeze}
Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu, "Squeeze-and-excitation networks," \textit{IEEE Transactions on Pattern Analysis and Machine Intelligence}, vol. 42, no. 8, pp. 2011--2023, Aug. 2020.

\bibitem{woo2018cbam}
Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon, "CBAM: Convolutional block attention module," in \textit{Proceedings of the European Conference on Computer Vision (ECCV)}, Munich, Germany, Sep. 2018, pp. 3--19.

\bibitem{zhang2019self}
Han Zhang, Ian Goodfellow, Dimitrios Metaxas, and Augustus Odena, "Self-attention generative adversarial networks," in \textit{International Conference on Machine Learning}, Long Beach, CA, USA, Jun. 2019, pp. 7354--7363.

\bibitem{wang2017residual}
Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, and Xiaoou Tang, "Residual attention network for image classification," in \textit{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, Honolulu, HI, USA, Jul. 2017, pp. 3156--3164.

\bibitem{fu2019dual}
Jun Fu, Jing Liu, Haijie Tian, Zhiwei Fang, and Lingqing Shen, "Dual attention network for scene segmentation," in \textit{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, Long Beach, CA, USA, Jun. 2019, pp. 3146--3154.

\bibitem{wang2018non}
Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He, "Non-local neural networks," in \textit{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, Salt Lake City, UT, USA, Jun. 2018, pp. 7794--7803.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin, "Attention is all you need," in \textit{Advances in Neural Information Processing Systems 30}, Long Beach, CA, USA, Dec. 2017, pp. 5998--6008.

\bibitem{hou2021coordinate}
Qibin Hou, Ma-Mi Zhai, Dacheng Tao, and Xian-Song, "Coordinate attention for efficient mobile network design," in \textit{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, Nashville, TN, USA, Jun. 2021, pp. 13708--13717.

\end{thebibliography}

\newpage
\appendix

\section{代码实现附录}

本附录提供了文中提到的所有注意力机制的完整PyTorch实现代码。

\subsection{SE块实现 (se\_block.py)}

\begin{lstlisting}[language=Python, caption=SE块的完整实现]
import torch
import torch.nn as nn

class SEBlock(nn.Module):
    """
    Squeeze-and-Excitation Block

    Args:
        channels (int): 输入特征图的通道数
        reduction (int): 降维比率，默认为16
    """
    def __init__(self, channels, reduction=16):
        super(SEBlock, self).__init__()

        # Squeeze: 全局平均池化
        self.avg_pool = nn.Adaptive\mathrm{AvgPool}2d(1)

        # Excitation: 两层全连接网络
        self.fc = nn.Sequential(
            # 降维: C -> C/r
            nn.Linear(channels, channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            # 升维: C/r -> C
            nn.Linear(channels // reduction, channels, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        """
        Args:
            x: 输入特征图 [B, C, H, W]
        Returns:
            out: 加权后的特征图 [B, C, H, W]
        """
        batch, channels, _, _ = x.size()

        # Squeeze: [B, C, H, W] -> [B, C, 1, 1]
        y = self.avg_pool(x)

        # Flatten: [B, C, 1, 1] -> [B, C]
        y = y.view(batch, channels)

        # Excitation: [B, C] -> [B, C]
        y = self.fc(y)

        # Reshape: [B, C] -> [B, C, 1, 1]
        y = y.view(batch, channels, 1, 1)

        # Scale: 特征重标定
        return x * y.expand_as(x)


# 使用示例
if __name__ == "__main__":
    # 创建SE块
    se_block = SEBlock(channels=64, reduction=16)

    # 测试输入
    x = torch.randn(2, 64, 56, 56)  # [B, C, H, W]
    output = se_block(x)

    print(f"输入形状: {x.shape}")
    print(f"输出形状: {output.shape}")
    print(f"参数数量: {sum(p.numel() for p in se_block.parameters())}")
\end{lstlisting}

\subsection{空间注意力实现 (spatial\_attention.py)}

\begin{lstlisting}[language=Python, caption=空间注意力模块的完整实现]
import torch
import torch.nn as nn

class SpatialAttention(nn.Module):
    """
    Spatial Attention Module

    Args:
        kernel_size (int): 卷积核大小，默认为7
    """
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()

        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'
        padding = 3 if kernel_size == 7 else 1

        # 7x7卷积层
        self.conv = nn.Conv2d(
            in_channels=2,  # 平均池化 + 最大池化
            out_channels=1,
            kernel_size=kernel_size,
            padding=padding,
            bias=False
        )
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        """
        Args:
            x: 输入特征图 [B, C, H, W]
        Returns:
            out: 加权后的特征图 [B, C, H, W]
        """
        # 沿通道维度进行池化
        avg_out = torch.mean(x, dim=1, keepdim=True)  # [B, 1, H, W]
        max_out, _ = torch.max(x, dim=1, keepdim=True)  # [B, 1, H, W]

        # Concatenate: [B, 2, H, W]
        pooled = torch.cat([avg_out, max_out], dim=1)

        # 卷积 + Sigmoid: [B, 2, H, W] -> [B, 1, H, W]
        attention = self.sigmoid(self.conv(pooled))

        # 空间加权
        return x * attention


# 使用示例
if __name__ == "__main__":
    # 创建空间注意力模块
    spatial_attn = SpatialAttention(kernel_size=7)

    # 测试输入
    x = torch.randn(2, 64, 56, 56)
    output = spatial_attn(x)

    print(f"输入形状: {x.shape}")
    print(f"输出形状: {output.shape}")
    print(f"参数数量: {sum(p.numel() for p in spatial_attn.parameters())}")
\end{lstlisting}

\subsection{CBAM完整实现 (cbam.py)}

\begin{lstlisting}[language=Python, caption=CBAM模块的完整实现]
import torch
import torch.nn as nn

class ChannelAttention(nn.Module):
    """
    Channel Attention Module (CBAM的通道注意力子模块)

    Args:
        channels (int): 输入特征图的通道数
        reduction (int): 降维比率，默认为16
    """
    def __init__(self, channels, reduction=16):
        super(ChannelAttention, self).__init__()

        self.avg_pool = nn.Adaptive\mathrm{AvgPool}2d(1)
        self.max_pool = nn.Adaptive\mathrm{MaxPool}2d(1)

        # 共享的\mathrm{MLP}
        self.\mathrm{MLP} = nn.Sequential(
            nn.Conv2d(channels, channels // reduction, 1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels // reduction, channels, 1, bias=False)
        )
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # 平均池化分支
        avg_out = self.\mathrm{MLP}(self.avg_pool(x))
        # 最大池化分支
        max_out = self.\mathrm{MLP}(self.max_pool(x))
        # 合并并激活
        out = self.sigmoid(avg_out + max_out)
        return x * out


class SpatialAttention(nn.Module):
    """
    Spatial Attention Module (CBAM的空间注意力子模块)

    Args:
        kernel_size (int): 卷积核大小，默认为7
    """
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()

        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'
        padding = 3 if kernel_size == 7 else 1

        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        pooled = torch.cat([avg_out, max_out], dim=1)
        attention = self.sigmoid(self.conv(pooled))
        return x * attention


class CBAM(nn.Module):
    """
    Convolutional Block Attention Module

    Args:
        channels (int): 输入特征图的通道数
        reduction (int): 通道注意力的降维比率，默认为16
        kernel_size (int): 空间注意力的卷积核大小，默认为7
    """
    def __init__(self, channels, reduction=16, kernel_size=7):
        super(CBAM, self).__init__()

        self.channel_attention = ChannelAttention(channels, reduction)
        self.spatial_attention = SpatialAttention(kernel_size)

    def forward(self, x):
        # 先应用通道注意力
        out = self.channel_attention(x)
        # 再应用空间注意力
        out = self.spatial_attention(out)
        return out


# 使用示例
if __name__ == "__main__":
    # 创建CBAM模块
    cbam = CBAM(channels=64, reduction=16, kernel_size=7)

    # 测试输入
    x = torch.randn(2, 64, 56, 56)
    output = cbam(x)

    print(f"输入形状: {x.shape}")
    print(f"输出形状: {output.shape}")
    print(f"参数数量: {sum(p.numel() for p in cbam.parameters())}")
\end{lstlisting}

\subsection{ResNet-CBAM集成 (resnet\_cbam.py)}

\begin{lstlisting}[language=Python, caption=CBAM集成到ResNet的完整实现]
import torch
import torch.nn as nn
from cbam import CBAM

class CBAMBottleneck(nn.Module):
    """
    ResNet Bottleneck块 + CBAM

    Args:
        in_channels (int): 输入通道数
        out_channels (int): 输出通道数
        stride (int): 步长
        reduction (int): CBAM的降维比率
    """
    expansion = 4

    def __init__(self, in_channels, out_channels, stride=1,
                 downsample=None, reduction=16):
        super(CBAMBottleneck, self).__init__()

        # 1x1卷积降维
        self.conv1 = nn.Conv2d(in_channels, out_channels,
                               kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)

        # 3x3卷积
        self.conv2 = nn.Conv2d(out_channels, out_channels,
                               kernel_size=3, stride=stride,
                               padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)

        # 1x1卷积升维
        self.conv3 = nn.Conv2d(out_channels,
                               out_channels * self.expansion,
                               kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)

        # CBAM模块
        self.cbam = CBAM(out_channels * self.expansion, reduction)

        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        identity = x

        # 主路径
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        # 应用CBAM
        out = self.cbam(out)

        # 残差连接
        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out


class ResNetCBAM(nn.Module):
    """
    ResNet with CBAM

    Args:
        block: 残差块类型
        layers: 每个stage的块数量
        num_classes: 分类类别数
    """
    def __init__(self, block, layers, num_classes=1000):
        super(ResNetCBAM, self).__init__()

        self.in_channels = 64

        # 初始卷积层
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2,
                               padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.\mathrm{MaxPool} = nn.\mathrm{MaxPool}2d(kernel_size=3, stride=2, padding=1)

        # 4个残差stage
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)

        # 分类头
        self.\mathrm{AvgPool} = nn.Adaptive\mathrm{AvgPool}2d((1, 1))
        self.fc = nn.Linear(512 * block.expansion, num_classes)

        # 权重初始化
        self._initialize_weights()

    def _make_layer(self, block, out_channels, blocks, stride=1):
        downsample = None
        if stride != 1 or self.in_channels != out_channels * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.in_channels, out_channels * block.expansion,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels * block.expansion),
            )

        layers = []
        layers.append(block(self.in_channels, out_channels, stride, downsample))
        self.in_channels = out_channels * block.expansion

        for _ in range(1, blocks):
            layers.append(block(self.in_channels, out_channels))

        return nn.Sequential(*layers)

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out',
                                        nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.\mathrm{MaxPool}(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = self.\mathrm{AvgPool}(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)

        return x


def resnet50_cbam(num_classes=1000):
    """构建ResNet-50-CBAM模型"""
    return ResNetCBAM(CBAMBottleneck, [3, 4, 6, 3], num_classes)


def resnet101_cbam(num_classes=1000):
    """构建ResNet-101-CBAM模型"""
    return ResNetCBAM(CBAMBottleneck, [3, 4, 23, 3], num_classes)


# 使用示例
if __name__ == "__main__":
    model = resnet50_cbam(num_classes=1000)
    x = torch.randn(2, 3, 224, 224)
    output = model(x)

    print(f"输入形状: {x.shape}")
    print(f"输出形状: {output.shape}")
    print(f"总参数数量: {sum(p.numel() for p in model.parameters()):,}")
\end{lstlisting}

\subsection{注意力机制应用示例 (attention\_applications.py)}

\begin{lstlisting}[language=Python, caption=注意力机制在不同任务中的应用]
import torch
import torch.nn as nn
from cbam import CBAM
from resnet_cbam import CBAMBottleneck

# ============== 1. 图像分类应用 ==============
class ImageClassifier_CBAM(nn.Module):
    """使用CBAM的图像分类器"""
    def __init__(self, num_classes=10):
        super(ImageClassifier_CBAM, self).__init__()

        self.features = nn.Sequential(
            # Stage 1
            nn.Conv2d(3, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            CBAM(64),
            nn.\mathrm{MaxPool}2d(2, 2),

            # Stage 2
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            CBAM(128),
            nn.\mathrm{MaxPool}2d(2, 2),

            # Stage 3
            nn.Conv2d(128, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            CBAM(256),
            nn.\mathrm{MaxPool}2d(2, 2),
        )

        self.classifier = nn.Sequential(
            nn.Adaptive\mathrm{AvgPool}2d((1, 1)),
            nn.Flatten(),
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x


# ============== 2. 目标检测应用 ==============
class DetectionBackbone_CBAM(nn.Module):
    """使用CBAM的目标检测骨干网络"""
    def __init__(self):
        super(DetectionBackbone_CBAM, self).__init__()

        # 多尺度特征提取
        self.stage1 = nn.Sequential(
            nn.Conv2d(3, 64, 7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.\mathrm{MaxPool}2d(3, stride=2, padding=1),
        )

        self.stage2 = self._make_stage(64, 128, 2)
        self.stage3 = self._make_stage(128, 256, 2)
        self.stage4 = self._make_stage(256, 512, 2)

    def _make_stage(self, in_ch, out_ch, num_blocks):
        layers = []
        layers.append(nn.Conv2d(in_ch, out_ch, 3, stride=2, padding=1))
        layers.append(nn.BatchNorm2d(out_ch))
        layers.append(nn.ReLU(inplace=True))

        for _ in range(num_blocks):
            layers.append(nn.Conv2d(out_ch, out_ch, 3, padding=1))
            layers.append(nn.BatchNorm2d(out_ch))
            layers.append(nn.ReLU(inplace=True))
            layers.append(CBAM(out_ch))

        return nn.Sequential(*layers)

    def forward(self, x):
        # 返回多尺度特征
        c1 = self.stage1(x)
        c2 = self.stage2(c1)
        c3 = self.stage3(c2)
        c4 = self.stage4(c3)

        return [c2, c3, c4]  # 用于FPN


# ============== 3. 语义分割应用 ==============
class SegmentationDecoder_CBAM(nn.Module):
    """使用CBAM的语义分割解码器"""
    def __init__(self, in_channels, num_classes):
        super(SegmentationDecoder_CBAM, self).__init__()

        # 上采样路径
        self.up1 = nn.ConvTranspose2d(in_channels, 256, 2, stride=2)
        self.cbam1 = CBAM(256)
        self.conv1 = self._conv_block(256, 256)

        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)
        self.cbam2 = CBAM(128)
        self.conv2 = self._conv_block(128, 128)

        self.up3 = nn.ConvTranspose2d(128, 64, 2, stride=2)
        self.cbam3 = CBAM(64)
        self.conv3 = self._conv_block(64, 64)

        # 最终分类层
        self.final = nn.Conv2d(64, num_classes, 1)

    def _conv_block(self, in_ch, out_ch):
        return nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
        )

    def forward(self, x):
        x = self.up1(x)
        x = self.cbam1(x)
        x = self.conv1(x)

        x = self.up2(x)
        x = self.cbam2(x)
        x = self.conv2(x)

        x = self.up3(x)
        x = self.cbam3(x)
        x = self.conv3(x)

        x = self.final(x)
        return x


# ============== 4. 注意力可视化工具 ==============
class AttentionVisualizer:
    """注意力权重可视化工具"""

    @staticmethod
    def visualize_channel_attention(model, input_tensor, layer_name):
        """可视化通道注意力权重"""
        activations = {}

        def hook_fn(module, input, output):
            if hasattr(module, 'channel_attention'):
                # 获取通道注意力权重
                attn = module.channel_attention(input[0])
                activations['channel_weights'] = attn.detach()

        # 注册hook
        for name, module in model.named_modules():
            if name == layer_name:
                module.register_forward_hook(hook_fn)

        # 前向传播
        with torch.no_grad():
            _ = model(input_tensor)

        return activations.get('channel_weights', None)

    @staticmethod
    def visualize_spatial_attention(model, input_tensor, layer_name):
        """可视化空间注意力图"""
        activations = {}

        def hook_fn(module, input, output):
            if hasattr(module, 'spatial_attention'):
                # 获取空间注意力图
                attn = module.spatial_attention(input[0])
                activations['spatial_map'] = attn.detach()

        # 注册hook
        for name, module in model.named_modules():
            if name == layer_name:
                module.register_forward_hook(hook_fn)

        # 前向传播
        with torch.no_grad():
            _ = model(input_tensor)

        return activations.get('spatial_map', None)


# 使用示例
if __name__ == "__main__":
    # 1. 图像分类
    print("=" * 50)
    print("图像分类应用")
    classifier = ImageClassifier_CBAM(num_classes=10)
    x = torch.randn(2, 3, 32, 32)
    out = classifier(x)
    print(f"输入: {x.shape}, 输出: {out.shape}")

    # 2. 目标检测
    print("\n" + "=" * 50)
    print("目标检测应用")
    detector_backbone = DetectionBackbone_CBAM()
    x = torch.randn(2, 3, 640, 640)
    features = detector_backbone(x)
    print(f"输入: {x.shape}")
    for i, feat in enumerate(features):
        print(f"特征{i+1}: {feat.shape}")

    # 3. 语义分割
    print("\n" + "=" * 50)
    print("语义分割应用")
    seg_decoder = SegmentationDecoder_CBAM(512, num_classes=21)
    x = torch.randn(2, 512, 28, 28)
    out = seg_decoder(x)
    print(f"输入: {x.shape}, 输出: {out.shape}")
\end{lstlisting}

\subsection{性能基准测试 (benchmark.py)}

\begin{lstlisting}[language=Python, caption=注意力机制性能基准测试]
import torch
import torch.nn as nn
import time
import numpy as np
from thop import profile, clever_format
from resnet_cbam import resnet50_cbam
from torchvision.models import resnet50

class AttentionBenchmark:
    """注意力机制性能基准测试工具"""

    @staticmethod
    def count_parameters(model):
        """统计模型参数数量"""
        return sum(p.numel() for p in model.parameters())

    @staticmethod
    def measure_inference_time(model, input_size=(1, 3, 224, 224),
                              num_iterations=100, warmup=10):
        """测量推理时间"""
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = model.to(device)
        model.eval()

        # 创建测试输入
        x = torch.randn(input_size).to(device)

        # 预热
        with torch.no_grad():
            for _ in range(warmup):
                _ = model(x)

        # 同步GPU
        if torch.cuda.is_available():
            torch.cuda.synchronize()

        # 测量时间
        times = []
        with torch.no_grad():
            for _ in range(num_iterations):
                start = time.time()
                _ = model(x)

                if torch.cuda.is_available():
                    torch.cuda.synchronize()

                times.append(time.time() - start)

        times = np.array(times)
        return {
            'mean': times.mean() * 1000,  # ms
            'std': times.std() * 1000,
            'min': times.min() * 1000,
            'max': times.max() * 1000
        }

    @staticmethod
    def measure_flops(model, input_size=(1, 3, 224, 224)):
        """测量FLOPs"""
        x = torch.randn(input_size)
        flops, params = profile(model, inputs=(x,), verbose=False)
        flops, params = clever_format([flops, params], "%.3f")
        return {'FLOPs': flops, 'Params': params}

    @staticmethod
    def compare_models(models_dict, input_size=(1, 3, 224, 224)):
        """比较多个模型的性能"""
        results = {}

        for name, model in models_dict.items():
            print(f"\n测试模型: {name}")
            print("-" * 50)

            # 参数数量
            params = AttentionBenchmark.count_parameters(model)
            print(f"参数数量: {params:,}")

            # FLOPs
            flops_info = AttentionBenchmark.measure_flops(model, input_size)
            print(f"FLOPs: {flops_info['FLOPs']}")

            # 推理时间
            time_info = AttentionBenchmark.measure_inference_time(
                model, input_size
            )
            print(f"推理时间: {time_info['mean']:.2f} +/- "
                  f"{time_info['std']:.2f} ms")

            results[name] = {
                'params': params,
                'flops': flops_info['FLOPs'],
                'inference_time_ms': time_info['mean'],
                'inference_std_ms': time_info['std']
            }

        return results


# 使用示例
if __name__ == "__main__":
    print("=" * 60)
    print("注意力机制性能基准测试")
    print("=" * 60)

    # 准备模型
    models = {
        'ResNet-50': resnet50(pretrained=False),
        'ResNet-50-CBAM': resnet50_cbam(num_classes=1000),
    }

    # 运行基准测试
    results = AttentionBenchmark.compare_models(
        models,
        input_size=(1, 3, 224, 224)
    )

    # 打印对比结果
    print("\n" + "=" * 60)
    print("性能对比总结")
    print("=" * 60)

    baseline_params = results['ResNet-50']['params']
    baseline_time = results['ResNet-50']['inference_time_ms']

    for name, metrics in results.items():
        print(f"\n{name}:")
        print(f"  参数数量: {metrics['params']:,} "
              f"({metrics['params']/baseline_params:.2%})")
        print(f"  FLOPs: {metrics['flops']}")
        print(f"  推理时间: {metrics['inference_time_ms']:.2f} ms "
              f"({metrics['inference_time_ms']/baseline_time:.2%})")
\end{lstlisting}

\subsection{训练脚本示例 (train\_cbam.py)}

\begin{lstlisting}[language=Python, caption=使用CBAM的完整训练脚本]
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from resnet_cbam import resnet50_cbam
import wandb  # 可选：用于实验跟踪


def train_one_epoch(model, train_loader, criterion, optimizer,
                    device, epoch):
    """训练一个epoch"""
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for batch_idx, (inputs, targets) in enumerate(train_loader):
        inputs, targets = inputs.to(device), targets.to(device)

        # 前向传播
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)

        # 反向传播
        loss.backward()
        optimizer.step()

        # 统计
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()

        if batch_idx % 100 == 0:
            print(f'Epoch: {epoch} | Batch: {batch_idx}/{len(train_loader)} '
                  f'| Loss: {running_loss/(batch_idx+1):.3f} '
                  f'| Acc: {100.*correct/total:.2f}%')

    return running_loss / len(train_loader), 100. * correct / total


def validate(model, val_loader, criterion, device):
    """验证"""
    model.eval()
    val_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, targets in val_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, targets)

            val_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

    return val_loss / len(val_loader), 100. * correct / total


def main():
    # 设置
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    num_epochs = 100
    batch_size = 128
    learning_rate = 0.1

    # 数据预处理
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465),
                           (0.2023, 0.1994, 0.2010)),
    ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465),
                           (0.2023, 0.1994, 0.2010)),
    ])

    # 数据加载
    train_dataset = datasets.CIFAR10(root='./data', train=True,
                                     download=True,
                                     transform=transform_train)
    test_dataset = datasets.CIFAR10(root='./data', train=False,
                                    download=True,
                                    transform=transform_test)

    train_loader = DataLoader(train_dataset, batch_size=batch_size,
                             shuffle=True, num_workers=4)
    test_loader = DataLoader(test_dataset, batch_size=batch_size,
                            shuffle=False, num_workers=4)

    # 模型
    model = resnet50_cbam(num_classes=10).to(device)

    # 损失函数和优化器
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=learning_rate,
                         momentum=0.9, weight_decay=5e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,
                                                     T_max=num_epochs)

    # 训练循环
    best_acc = 0.0
    for epoch in range(num_epochs):
        print(f'\nEpoch: {epoch+1}/{num_epochs}')

        # 训练
        train_loss, train_acc = train_one_epoch(
            model, train_loader, criterion, optimizer, device, epoch
        )

        # 验证
        val_loss, val_acc = validate(model, test_loader, criterion, device)

        # 学习率调度
        scheduler.step()

        print(f'Train Loss: {train_loss:.3f} | Train Acc: {train_acc:.2f}%')
        print(f'Val Loss: {val_loss:.3f} | Val Acc: {val_acc:.2f}%')

        # 保存最佳模型
        if val_acc > best_acc:
            best_acc = val_acc
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'accuracy': best_acc,
            }, 'best_cbam_model.pth')
            print(f'Saved best model with accuracy: {best_acc:.2f}%')

    print(f'\nTraining completed! Best accuracy: {best_acc:.2f}%')


if __name__ == "__main__":
    main()
\end{lstlisting}

\end{document}
