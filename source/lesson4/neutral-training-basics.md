# 神经网络训练基础

## 损失函数

损失函数衡量模型预测与真实标签之间的差异。对于分类任务，常用的损失函数包括：

```{math}
\text{交叉熵损失: } L = -\frac{1}{N} \sum_{i=1}^N \sum_{c=1}^C y_{i,c} \log(\hat{y}_{i,c})
```

```{math}
\text{均方误差: } L = \frac{1}{N} \sum_{i=1}^N (y_i - \hat{y}_i)^2
```

## 优化算法

梯度下降及其变体是训练神经网络的主要优化方法：

```{list-table} 优化算法比较
:header-rows: 1
:widths: 25 25 25 25

* - **算法**
  - **更新规则**
  - **优点**
  - **缺点**
* - SGD
  - $\theta \leftarrow \theta - \eta \nabla_\theta J(\theta)$
  - 简单，内存效率高
  - 收敛慢，易陷入局部最优
* - Momentum
  - $v \leftarrow \gamma v + \eta \nabla_\theta J(\theta)$  
  - 加速收敛，减少振荡
  - 需要调整动量参数
* - Adam
  - 复杂自适应更新
  - 自适应学习率，通常效果好
  - 超参数敏感
```

## 过拟合与正则化

过拟合是指模型在训练集上表现良好但在测试集上表现差的现象。常用正则化技术包括：

```{admonition} 正则化技术
:class: tip

1. **L1/L2正则化**：在损失函数中添加参数范数惩罚项
2. **Dropout**：训练时随机丢弃部分神经元
3. **数据增强**：通过对训练数据进行变换增加数据多样性
4. **早停法**：在验证集性能下降时停止训练
5. **批归一化**：规范化层输入，加速训练并提高稳定性
```
