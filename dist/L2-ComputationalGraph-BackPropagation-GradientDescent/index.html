<!DOCTYPE html>
<html lang="en-US">
<head>
<meta charset="UTF-8" />
<meta name="generator" content="LaTeX Lwarp package" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title> 计算图、反向传播与梯度下降：深度学习的数学基础 </title>
<link rel="stylesheet" type="text/css" href="lwarp.css" />

<script>
// Lwarp MathJax emulation code
//
// Based on code by Davide P. Cervone.
// Equation numbering: https://github.com/mathjax/MathJax/issues/2427
// Starred and ifnextchar macros: https://github.com/mathjax/MathJax/issues/2428
// \left, \right delimiters: https://github.com/mathjax/MathJax/issues/2535
//
// Modified by Brian Dunn to adjust equation numbering and add subequations.
//
// LaTeX can use \seteqnumber{subequations?}{section}{number} before each equation.
// subequations? is 0 usually, 1 if inside subequations.
// section is a string printed as-is, or empty.
// number is auto-incremented by MathJax between equations.
//
MathJax = {
    subequations: "0",
    section: "",
    loader: {
         load: ['[tex]/tagformat', '[tex]/textmacros'],
    },
    startup: {
         ready() {
             //      These would be replaced by import commands if you wanted to make
             //      a proper extension.
             const Configuration = MathJax._.input.tex.Configuration.Configuration;
             const CommandMap = MathJax._.input.tex.SymbolMap.CommandMap;
             const Macro = MathJax._.input.tex.Symbol.Macro;
             const TexError = MathJax._.input.tex.TexError.default;
             const ParseUtil = MathJax._.input.tex.ParseUtil.default;
             const expandable = MathJax._.util.Options.expandable;


             //      Insert the replacement string into the TeX string, and check
             //      that there haven't been too many maxro substitutions (prevents
             //      infinite loops).
             const useArgument = (parser, text) => {
                 parser.string = ParseUtil.addArgs(parser, text, parser.string.slice(parser.i));
                 parser.i = 0;
                 if (++parser.macroCount > parser.configuration.options.maxMacros) {
                     throw new TexError('MaxMacroSub1',
                     'MathJax maximum macro substitution count exceeded; ' +
                     'is there a recursive macro call?');
                 }
             }


             //      Create the command map for:
             //          \ifstar, \ifnextchar, \ifblank, \ifstrequal, \gsub, \seteqnumber
             new CommandMap('Lwarp-macros', {
                 ifstar: 'IfstarFunction',
                 ifnextchar: 'IfnextcharFunction',
                 ifblank: 'IfblankFunction',
                 ifstrequal: 'IfstrequalFunction',
                 gsubstitute: 'GsubstituteFunction',
                 seteqnumber: 'SeteqnumberFunction'
             }, {
                 //      This function implements an ifstar macro.
                 IfstarFunction(parser, name) {
                     const resultstar = parser.GetArgument(name);
                     const resultnostar = parser.GetArgument(name);
                     const star = parser.GetStar();                        // true if there is a *
                     useArgument(parser, star ? resultstar : resultnostar);
                 },


                 //      This function implements an ifnextchar macro.
                 IfnextcharFunction(parser, name) {
                     let whichchar = parser.GetArgument(name);
                     if (whichchar.match(/^(?:0x[0-9A-F]+|[0-9]+)$/i)) {
                         // $ syntax highlighting
                         whichchar = String.fromCodePoint(parseInt(whichchar));
                     }
                     const resultnextchar = parser.GetArgument(name);
                     const resultnotnextchar = parser.GetArgument(name);
                     const gotchar = (parser.GetNext() === whichchar);
                     useArgument(parser, gotchar ? resultnextchar : resultnotnextchar);
                 },


                 // This function implements an ifblank macro.
                 IfblankFunction(parser, name) {
                     const blankarg = parser.GetArgument(name);
                     const resultblank = parser.GetArgument(name);
                     const resultnotblank = parser.GetArgument(name);
                     const isblank = (blankarg.trim() == "");
                     useArgument(parser, isblank ? resultblank : resultnotblank);
                 },


                 // This function implements an ifstrequal macro.
                 IfstrequalFunction(parser, name) {
                     const strequalfirst = parser.GetArgument(name);
                     const strequalsecond = parser.GetArgument(name);
                     const resultequal = parser.GetArgument(name);
                     const resultnotequal = parser.GetArgument(name);
                     const isequal = (strequalfirst == strequalsecond);
                     useArgument(parser, isequal ? resultequal : resultnotequal);
                 },


                 // This function implements a gsub macro.
                 GsubstituteFunction(parser, name) {
                     const gsubfirst = parser.GetArgument(name);
                     const gsubsecond = parser.GetArgument(name);
                     const gsubthird = parser.GetArgument(name);
                     let gsubresult=gsubfirst.replace(gsubsecond, gsubthird);
                     useArgument(parser, gsubresult);
                 },


                 //      This function modifies the equation numbers.
                 SeteqnumberFunction(parser, name) {
                         //   Get the macro parameters
                         const star = parser.GetStar();                       // true if there is a *
                         const optBrackets = parser.GetBrackets(name);        // contents of optional brackets
                         const newsubequations = parser.GetArgument(name);       // the subequations argument
                         const neweqsection = parser.GetArgument(name);       // the eq section argument
                         const neweqnumber = parser.GetArgument(name);        // the eq number argument
                         MathJax.config.subequations=newsubequations ;        // a string with boolean meaning
                         MathJax.config.section=neweqsection ;                // a string with numeric meaning
                         parser.tags.counter = parser.tags.allCounter = neweqnumber ;
                 }


             });


             //      Create the Lwarp-macros package
             Configuration.create('Lwarp-macros', {
                 handler: {macro: ['Lwarp-macros']}
             });


             MathJax.startup.defaultReady();


             // For forward references:
             MathJax.startup.input[0].preFilters.add(({math}) => {
                 if (math.inputData.recompile){
                         MathJax.config.subequations = math.inputData.recompile.subequations;
                         MathJax.config.section = math.inputData.recompile.section;
                 }
             });
             MathJax.startup.input[0].postFilters.add(({math}) => {
                 if (math.inputData.recompile){
                         math.inputData.recompile.subequations = MathJax.config.subequations;
                         math.inputData.recompile.section = MathJax.config.section;
                 }
             });


                 // For \left, \right with unicode-math:
                 const {DelimiterMap} = MathJax._.input.tex.SymbolMap;
                 const {Symbol} = MathJax._.input.tex.Symbol;
                 const {MapHandler} = MathJax._.input.tex.MapHandler;
                 const delimiter = MapHandler.getMap('delimiter');
                 delimiter.add('\\lBrack', new Symbol('\\lBrack', '\u27E6'));
                 delimiter.add('\\rBrack', new Symbol('\\rBrack', '\u27E7'));
                 delimiter.add('\\lAngle', new Symbol('\\lAngle', '\u27EA'));
                 delimiter.add('\\rAngle', new Symbol('\\rAngle', '\u27EB'));
                 delimiter.add('\\lbrbrak', new Symbol('\\lbrbrak', '\u2772'));
                 delimiter.add('\\rbrbrak', new Symbol('\\rbrbrak', '\u2773'));
                 delimiter.add('\\lbag', new Symbol('\\lbag', '\u27C5'));
                 delimiter.add('\\rbag', new Symbol('\\rbag', '\u27C6'));
                 delimiter.add('\\llparenthesis', new Symbol('\\llparenthesis', '\u2987'));
                 delimiter.add('\\rrparenthesis', new Symbol('\\rrparenthesis', '\u2988'));
                 delimiter.add('\\llangle', new Symbol('\\llangle', '\u2989'));
                 delimiter.add('\\rrangle', new Symbol('\\rrangle', '\u298A'));
                 delimiter.add('\\Lbrbrak', new Symbol('\\Lbrbrak', '\u27EC'));
                 delimiter.add('\\Rbrbrak', new Symbol('\\Rbrbrak', '\u27ED'));
                 delimiter.add('\\lBrace', new Symbol('\\lBrace', '\u2983'));
                 delimiter.add('\\rBrace', new Symbol('\\rBrace', '\u2984'));
                 delimiter.add('\\lParen', new Symbol('\\lParen', '\u2985'));
                 delimiter.add('\\rParen', new Symbol('\\rParen', '\u2986'));
                 delimiter.add('\\lbrackubar', new Symbol('\\lbrackubar', '\u298B'));
                 delimiter.add('\\rbrackubar', new Symbol('\\rbrackubar', '\u298C'));
                 delimiter.add('\\lbrackultick', new Symbol('\\lbrackultick', '\u298D'));
                 delimiter.add('\\rbracklrtick', new Symbol('\\rbracklrtick', '\u298E'));
                 delimiter.add('\\lbracklltick', new Symbol('\\lbracklltick', '\u298F'));
                 delimiter.add('\\rbrackurtick', new Symbol('\\rbrackurtick', '\u2990'));
                 delimiter.add('\\langledot', new Symbol('\\langledot', '\u2991'));
                 delimiter.add('\\rangledot', new Symbol('\\rangledot', '\u2992'));
                 delimiter.add('\\lparenless', new Symbol('\\lparenless', '\u2993'));
                 delimiter.add('\\rparengtr', new Symbol('\\rparengtr', '\u2994'));
                 delimiter.add('\\Lparengtr', new Symbol('\\Lparengtr', '\u2995'));
                 delimiter.add('\\Rparenless', new Symbol('\\Rparenless', '\u2996'));
                 delimiter.add('\\lblkbrbrak', new Symbol('\\lblkbrbrak', '\u2997'));
                 delimiter.add('\\rblkbrbrak', new Symbol('\\rblkbrbrak', '\u2998'));
                 delimiter.add('\\lvzigzag', new Symbol('\\lvzigzag', '\u29D8'));
                 delimiter.add('\\rvzigzag', new Symbol('\\rvzigzag', '\u29D9'));
                 delimiter.add('\\Lvzigzag', new Symbol('\\Lvzigzag', '\u29DA'));
                 delimiter.add('\\Rvzigzag', new Symbol('\\Rvzigzag', '\u29DB'));
                 delimiter.add('\\lcurvyangle', new Symbol('\\lcurvyangle', '\u29FC'));
                 delimiter.add('\\rcurvyangle', new Symbol('\\rcurvyangle', '\u29FD'));
                 delimiter.add('\\Vvert', new Symbol('\\Vvert', '\u2980'));
         }       // ready
    },           // startup


    tex: {
         packages: {'[+]': ['tagformat', 'Lwarp-macros', 'textmacros']},
         tags: "ams",
                 tagformat: {
                         number: function (n) {
                              if(MathJax.config.subequations==0)
                                     return(MathJax.config.section + n);
                              else
                                     return(MathJax.config.section + String.fromCharCode(96+n));
                         },
                 },
    }
}
</script>


<script
         id="MathJax-script"
         src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"
></script>


</head>
<body>
<!--|Using lwarp|document.html|-->



<div class="bodywithoutsidetoc">



<main class="bodycontainer">



<section class="textbody">

<a id="document-autofile-0"></a>

<!--MathJax customizations:-->
<div data-nosnippet
         style="display:none"
>

\(\newcommand{\footnotename}{footnote}\)

\(\def \LWRfootnote {1}\)

\(\newcommand {\footnote }[2][\LWRfootnote ]{{}^{\mathrm {#1}}}\)

\(\newcommand {\footnotemark }[1][\LWRfootnote ]{{}^{\mathrm {#1}}}\)

\(\let \LWRorighspace \hspace \)

\(\renewcommand {\hspace }{\ifstar \LWRorighspace \LWRorighspace }\)

\(\newcommand {\TextOrMath }[2]{#2}\)

\(\newcommand {\mathnormal }[1]{{#1}}\)

\(\newcommand \ensuremath [1]{#1}\)

\(\newcommand {\LWRframebox }[2][]{\fbox {#2}} \newcommand {\framebox }[1][]{\LWRframebox } \)

\(\newcommand {\setlength }[2]{}\)

\(\newcommand {\addtolength }[2]{}\)

\(\newcommand {\setcounter }[2]{}\)

\(\newcommand {\addtocounter }[2]{}\)

\(\newcommand {\arabic }[1]{}\)

\(\newcommand {\number }[1]{}\)

\(\newcommand {\noalign }[1]{\text {#1}\notag \\}\)

\(\newcommand {\cline }[1]{}\)

\(\newcommand {\directlua }[1]{\text {(directlua)}}\)

\(\newcommand {\luatexdirectlua }[1]{\text {(directlua)}}\)

\(\newcommand {\protect }{}\)

\(\def \LWRabsorbnumber #1 {}\)

\(\def \LWRabsorbquotenumber &quot;#1 {}\)

\(\newcommand {\LWRabsorboption }[1][]{}\)

\(\newcommand {\LWRabsorbtwooptions }[1][]{\LWRabsorboption }\)

\(\def \mathchar {\ifnextchar &quot;\LWRabsorbquotenumber \LWRabsorbnumber }\)

\(\def \mathcode #1={\mathchar }\)

\(\let \delcode \mathcode \)

\(\let \delimiter \mathchar \)

\(\def \oe {\unicode {x0153}}\)

\(\def \OE {\unicode {x0152}}\)

\(\def \ae {\unicode {x00E6}}\)

\(\def \AE {\unicode {x00C6}}\)

\(\def \aa {\unicode {x00E5}}\)

\(\def \AA {\unicode {x00C5}}\)

\(\def \o {\unicode {x00F8}}\)

\(\def \O {\unicode {x00D8}}\)

\(\def \l {\unicode {x0142}}\)

\(\def \L {\unicode {x0141}}\)

\(\def \ss {\unicode {x00DF}}\)

\(\def \SS {\unicode {x1E9E}}\)

\(\def \dag {\unicode {x2020}}\)

\(\def \ddag {\unicode {x2021}}\)

\(\def \P {\unicode {x00B6}}\)

\(\def \copyright {\unicode {x00A9}}\)

\(\def \pounds {\unicode {x00A3}}\)

\(\let \LWRref \ref \)

\(\renewcommand {\ref }{\ifstar \LWRref \LWRref }\)

\( \newcommand {\multicolumn }[3]{#3}\)

\(\require {textcomp}\)

\(\newcommand {\intertext }[1]{\text {#1}\notag \\}\)

\(\let \Hat \hat \)

\(\let \Check \check \)

\(\let \Tilde \tilde \)

\(\let \Acute \acute \)

\(\let \Grave \grave \)

\(\let \Dot \dot \)

\(\let \Ddot \ddot \)

\(\let \Breve \breve \)

\(\let \Bar \bar \)

\(\let \Vec \vec \)

\(\newcommand {\toprule }[1][]{\hline }\)

\(\let \midrule \toprule \)

\(\let \bottomrule \toprule \)

\(\def \LWRbooktabscmidruleparen (#1)#2{}\)

\(\newcommand {\LWRbooktabscmidrulenoparen }[1]{}\)

\(\newcommand {\cmidrule }[1][]{\ifnextchar (\LWRbooktabscmidruleparen \LWRbooktabscmidrulenoparen }\)

\(\newcommand {\morecmidrules }{}\)

\(\newcommand {\specialrule }[3]{\hline }\)

\(\newcommand {\addlinespace }[1][]{}\)

\(\newcommand {\tcbset }[1]{}\)

\(\newcommand {\tcbsetforeverylayer }[1]{}\)

\(\newcommand {\tcbox }[2][]{\boxed {\text {#2}}}\)

\(\newcommand {\tcboxfit }[2][]{\boxed {#2}}\)

\(\newcommand {\tcblower }{}\)

\(\newcommand {\tcbline }{}\)

\(\newcommand {\tcbtitle }{}\)

\(\newcommand {\tcbsubtitle [2][]{\mathrm {#2}}}\)

\(\newcommand {\tcboxmath }[2][]{\boxed {#2}}\)

\(\newcommand {\tcbhighmath }[2][]{\boxed {#2}}\)

</div>

<a id="document-autopage-1"></a>
<div class="titlepage">

<h1><b> 计算图、反向传播与梯度下降：深度学习的数学基础 </b></h1>



<div class="author">



<div class="oneauthor">

<p>
Anson, 深度学习社 &#x2003;Cooperated with <kbd>Kimi K2 0905</kbd>
</p>
</div>

</div>



<div class="titledate">

<p>
2025 年 10 月 16 日
</p>
</div>

</div>
<div class="abstract">



<div class="abstracttitle"> 摘要 </div>

<p>
本文深入探讨了深度学习中三个核心数学概念：计算图、反向传播和梯度下降。计算图提供了数学表达式的可视化表示，为自动微分奠定基础；反向传播算法基于链式法则高效计算梯度；梯度下降利用梯度信息优化模型参数。文章
从基础概念出发，通过详细的数学推导、直观的图示和实际例子，系统性地阐述了这些技术在深度学习中的重要作用。我们不仅介绍了基本理论，还探讨了现代深度学习框架如何实现这些机制，以及它们在实际应用中的优化策略。
</p>
</div>
<!--
...... section 目录......
-->
<h4 id="autosec-4">目录</h4>
<a id="document-autopage-4"></a>




<nav class="toc">

<p>
<a href="document.html#autosec-5" class="tocsection" >
<span class="sectionnumber">1</span>&#x2003;引言</a>
</p>



<p>
<a href="document.html#autosec-6" class="tocsubsection" >
<span class="sectionnumber">1.1</span>&#x2003;机器学习的基本问题</a>
</p>



<p>
<a href="document.html#autosec-7" class="tocsubsection" >
<span class="sectionnumber">1.2</span>&#x2003;为什么需要计算图、反向传播和梯度下降？</a>
</p>



<p>
<a href="document.html#autosec-8" class="tocsection" >
<span class="sectionnumber">2</span>&#x2003;计算图基础</a>
</p>



<p>
<a href="document.html#autosec-9" class="tocsubsection" >
<span class="sectionnumber">2.1</span>&#x2003;什么是计算图？</a>
</p>



<p>
<a href="document.html#autosec-10" class="tocsubsection" >
<span class="sectionnumber">2.2</span>&#x2003;计算图的基本元素</a>
</p>



<p>
<a href="document.html#autosec-14" class="tocsubsubsection" >
<span class="sectionnumber">2.2.1</span>&#x2003;节点类型</a>
</p>



<p>
<a href="document.html#autosec-15" class="tocsubsubsection" >
<span class="sectionnumber">2.2.2</span>&#x2003;边的语义</a>
</p>



<p>
<a href="document.html#autosec-16" class="tocsubsection" >
<span class="sectionnumber">2.3</span>&#x2003;计算图的构建规则</a>
</p>



<p>
<a href="document.html#autosec-17" class="tocsubsection" >
<span class="sectionnumber">2.4</span>&#x2003;复杂模型的表示</a>
</p>



<p>
<a href="document.html#autosec-21" class="tocsection" >
<span class="sectionnumber">3</span>&#x2003;前向传播</a>
</p>



<p>
<a href="document.html#autosec-22" class="tocsubsection" >
<span class="sectionnumber">3.1</span>&#x2003;前向传播的基本概念</a>
</p>



<p>
<a href="document.html#autosec-23" class="tocsubsection" >
<span class="sectionnumber">3.2</span>&#x2003;拓扑排序的重要性</a>
</p>



<p>
<a href="document.html#autosec-24" class="tocsubsection" >
<span class="sectionnumber">3.3</span>&#x2003;前向传播的详细步骤</a>
</p>



<p>
<a href="document.html#autosec-29" class="tocsubsection" >
<span class="sectionnumber">3.4</span>&#x2003;前向传播的计算复杂度</a>
</p>



<p>
<a href="document.html#autosec-32" class="tocsection" >
<span class="sectionnumber">4</span>&#x2003;反向传播</a>
</p>



<p>
<a href="document.html#autosec-33" class="tocsubsection" >
<span class="sectionnumber">4.1</span>&#x2003;为什么需要反向传播？</a>
</p>



<p>
<a href="document.html#autosec-34" class="tocsubsection" >
<span class="sectionnumber">4.2</span>&#x2003;链式法则：反向传播的数学基础</a>
</p>



<p>
<a href="document.html#autosec-35" class="tocsubsection" >
<span class="sectionnumber">4.3</span>&#x2003;反向传播的直观理解</a>
</p>



<p>
<a href="document.html#autosec-36" class="tocsubsection" >
<span class="sectionnumber">4.4</span>&#x2003;反向传播的直观理解</a>
</p>



<p>
<a href="document.html#autosec-37" class="tocsubsection" >
<span class="sectionnumber">4.5</span>&#x2003;反向传播的算法步骤</a>
</p>



<p>
<a href="document.html#autosec-38" class="tocsubsection" >
<span class="sectionnumber">4.6</span>&#x2003;详细计算示例</a>
</p>



<p>
<a href="document.html#autosec-39" class="tocsubsection" >
<span class="sectionnumber">4.7</span>&#x2003;计算图可视化：前向传播与反向传播对比</a>
</p>



<p>
<a href="document.html#autosec-45" class="tocsubsection" >
<span class="sectionnumber">4.8</span>&#x2003;常见操作的梯度计算规则</a>
</p>



<p>
<a href="document.html#autosec-48" class="tocsection" >
<span class="sectionnumber">5</span>&#x2003;梯度下降</a>
</p>



<p>
<a href="document.html#autosec-49" class="tocsubsection" >
<span class="sectionnumber">5.1</span>&#x2003;优化问题的数学表述</a>
</p>



<p>
<a href="document.html#autosec-50" class="tocsubsection" >
<span class="sectionnumber">5.2</span>&#x2003;梯度下降的基本思想</a>
</p>



<p>
<a href="document.html#autosec-51" class="tocsubsection" >
<span class="sectionnumber">5.3</span>&#x2003;梯度下降的数学表达</a>
</p>



<p>
<a href="document.html#autosec-52" class="tocsubsection" >
<span class="sectionnumber">5.4</span>&#x2003;学习率的重要性</a>
</p>



<p>
<a href="document.html#autosec-60" class="tocsubsection" >
<span class="sectionnumber">5.5</span>&#x2003;梯度下降的变体</a>
</p>



<p>
<a href="document.html#autosec-63" class="tocsubsection" >
<span class="sectionnumber">5.6</span>&#x2003;梯度下降的收敛性分析</a>
</p>



<p>
<a href="document.html#autosec-64" class="tocsection" >
<span class="sectionnumber">6</span>&#x2003;综合实例：训练线性回归模型</a>
</p>



<p>
<a href="document.html#autosec-65" class="tocsubsection" >
<span class="sectionnumber">6.1</span>&#x2003;问题设定</a>
</p>



<p>
<a href="document.html#autosec-66" class="tocsubsection" >
<span class="sectionnumber">6.2</span>&#x2003;计算图表示</a>
</p>



<p>
<a href="document.html#autosec-70" class="tocsubsection" >
<span class="sectionnumber">6.3</span>&#x2003;详细训练过程</a>
</p>



<p>
<a href="document.html#autosec-71" class="tocsubsection" >
<span class="sectionnumber">6.4</span>&#x2003;多轮迭代的收敛过程</a>
</p>



<p>
<a href="document.html#autosec-74" class="tocsection" >
<span class="sectionnumber">7</span>&#x2003;现代深度学习框架的实现</a>
</p>



<p>
<a href="document.html#autosec-75" class="tocsubsection" >
<span class="sectionnumber">7.1</span>&#x2003;自动微分系统</a>
</p>



<p>
<a href="document.html#autosec-76" class="tocsubsection" >
<span class="sectionnumber">7.2</span>&#x2003;PyTorch 实现示例</a>
</p>



<p>
<a href="document.html#autosec-78" class="tocsubsection" >
<span class="sectionnumber">7.3</span>&#x2003;优化器的抽象</a>
</p>



<p>
<a href="document.html#autosec-80" class="tocsection" >
<span class="sectionnumber">8</span>&#x2003;高级主题与优化策略</a>
</p>



<p>
<a href="document.html#autosec-81" class="tocsubsection" >
<span class="sectionnumber">8.1</span>&#x2003;梯度消失与梯度爆炸</a>
</p>



<p>
<a href="document.html#autosec-82" class="tocsubsection" >
<span class="sectionnumber">8.2</span>&#x2003;二阶优化方法</a>
</p>



<p>
<a href="document.html#autosec-85" class="tocsubsection" >
<span class="sectionnumber">8.3</span>&#x2003;自适应优化算法</a>
</p>



<p>
<a href="document.html#autosec-86" class="tocsection" >
<span class="sectionnumber">9</span>&#x2003;结论与展望</a>
</p>



<p>
<a href="document.html#autosec-87" class="tocsubsection" >
<span class="sectionnumber">9.1</span>&#x2003;核心概念总结</a>
</p>



<p>
<a href="document.html#autosec-88" class="tocsubsection" >
<span class="sectionnumber">9.2</span>&#x2003;技术发展历程</a>
</p>



<p>
<a href="document.html#autosec-89" class="tocsubsection" >
<span class="sectionnumber">9.3</span>&#x2003;未来发展方向</a>
</p>



<p>
<a href="document.html#autosec-90" class="tocsubsection" >
<span class="sectionnumber">9.4</span>&#x2003;实际应用建议</a>
</p>
</nav>
<!--
...... section 引言......
-->
<h4 id="autosec-5"><span class="sectionnumber">1&#x2003;</span>引言</h4>
<a id="document-autopage-5"></a>
<!--
...... subsection 机器学习的基本问题......
-->
<h5 id="autosec-6"><span class="sectionnumber">1.1&#x2003;</span>机器学习的基本问题</h5>
<a id="document-autopage-6"></a>



<p>
在深入探讨计算图、反向传播和梯度下降之前，我们首先需要理解机器学习的基本问题。机器学习的目标是让计算机从数据中学习模式，而不需要明确编程每一个规则。这种学习过程的核心在于如何自动调整模型的参数，使其能够
更好地拟合数据并做出准确的预测。
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
机器学习的核心挑战
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b> 模型复杂度：</b> 现代机器学习模型（特别是深度神经网络）包含数百万甚至数十亿个参数
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 优化难度：</b> 在高维参数空间中寻找最优解是一个极具挑战性的问题
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 计算效率：</b> 需要高效的算法来处理大规模数据和复杂模型
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 泛化能力：</b> 确保模型在未见过的数据上表现良好
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 为什么需要计算图、反向传播和梯度下降？......
-->
<h5 id="autosec-7"><span class="sectionnumber">1.2&#x2003;</span>为什么需要计算图、反向传播和梯度下降？</h5>
<a id="document-autopage-7"></a>



<p>
这三个概念构成了现代深度学习的基础设施：
</p>

<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker">1.</span> <b> 计算图：</b> 将复杂的数学表达式可视化为图结构，便于理解和计算
</p>


</li>
<li>


<p>
<span class="listmarker">2.</span> <b> 反向传播：</b> 高效计算梯度（偏导数）的算法，告诉我们每个参数对最终输出的影响程度
</p>


</li>
<li>


<p>
<span class="listmarker">3.</span> <b> 梯度下降：</b> 利用梯度信息优化参数的迭代算法，逐步减小损失函数
</p>
</li>
</ul>



<div
      class="tcolorbox"
      style=" border: 1px solid #BF0040; background: #FFFFFF; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #BF0040; border-bottom: 1px solid #BF0040; background: #BF0040; color: #FFFFFF; "
>

<p>
历史背景
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
这些概念的发展历程可以追溯到 20 世纪 60‑80 年代：
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> 1960s：链式法则在神经网络中的应用首次被提出
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 1970s：反向传播算法的雏形出现
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 1986 年：Rumelhart、Hinton 和 Williams 重新发现并推广了反向传播算法
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 1990s 至今：这些技术成为所有深度学习框架的核心
</p>
</li>
</ul>

</div>

</div>
<!--
...... section 计算图基础......
-->
<h4 id="autosec-8"><span class="sectionnumber">2&#x2003;</span>计算图基础</h4>
<a id="document-autopage-8"></a>
<!--
...... subsection 什么是计算图？......
-->
<h5 id="autosec-9"><span class="sectionnumber">2.1&#x2003;</span>什么是计算图？</h5>
<a id="document-autopage-9"></a>



<p>
计算图（Computational Graph）是一种将数学表达式表示为有向图的数据结构。图中的节点代表变量或操作，边代表数据流动的方向。这种表示方法不仅直观，而且为自动微分提供了数学基础。
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
计算图的正式定义
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
计算图 \(G = (V, E)\) 是一个有向图，其中：
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> \(V\) 是节点集合，包括：
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">–</span> <b> 输入节点：</b> 表示变量或常数
</p>


</li>
<li>


<p>
<span class="listmarker">–</span> <b> 操作节点：</b> 表示数学运算（加法、乘法、函数等）
</p>


</li>
<li>


<p>
<span class="listmarker">–</span> <b> 输出节点：</b> 表示最终计算结果
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker">‧</span> \(E\) 是边集合，表示数据依赖关系
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 每个节点 \(v \in V\) 都有一个对应的值 \(val(v)\)
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 边 \((u, v) \in E\) 表示节点 \(u\) 的值是节点 \(v\) 的输入
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 计算图的基本元素......
-->
<h5 id="autosec-10"><span class="sectionnumber">2.2&#x2003;</span>计算图的基本元素</h5>
<a id="document-autopage-10"></a>



<figure id="autoid-1" class="figure ">
<div class="center">

<p>
<span
      id="lateximage-document-1"
      class="lateximagesource"
><!--
x       y       y




    +       ×       f (x, y)
--><img
   src="./Assets//image-1.svg"
   alt="(-tikz-&nbsp;diagram)"
   style=""
   class="lateximage"
></span>
</p>



<div class="figurecaption">


图&nbsp;1: 计算图示例：\(f(x,y) = (x+y) \times y\)

</div>

</div>

</figure>
<!--
...... subsubsection 节点类型......
-->
<h6 id="autosec-14"><span class="sectionnumber">2.2.1&#x2003;</span>节点类型</h6>
<a id="document-autopage-14"></a>




<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
输入节点（Input Nodes）
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> 表示模型的输入变量或常数
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 通常是计算图的起点
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 在训练过程中，这些节点的值会被赋予具体数值
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 示例：\(x\), \(y\), \(w\), \(b\) 等
</p>
</li>
</ul>

</div>

</div>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
操作节点（Operation Nodes）
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> 表示数学运算或函数
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 接收一个或多个输入，产生一个输出
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 示例：加法 (+)、乘法 (\(\times \))、sigmoid(\(\sigma \))、ReLU 等
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 每个操作节点都对应一个前向计算和反向传播规则
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsubsection 边的语义......
-->
<h6 id="autosec-15"><span class="sectionnumber">2.2.2&#x2003;</span>边的语义</h6>
<a id="document-autopage-15"></a>



<p>
边在计算图中承载着重要的语义信息：
</p>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b> 数据流：</b> 表示数值从源节点流向目标节点
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 依赖关系：</b> 显示计算过程中的依赖关系
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 梯度传播：</b> 在反向传播中，梯度沿着边反向流动
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 计算顺序：</b> 边的方向决定了计算的拓扑顺序
</p>
</li>
</ul>
<!--
...... subsection 计算图的构建规则......
-->
<h5 id="autosec-16"><span class="sectionnumber">2.3&#x2003;</span>计算图的构建规则</h5>
<a id="document-autopage-16"></a>



<p>
构建计算图需要遵循特定的规则，以确保计算的正确性和效率：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
构建原则
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker">1.</span> <b> 无环性：</b> 计算图必须是有向无环图（DAG），避免循环依赖
</p>


</li>
<li>


<p>
<span class="listmarker">2.</span> <b> 完整性：</b> 每个操作节点的所有输入都必须有明确的来源
</p>


</li>
<li>


<p>
<span class="listmarker">3.</span> <b> 一致性：</b> 数据类型和维度必须匹配
</p>


</li>
<li>


<p>
<span class="listmarker">4.</span> <b> 可微分性：</b> 所有操作节点必须支持前向计算和反向传播
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 复杂模型的表示......
-->
<h5 id="autosec-17"><span class="sectionnumber">2.4&#x2003;</span>复杂模型的表示</h5>
<a id="document-autopage-17"></a>



<p>
现代深度学习模型可以表示为极其复杂的计算图。以逻辑回归为例：
</p>

<figure id="autoid-2" class="figure ">
<div class="center">

<p>
<span
      id="lateximage-document-2"
      class="lateximagesource"
><!--
x1

x2   ×
         +   σ   ŷ
w1   ×

w2

b
--><img
      src="./Assets//image-2.svg"
      alt="(-tikz-&nbsp;diagram)"
      style=""
      class="lateximage"
></span>
</p>



<div class="figurecaption">


图&nbsp;2: 逻辑回归模型的计算图表示：\(\hat {y} = \sigma (w_1x_1 + w_2x_2 + b)\)

</div>

</div>

</figure>

<p>
这个计算图清晰地展示了逻辑回归模型的完整计算过程：
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> 输入特征 \(x_1\), \(x_2\) 与权重 \(w_1\), \(w_2\) 相乘
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 加权和加上偏置项 \(b\)
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 通过 sigmoid 函数 \(\sigma \) 得到概率预测 \(\hat {y}\)
</p>
</li>
</ul>
<!--
...... section 前向传播......
-->
<h4 id="autosec-21"><span class="sectionnumber">3&#x2003;</span>前向传播</h4>
<a id="document-autopage-21"></a>
<!--
...... subsection 前向传播的基本概念......
-->
<h5 id="autosec-22"><span class="sectionnumber">3.1&#x2003;</span>前向传播的基本概念</h5>
<a id="document-autopage-22"></a>



<p>
前向传播（Forward Propagation）是指沿着计算图从输入节点到输出节点的计算过程。在这个过程中，每个节点的值都基于其输入节点的值计算得出。
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
前向传播的数学定义
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
对于计算图 \(G = (V, E)\)，前向传播算法可以形式化描述为：
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker">1.</span> 对每个输入节点 \(v \in V_{input}\)，设置 \(val(v)\) 为给定的输入值
</p>


</li>
<li>


<p>
<span class="listmarker">2.</span> 按照拓扑顺序遍历所有节点 \(v \in V\)
</p>


</li>
<li>


<p>
<span class="listmarker">3.</span> 对于每个操作节点 \(v\)，计算：
</p>
<p>
\[val(v) = f_v(val(u_1), val(u_2), \dots , val(u_k))\]
</p>
<p>
其中 \(u_1, u_2, \dots , u_k\) 是 \(v\) 的所有前驱节点，\(f_v\) 是节点 \(v\) 对应的操作函数
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 拓扑排序的重要性......
-->
<h5 id="autosec-23"><span class="sectionnumber">3.2&#x2003;</span>拓扑排序的重要性</h5>
<a id="document-autopage-23"></a>



<p>
拓扑排序确保在计算一个节点的值之前，其所有输入节点的值都已经计算完毕。这是前向传播正确性的关键保证。
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #008080; background: #FFFFFF; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #008080; border-bottom: 1px solid #008080; background: #008080; color: #FFFFFF; "
>

<p>
拓扑排序示例
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
考虑计算图：\(c = a + b\), \(e = c \times d\)
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> 有效拓扑顺序：\([a, b, d, c, e]\) 或 \([b, a, d, c, e]\)
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 无效顺序：\([c, a, b, d, e]\)（计算 \(c\) 时 \(a\) 和 \(b\) 还未计算）
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 前向传播的详细步骤......
-->
<h5 id="autosec-24"><span class="sectionnumber">3.3&#x2003;</span>前向传播的详细步骤</h5>
<a id="document-autopage-24"></a>



<p>
让我们通过具体例子详细分析前向传播的过程：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #008080; background: #FFFFFF; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #008080; border-bottom: 1px solid #008080; background: #008080; color: #FFFFFF; "
>

<p>
详细计算
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
假设 \(x=3\), \(y=2\)：
</p>
<div class="multicols">

<p>
<b> 计算步骤：</b>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker">1.</span> 初始化输入节点：\(x=3\), \(y=2\)
</p>


</li>
<li>


<p>
<span class="listmarker">2.</span> 计算 \(u = x + y = 3 + 2 = 5\)
</p>


</li>
<li>


<p>
<span class="listmarker">3.</span> 计算 \(f = u \times y = 5 \times 2 = 10\)
</p>


</li>
<li>


<p>
<span class="listmarker">4.</span> 得到最终结果：\(f(3,2) = 10\)
</p>
</li>
</ul>
<div class="center">

<p>
<span
      id="lateximage-document-3"
      class="lateximagesource"
><!--
x=3         y=2              y=2

  3         2                2

                5
      u=5           f = 10

  f (x, y) = (x + y) × y
--><img
   src="./Assets//image-3.svg"
   alt="(-tikz-&nbsp;diagram)"
   style=""
   class="lateximage"
></span>
</p>
</div>

</div>

</div>

</div>
<!--
...... subsection 前向传播的计算复杂度......
-->
<h5 id="autosec-29"><span class="sectionnumber">3.4&#x2003;</span>前向传播的计算复杂度</h5>
<a id="document-autopage-29"></a>



<p>
前向传播的计算复杂度取决于计算图的结构和操作类型：
</p>

<figure id="autoid-3" class="table ">
<div class="center">
<table>

<tr style="display:none"><th>.</th></tr>


<tr class="tbrule">
<td class="tdl"><b> 操作类型 </b></td>
<td class="tdl"><b> 时间复杂度 </b></td>
<td class="tdl"><b> 空间复杂度 </b></td>
</tr>


<tr class="hline">
<td class="tdl"> 基本算术运算 </td>
<td class="tdl">\(O(1)\)</td>
<td class="tdl">\(O(1)\)</td>
</tr>


<tr>
<td class="tdl"> 矩阵乘法 </td>
<td class="tdl">\(O(n^3)\)</td>
<td class="tdl">\(O(n^2)\)</td>
</tr>


<tr>
<td class="tdl"> 卷积操作 </td>
<td class="tdl">\(O(k \cdot n^2)\)</td>
<td class="tdl">\(O(n^2)\)</td>
</tr>


<tr>
<td class="tdl"> 激活函数 </td>
<td class="tdl">\(O(n)\)</td>
<td class="tdl">\(O(n)\)</td>
</tr>


<tr class="tbrule" aria-hidden="true">
<td class="tdl"></td>
<td class="tdl"></td>
<td class="tdl"></td>
</tr>
</table>



<div class="figurecaption">


表&nbsp;1: 常见操作的计算复杂度

</div>

</div>

</figure>

<p>
其中 \(n\) 表示输入尺寸，\(k\) 表示卷积核大小。
</p>
<!--
...... section 反向传播......
-->
<h4 id="autosec-32"><span class="sectionnumber">4&#x2003;</span>反向传播</h4>
<a id="document-autopage-32"></a>
<!--
...... subsection 为什么需要反向传播？......
-->
<h5 id="autosec-33"><span class="sectionnumber">4.1&#x2003;</span>为什么需要反向传播？</h5>
<a id="document-autopage-33"></a>



<p>
在机器学习中，我们不仅需要计算模型的输出，更重要的是要知道如何调整参数来改进模型性能。反向传播（Backpropagation）算法解决了这个核心问题。
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
梯度计算的重要性
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b> 参数优化：</b> 梯度告诉我们每个参数应该向哪个方向调整
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 影响分析：</b> 了解每个参数对最终输出的贡献程度
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 训练效率：</b> 高效的梯度计算大大加速了模型训练
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 理论保证：</b> 为各种优化算法提供数学基础
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 链式法则：反向传播的数学基础......
-->
<h5 id="autosec-34"><span class="sectionnumber">4.2&#x2003;</span>链式法则：反向传播的数学基础</h5>
<a id="document-autopage-34"></a>



<p>
链式法则是多元微积分中的基本定理，也是反向传播算法的核心：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
链式法则（Chain Rule）
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
如果 \(z = f(y)\) 且 \(y = g(x)\)，那么：
</p>

<p>
\[\frac {dz}{dx} = \frac {dz}{dy} \times \frac {dy}{dx}\]
</p>

<p>
对于多元函数，如果 \(z = f(y_1, y_2, \dots , y_n)\) 且每个 \(y_i = g_i(x)\)，那么：
</p>

<p>
\[\frac {\partial z}{\partial x} = \sum _{i=1}^n \frac {\partial z}{\partial y_i} \times \frac {\partial y_i}{\partial x}\]
</p>

</div>

</div>
<!--
...... subsection 反向传播的直观理解......
-->
<h5 id="autosec-35"><span class="sectionnumber">4.3&#x2003;</span>反向传播的直观理解</h5>
<a id="document-autopage-35"></a>



<p>
反向传播可以理解为” 责任分配” 的过程：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #008080; background: #FFFFFF; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #008080; border-bottom: 1px solid #008080; background: #008080; color: #FFFFFF; "
>

<p>
责任分配类比
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
想象一个公司组织：
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b> 前向传播：</b> 信息从基层员工流向总经理
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 反向传播：</b> 当公司业绩不佳时，责任从总经理反向分配到各个部门和个人
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 梯度：</b> 每个部门或个人对最终结果的” 责任程度”
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 参数更新：</b> 根据责任程度调整工作方式
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 反向传播的直观理解......
-->
<h5 id="autosec-36"><span class="sectionnumber">4.4&#x2003;</span>反向传播的直观理解</h5>
<a id="document-autopage-36"></a>



<p>
反向传播可以理解为” 责任分配” 的过程：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #008080; background: #FFFFFF; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #008080; border-bottom: 1px solid #008080; background: #008080; color: #FFFFFF; "
>

<p>
责任分配类比
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
想象一个公司组织：
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b> 前向传播：</b> 信息从基层员工流向总经理
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 反向传播：</b> 当公司业绩不佳时，责任从总经理反向分配到各个部门和个人
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 梯度：</b> 每个部门或个人对最终结果的” 责任程度”
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 参数更新：</b> 根据责任程度调整工作方式
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 反向传播的算法步骤......
-->
<h5 id="autosec-37"><span class="sectionnumber">4.5&#x2003;</span>反向传播的算法步骤</h5>
<a id="document-autopage-37"></a>



<p>
反向传播算法可以形式化地描述为以下步骤：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
反向传播算法
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker">1.</span> <b> 前向传播：</b> 计算所有节点的值并保存中间结果
</p>


</li>
<li>


<p>
<span class="listmarker">2.</span> <b> 初始化梯度：</b> 设置输出节点的梯度为 \(\frac {\partial L}{\partial L} = 1\)
</p>


</li>
<li>


<p>
<span class="listmarker">3.</span> <b> 反向遍历：</b> 按照逆拓扑顺序遍历所有节点
</p>


</li>
<li>


<p>
<span class="listmarker">4.</span> <b> 梯度计算：</b> 对于每个节点 \(v\)，计算其对损失函数的梯度：
</p>
<p>
\[\frac {\partial L}{\partial v} = \sum _{u \in \text {后继}(v)} \frac {\partial L}{\partial u} \times \frac {\partial u}{\partial v}\]
</p>
</li>
<li>


<p>
<span class="listmarker">5.</span> <b> 参数梯度：</b> 对于参数节点，计算 \(\frac {\partial L}{\partial \theta }\) 用于后续优化
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 详细计算示例......
-->
<h5 id="autosec-38"><span class="sectionnumber">4.6&#x2003;</span>详细计算示例</h5>
<a id="document-autopage-38"></a>



<p>
让我们通过具体例子详细分析反向传播的过程：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #008080; background: #FFFFFF; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #008080; border-bottom: 1px solid #008080; background: #008080; color: #FFFFFF; "
>

<p>
反向传播计算
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
假设 \(x=3\), \(y=2\)，我们要计算 \(\frac {\partial f}{\partial x}\) 和 \(\frac {\partial f}{\partial y}\)：
</p>

<p>
<b> 前向传播结果：</b>
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> \(u = x + y = 3 + 2 = 5\)
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> \(f = u \times y = 5 \times 2 = 10\)
</p>
</li>
</ul>

<p>
<b> 反向传播计算：</b>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker">1.</span> 初始化：\(\frac {\partial f}{\partial f} = 1\)
</p>


</li>
<li>


<p>
<span class="listmarker">2.</span> 计算 \(\frac {\partial f}{\partial u} = \frac {\partial }{\partial u}(u \times y) = y = 2\)
</p>


</li>
<li>


<p>
<span class="listmarker">3.</span> 计算 \(\frac {\partial f}{\partial y} = \frac {\partial }{\partial y}(u \times y) + \frac {\partial f}{\partial u} \times \frac {\partial u}{\partial y} = u +
2 \times 1 = 5 + 2 = 7\)
</p>


</li>
<li>


<p>
<span class="listmarker">4.</span> 计算 \(\frac {\partial f}{\partial x} = \frac {\partial f}{\partial u} \times \frac {\partial u}{\partial x} = 2 \times 1 = 2\)
</p>
</li>
</ul>

<p>
<b> 结果：</b> \(\frac {\partial f}{\partial x} = 2\), \(\frac {\partial f}{\partial y} = 7\)
</p>
</div>

</div>
<!--
...... subsection 计算图可视化：前向传播与反向传播对比......
-->
<h5 id="autosec-39"><span class="sectionnumber">4.7&#x2003;</span>计算图可视化：前向传播与反向传播对比</h5>
<a id="document-autopage-39"></a>



<figure id="autoid-4" class="figure ">
<div class="center">

<p>
<span
      id="lateximage-document-4"
      class="lateximagesource"
><!--
       前向传播值


x=3     y1 = 2      y2 = 2



      u=5     f = 10

 u = x + y1 f = u × y 2
--><img
      src="./Assets//image-4.svg"
      alt="(-tikz-&nbsp;diagram)"
      style=""
      class="lateximage"
></span>
</p>

<p>
<span
      id="lateximage-document-5"
      class="lateximagesource"
><!--
         反向传播梯度



 ∂f              ∂f               ∂f
 ∂x
    =2           ∂y1
                     =2           ∂y2
                                      =5


    ×1




                              ×5
             ×1
         ∂f               ∂f
            =2               =1
         ∂u
                  ×2      ∂f




            ∂x = ∂u · ∂x
反向传播应用链式法则: ∂f   ∂f ∂u
--><img
   src="./Assets//image-5.svg"
   alt="(-tikz-&nbsp;diagram)"
   style=""
   class="lateximage"
></span>
</p>



<div class="figurecaption">


图&nbsp;3: 前向传播与反向传播对比示意图

</div>

</div>

</figure>
<!--
...... subsection 常见操作的梯度计算规则......
-->
<h5 id="autosec-45"><span class="sectionnumber">4.8&#x2003;</span>常见操作的梯度计算规则</h5>
<a id="document-autopage-45"></a>



<p>
在反向传播中，我们需要为每种操作定义梯度计算规则：
</p>

<figure id="autoid-5" class="table ">
<div class="center">
<table>

<tr style="display:none"><th>.</th></tr>


<tr class="tbrule">
<td class="tdl"><b> 操作类型 </b></td>
<td class="tdl"><b> 前向计算 </b></td>
<td class="tdl"><b> 反向梯度 </b></td>
</tr>


<tr class="hline">
<td class="tdl"> 加法 </td>
<td class="tdl">\(z = x + y\)</td>
<td class="tdl">\(\frac {\partial L}{\partial x} = \frac {\partial L}{\partial z}\), \(\frac {\partial L}{\partial y} = \frac {\partial L}{\partial z}\)</td>
</tr>


<tr>
<td class="tdl"> 乘法 </td>
<td class="tdl">\(z = x \times y\)</td>
<td class="tdl">\(\frac {\partial L}{\partial x} = \frac {\partial L}{\partial z} \times y\), \(\frac {\partial L}{\partial y} = \frac {\partial L}{\partial z} \times x\)</td>
</tr>


<tr>
<td class="tdl"> 矩阵乘法 </td>
<td class="tdl">\(Z = X \cdot Y\)</td>
<td class="tdl">\(\frac {\partial L}{\partial X} = \frac {\partial L}{\partial Z} \cdot Y^T\), \(\frac {\partial L}{\partial Y} = X^T \cdot \frac {\partial L}{\partial Z}\)</td>
</tr>


<tr>
<td class="tdl">ReLU</td>
<td class="tdl">\(z = \max (0, x)\)</td>
<td class="tdl">\(\frac {\partial L}{\partial x} = \begin {cases} \frac {\partial L}{\partial z} &amp; \text {if } x &gt; 0 \\ 0 &amp; \text {otherwise} \end {cases}\)</td>
</tr>


<tr>
<td class="tdl">Sigmoid</td>
<td class="tdl">\(z = \sigma (x)\)</td>
<td class="tdl">\(\frac {\partial L}{\partial x} = \frac {\partial L}{\partial z} \times z(1-z)\)</td>
</tr>


<tr class="tbrule" aria-hidden="true">
<td class="tdl"></td>
<td class="tdl"></td>
<td class="tdl"></td>
</tr>
</table>



<div class="figurecaption">


表&nbsp;2: 常见操作的梯度计算规则

</div>

</div>

</figure>
<!--
...... section 梯度下降......
-->
<h4 id="autosec-48"><span class="sectionnumber">5&#x2003;</span>梯度下降</h4>
<a id="document-autopage-48"></a>
<!--
...... subsection 优化问题的数学表述......
-->
<h5 id="autosec-49"><span class="sectionnumber">5.1&#x2003;</span>优化问题的数学表述</h5>
<a id="document-autopage-49"></a>



<p>
在机器学习中，训练模型可以形式化为一个优化问题：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
机器学习优化问题
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
给定训练数据集 \(\mathcal {D} = \{(x_i, y_i)\}_{i=1}^N\)，模型参数 \(\theta \)，损失函数 \(\mathcal {L}\)，我们的目标是：
</p>

<p>
\[\min _{\theta } \mathcal {L}(\theta ) = \frac {1}{N} \sum _{i=1}^N \ell (f(x_i; \theta ), y_i)\]
</p>

<p>
其中 \(f(x; \theta )\) 是模型预测，\(\ell \) 是单个样本的损失函数。
</p>
</div>

</div>
<!--
...... subsection 梯度下降的基本思想......
-->
<h5 id="autosec-50"><span class="sectionnumber">5.2&#x2003;</span>梯度下降的基本思想</h5>
<a id="document-autopage-50"></a>



<p>
梯度下降（Gradient Descent）是最基本的优化算法，其核心思想是利用梯度信息来寻找函数的最小值。
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
梯度下降的直观理解
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
想象你在山上，想要下到山谷：
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b> 当前位置：</b> 当前参数值 \(\theta \)
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 梯度：</b> 最陡的下山方向 \(-\nabla \mathcal {L}(\theta )\)
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 学习率：</b> 步长大小 \(\alpha \)
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 更新：</b> 沿着最陡方向走一小步 \(\theta \leftarrow \theta - \alpha \nabla \mathcal {L}(\theta )\)
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 梯度下降的数学表达......
-->
<h5 id="autosec-51"><span class="sectionnumber">5.3&#x2003;</span>梯度下降的数学表达</h5>
<a id="document-autopage-51"></a>




<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
梯度下降算法
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
对于参数 \(\theta \)，梯度下降的更新规则为：
</p>

<p>
\[\theta _{t+1} = \theta _t - \alpha \nabla \mathcal {L}(\theta _t)\]
</p>

<p>
其中：
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> \(\theta _t\)：第 \(t\) 次迭代的参数值
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> \(\alpha \)：学习率（步长）
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> \(\nabla \mathcal {L}(\theta _t)\)：损失函数在 \(\theta _t\) 处的梯度
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 学习率的重要性......
-->
<h5 id="autosec-52"><span class="sectionnumber">5.4&#x2003;</span>学习率的重要性</h5>
<a id="document-autopage-52"></a>



<p>
学习率是梯度下降中最重要的超参数之一，它控制着参数更新的步长：
</p>
<div class="multicols">



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
学习率太小
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> 收敛速度很慢
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 需要很多迭代
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 可能陷入局部最优
</p>
</li>
</ul>
<div class="center">

<p>
<span
      id="lateximage-document-6"
      class="lateximagesource"
><!--
小步移动
--><img
      src="./Assets//image-6.svg"
      alt="(-tikz-&nbsp;diagram)"
      style=""
      class="lateximage"
></span>
</p>
</div>

</div>

</div>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
学习率太大
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> 可能错过最优解
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 在最优解附近震荡
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 甚至发散（不收敛）
</p>
</li>
</ul>
<div class="center">

<p>
<span
      id="lateximage-document-7"
      class="lateximagesource"
><!--
来回震荡
--><img
      src="./Assets//image-7.svg"
      alt="(-tikz-&nbsp;diagram)"
      style=""
      class="lateximage"
></span>
</p>
</div>

</div>

</div>

</div>



<div
      class="tcolorbox"
      style=" border: 1px solid #BF0040; background: #FFFFFF; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #BF0040; border-bottom: 1px solid #BF0040; background: #BF0040; color: #FFFFFF; "
>

<p>
选择合适的学习率
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> 通常从 0.01, 0.001 等值开始尝试
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 可以随着训练逐渐减小（学习率衰减）
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 使用自适应方法（如 Adam, RMSprop）
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 通过验证集性能来选择最佳学习率
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 梯度下降的变体......
-->
<h5 id="autosec-60"><span class="sectionnumber">5.5&#x2003;</span>梯度下降的变体</h5>
<a id="document-autopage-60"></a>



<p>
根据使用数据量的不同，梯度下降有多种变体：
</p>

<figure id="autoid-6" class="table ">
<div class="center">
<table>

<tr style="display:none"><th>.</th></tr>


<tr class="tbrule">
<td class="tdl"><b> 算法类型 </b></td>
<td class="tdl"><b> 更新规则 </b></td>
<td class="tdl"><b> 特点 </b></td>
</tr>


<tr class="hline">
<td class="tdl"> 批量梯度下降 </td>
<td class="tdl">\(\theta \leftarrow \theta - \alpha \nabla \mathcal {L}(\theta )\)</td>
<td class="tdl"> 使用全部数据，稳定但慢 </td>
</tr>


<tr>
<td class="tdl"> 随机梯度下降 </td>
<td class="tdl">\(\theta \leftarrow \theta - \alpha \nabla \ell _i(\theta )\)</td>
<td class="tdl"> 使用单个样本，快但不稳定 </td>
</tr>


<tr>
<td class="tdl"> 小批量梯度下降 </td>
<td class="tdl">\(\theta \leftarrow \theta - \alpha \nabla \mathcal {L}_B(\theta )\)</td>
<td class="tdl"> 使用小批量，平衡稳定性和速度 </td>
</tr>


<tr class="tbrule" aria-hidden="true">
<td class="tdl"></td>
<td class="tdl"></td>
<td class="tdl"></td>
</tr>
</table>



<div class="figurecaption">


表&nbsp;3: 梯度下降算法变体对比

</div>

</div>

</figure>
<!--
...... subsection 梯度下降的收敛性分析......
-->
<h5 id="autosec-63"><span class="sectionnumber">5.6&#x2003;</span>梯度下降的收敛性分析</h5>
<a id="document-autopage-63"></a>



<p>
梯度下降的收敛性可以通过数学分析来理解：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
收敛性条件
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
如果损失函数 \(\mathcal {L}\) 是凸函数且 Lipschitz 连续，即存在 \(L &gt; 0\) 使得：
</p>

<p>
\[\|\nabla \mathcal {L}(\theta ) - \nabla \mathcal {L}(\theta &apos;)\| \leq L\|\theta - \theta &apos;\|\]
</p>

<p>
那么当学习率 \(\alpha &lt; \frac {2}{L}\) 时，梯度下降保证收敛到全局最优解。
</p>
</div>

</div>

<p>
对于非凸函数（如神经网络）
            ，梯度下降只能保证收敛到局部最优解或鞍点。
</p>
<!--
...... section 综合实例：训练线性回归模型......
-->
<h4 id="autosec-64"><span class="sectionnumber">6&#x2003;</span>综合实例：训练线性回归模型</h4>
<a id="document-autopage-64"></a>
<!--
...... subsection 问题设定......
-->
<h5 id="autosec-65"><span class="sectionnumber">6.1&#x2003;</span>问题设定</h5>
<a id="document-autopage-65"></a>



<p>
让我们通过一个完整的例子来演示计算图、反向传播和梯度下降的协同工作。我们要训练一个简单的线性回归模型：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
线性回归模型
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
模型：\(f(x) = wx + b\) 数据点：\((2, 5)\) 损失函数：\(L = (\hat {y} - y)^2\) 目标：找到最优的 \(w\) 和 \(b\)
</p>
</div>

</div>
<!--
...... subsection 计算图表示......
-->
<h5 id="autosec-66"><span class="sectionnumber">6.2&#x2003;</span>计算图表示</h5>
<a id="document-autopage-66"></a>



<figure id="autoid-7" class="figure ">
<div class="center">

<p>
<span
      id="lateximage-document-8"
      class="lateximagesource"
><!--
x
    ×            预测值
w       +   ŷ         L
b                真实值 y = 5
--><img
   src="./Assets//image-8.svg"
   alt="(-tikz-&nbsp;diagram)"
   style=""
   class="lateximage"
></span>
</p>



<div class="figurecaption">


图&nbsp;4: 线性回归模型的计算图

</div>

</div>

</figure>
<!--
...... subsection 详细训练过程......
-->
<h5 id="autosec-70"><span class="sectionnumber">6.3&#x2003;</span>详细训练过程</h5>
<a id="document-autopage-70"></a>




<div
      class="tcolorbox"
      style=" border: 1px solid #008080; background: #FFFFFF; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #008080; border-bottom: 1px solid #008080; background: #008080; color: #FFFFFF; "
>

<p>
第 1 轮迭代详细计算
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
<b> 初始化：</b> \(w=1\), \(b=0\), 学习率 \(\alpha =0.1\)
</p>

<p>
<b> 前向传播：</b>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker">1.</span> \(\hat {y} = w \times x + b = 1 \times 2 + 0 = 2\)
</p>


</li>
<li>


<p>
<span class="listmarker">2.</span> \(L = (\hat {y} - y)^2 = (2 - 5)^2 = 9\)
</p>
</li>
</ul>

<p>
<b> 反向传播：</b>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker">1.</span> \(\frac {\partial L}{\partial \hat {y}} = 2(\hat {y} - y) = 2(2-5) = -6\)
</p>


</li>
<li>


<p>
<span class="listmarker">2.</span> \(\frac {\partial \hat {y}}{\partial w} = x = 2\), \(\frac {\partial \hat {y}}{\partial b} = 1\)
</p>


</li>
<li>


<p>
<span class="listmarker">3.</span> \(\frac {\partial L}{\partial w} = \frac {\partial L}{\partial \hat {y}} \times \frac {\partial \hat {y}}{\partial w} = -6 \times 2 = -12\)
</p>


</li>
<li>


<p>
<span class="listmarker">4.</span> \(\frac {\partial L}{\partial b} = \frac {\partial L}{\partial \hat {y}} \times \frac {\partial \hat {y}}{\partial b} = -6 \times 1 = -6\)
</p>
</li>
</ul>

<p>
<b> 梯度下降更新：</b>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker">1.</span> \(w_{\text {新}} = w - \alpha \frac {\partial L}{\partial w} = 1 - 0.1 \times (-12) = 2.2\)
</p>


</li>
<li>


<p>
<span class="listmarker">2.</span> \(b_{\text {新}} = b - \alpha \frac {\partial L}{\partial b} = 0 - 0.1 \times (-6) = 0.6\)
</p>
</li>
</ul>

<p>
<b> 结果：</b> 新的参数 \(w=2.2\), \(b=0.6\)，预测值 \(\hat {y} = 2.2 \times 2 + 0.6 = 5.0\)，更接近真实值 \(y=5\)。
</p>
</div>

</div>
<!--
...... subsection 多轮迭代的收敛过程......
-->
<h5 id="autosec-71"><span class="sectionnumber">6.4&#x2003;</span>多轮迭代的收敛过程</h5>
<a id="document-autopage-71"></a>



<p>
通过多轮迭代，模型会逐步收敛到最优解：
</p>

<figure id="autoid-8" class="table ">
<div class="center">
<table>

<tr style="display:none"><th>.</th></tr>


<tr class="tbrule">
<td class="tdc"><b> 迭代轮数 </b></td>
<td class="tdc"><b>\(w\)</b></td>
<td class="tdc"><b>\(b\)</b></td>
<td class="tdc"><b> 预测值 \(\hat {y}\)</b></td>
<td class="tdc"><b> 损失 \(L\)</b></td>
</tr>


<tr class="hline">
<td class="tdc">0</td>
<td class="tdc">1.000</td>
<td class="tdc">0.000</td>
<td class="tdc">2.000</td>
<td class="tdc">9.000</td>
</tr>


<tr>
<td class="tdc">1</td>
<td class="tdc">2.200</td>
<td class="tdc">0.600</td>
<td class="tdc">5.000</td>
<td class="tdc">0.000</td>
</tr>


<tr>
<td class="tdc">2</td>
<td class="tdc">2.200</td>
<td class="tdc">0.600</td>
<td class="tdc">5.000</td>
<td class="tdc">0.000</td>
</tr>


<tr class="tbrule" aria-hidden="true">
<td class="tdc"></td>
<td class="tdc"></td>
<td class="tdc"></td>
<td class="tdc"></td>
<td class="tdc"></td>
</tr>
</table>



<div class="figurecaption">


表&nbsp;4: 线性回归训练过程

</div>

</div>

</figure>

<p>
可以看到，仅经过一轮迭代，模型就找到了完美拟合数据的参数。
</p>
<!--
...... section 现代深度学习框架的实现......
-->
<h4 id="autosec-74"><span class="sectionnumber">7&#x2003;</span>现代深度学习框架的实现</h4>
<a id="document-autopage-74"></a>
<!--
...... subsection 自动微分系统......
-->
<h5 id="autosec-75"><span class="sectionnumber">7.1&#x2003;</span>自动微分系统</h5>
<a id="document-autopage-75"></a>



<p>
现代深度学习框架（如 PyTorch、TensorFlow、JAX）都内置了自动微分系统，这些系统基于计算图和反向传播原理：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
自动微分的关键特性
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b> 动态计算图：</b> 在运行时构建计算图，便于调试和动态控制流
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 静态计算图：</b> 预先构建完整的计算图，优化执行效率
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 梯度追踪：</b> 自动记录前向传播的操作序列
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 内存优化：</b> 智能管理中间结果的存储和释放
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection PyTorch 实现示例......
-->
<h5 id="autosec-76"><span class="sectionnumber">7.2&#x2003;</span>PyTorch 实现示例</h5>
<a id="document-autopage-76"></a>




<div
      class="tcolorbox"
      style=" border: 1px solid #008080; background: #FFFFFF; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #008080; border-bottom: 1px solid #008080; background: #008080; color: #FFFFFF; "
>

<p>
PyTorch 中的自动微分
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>
<div class="figurecaption">

</div>
<p>


</p>
<pre class="programlisting">
1    import torch
2
3    # 定义参数（启用梯度追踪）
4    w = torch . tensor (1.0,     requires_grad =True)
5    b = torch . tensor (0.0,     requires_grad =True)
6    x = torch . tensor (2.0)
7     y_true = torch . tensor (5.0)
8
9    # 前向传播
10    y_pred = w * x + b
11     loss = ( y_pred − y_true ) ** 2
12
13    # 反向传播（自动计算梯度）
14     loss . backward()
15
16    print ( f”梯度: dL/dw = {w.grad }, dL/db = {b. grad }”)
17    # 输出 : 梯度 : dL/dw = −12.0, dL/db = −6.0
18
19    # 手动更新参数（模拟梯度下降）
20    with torch . no_grad () :
21         w −= 0.1 * w.grad
22         b −= 0.1 * b. grad
23         # 清空梯度
24         w.grad . zero_ ()
25         b. grad . zero_ ()
</pre>


           <div class="figurecaption">
</div>

</div>

</div>
<!--
...... subsection 优化器的抽象......
-->
<h5 id="autosec-78"><span class="sectionnumber">7.3&#x2003;</span>优化器的抽象</h5>
<a id="document-autopage-78"></a>



<p>
现代框架提供了各种优化器来简化梯度下降过程：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #008080; background: #FFFFFF; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #008080; border-bottom: 1px solid #008080; background: #008080; color: #FFFFFF; "
>

<p>
使用优化器
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>
<div class="figurecaption">

</div>
<p>


</p>
<pre class="programlisting">
1    import torch . optim as optim
2
3    # 定义模型和优化器
4    model = torch . nn. Linear (1, 1)   # 线性回归模型
5    optimizer = optim.SGD(model.parameters () , lr =0.1)
6
7    # 训练循环
8    for epoch in range (100) :
9        # 前向传播
10        y_pred = model(x)
11         loss = ( y_pred − y_true ) ** 2
12
13        # 反向传播
14         optimizer . zero_grad ()   # 清空梯度
15         loss . backward()          # 计算梯度
16         optimizer . step ()        # 更新参数
</pre>


           <div class="figurecaption">
</div>

</div>

</div>
<!--
...... section 高级主题与优化策略......
-->
<h4 id="autosec-80"><span class="sectionnumber">8&#x2003;</span>高级主题与优化策略</h4>
<a id="document-autopage-80"></a>
<!--
...... subsection 梯度消失与梯度爆炸......
-->
<h5 id="autosec-81"><span class="sectionnumber">8.1&#x2003;</span>梯度消失与梯度爆炸</h5>
<a id="document-autopage-81"></a>



<p>
在深层网络中，反向传播可能面临梯度消失或梯度爆炸问题：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
梯度消失（Vanishing Gradient）
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b> 原因：</b> 连续的小梯度相乘导致最终梯度趋近于零
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 影响：</b> 深层网络的前面层无法有效学习
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 解决方案：</b> ReLU 激活函数、残差连接、批量归一化
</p>
</li>
</ul>

</div>

</div>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
梯度爆炸（Exploding Gradient）
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b> 原因：</b> 连续的大梯度相乘导致梯度数值溢出
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 影响：</b> 训练不稳定，参数更新过大
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 解决方案：</b> 梯度裁剪、权重初始化、学习率调整
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 二阶优化方法......
-->
<h5 id="autosec-82"><span class="sectionnumber">8.2&#x2003;</span>二阶优化方法</h5>
<a id="document-autopage-82"></a>



<p>
除了梯度下降，还有基于二阶导数的优化方法：
</p>

<figure id="autoid-9" class="table ">
<div class="center">
<table>

<tr style="display:none"><th>.</th></tr>


<tr class="tbrule">
<td class="tdl"><b> 方法 </b></td>
<td class="tdl"><b> 原理 </b></td>
<td class="tdl"><b> 特点 </b></td>
</tr>


<tr class="hline">
<td class="tdl"> 牛顿法 </td>
<td class="tdl"> 使用 Hessian 矩阵 </td>
<td class="tdl"> 收敛快但计算昂贵 </td>
</tr>


<tr>
<td class="tdl"> 拟牛顿法 </td>
<td class="tdl"> 近似 Hessian 矩阵 </td>
<td class="tdl"> 平衡收敛速度和计算成本 </td>
</tr>


<tr>
<td class="tdl"> 共轭梯度法 </td>
<td class="tdl"> 利用共轭方向 </td>
<td class="tdl"> 适合大规模问题 </td>
</tr>


<tr>
<td class="tdl"> 自然梯度 </td>
<td class="tdl"> 考虑参数空间的几何结构 </td>
<td class="tdl"> 在信息几何框架下优化 </td>
</tr>


<tr class="tbrule" aria-hidden="true">
<td class="tdl"></td>
<td class="tdl"></td>
<td class="tdl"></td>
</tr>
</table>



<div class="figurecaption">


表&nbsp;5: 二阶优化方法对比

</div>

</div>

</figure>
<!--
...... subsection 自适应优化算法......
-->
<h5 id="autosec-85"><span class="sectionnumber">8.3&#x2003;</span>自适应优化算法</h5>
<a id="document-autopage-85"></a>



<p>
现代深度学习广泛使用自适应优化算法：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
Adam 优化器
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
Adam（Adaptive Moment Estimation）结合了动量法和 RMSProp 的优点：
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b> 动量：</b> 累积梯度的一阶矩（均值）
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 自适应学习率：</b> 累积梯度的二阶矩（未中心化的方差）
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 偏差校正：</b> 解决初始阶段的偏差问题
</p>
</li>
</ul>

<p>
更新规则：
</p>
<span class="hidden"> \(\seteqnumber{0}{}{0}\)</span>



<!--




                                                                                       mt = β1 mt−1 + (1 − β1 )gt                                                                              (1)
                                                                                        vt = β2 vt−1 + (1 − β2 )gt2                                                                            (2)
                                                                                                mt
                                                                                       m̂t =                                                                                                   (3)
                                                                                              1 − β1t
                                                                                                vt
                                                                                        v̂t =                                                                                                  (4)
                                                                                              1 − β2t
                                                                                                       m̂t
                                                                                      θt+1 = θt − α √                                                                                          (5)
                                                                                                      v̂t + ϵ



-->



<p>


\begin{align}
m_t &amp;= \beta _1 m_{t-1} + (1 - \beta _1) g_t \\ v_t &amp;= \beta _2 v_{t-1} + (1 - \beta _2) g_t^2 \\ \hat {m}_t &amp;= \frac {m_t}{1 - \beta _1^t} \\ \hat {v}_t &amp;= \frac {v_t}{1 -
\beta _2^t} \\ \theta _{t+1} &amp;= \theta _t - \alpha \frac {\hat {m}_t}{\sqrt {\hat {v}_t} + \epsilon }
\end{align}


</p>
</div>

</div>
<!--
...... section 结论与展望......
-->
<h4 id="autosec-86"><span class="sectionnumber">9&#x2003;</span>结论与展望</h4>
<a id="document-autopage-86"></a>
<!--
...... subsection 核心概念总结......
-->
<h5 id="autosec-87"><span class="sectionnumber">9.1&#x2003;</span>核心概念总结</h5>
<a id="document-autopage-87"></a>



<p>
本文系统性地介绍了深度学习中三个核心数学概念：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
计算图：数学表达式的可视化表示
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> 将复杂计算分解为基本操作的图结构
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 为自动微分提供数学基础
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 便于理解和调试复杂模型
</p>
</li>
</ul>

</div>

</div>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
反向传播：高效的梯度计算算法
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> 基于链式法则从输出向输入传播梯度
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 避免了重复计算，大大提高了效率
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 是现代深度学习框架的核心
</p>
</li>
</ul>

</div>

</div>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
梯度下降：参数优化的基本方法
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> 利用梯度信息寻找函数最小值
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 学习率控制更新步长，影响收敛性
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> 有多种变体和改进算法
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 技术发展历程......
-->
<h5 id="autosec-88"><span class="sectionnumber">9.2&#x2003;</span>技术发展历程</h5>
<a id="document-autopage-88"></a>



<p>
这些概念的发展体现了深度学习领域的演进：
</p>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b>1960s‑1970s：</b> 理论基础建立，反向传播思想萌芽
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b>1980s：</b> 反向传播算法被重新发现和推广
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b>1990s：</b> 计算图概念在自动微分中系统化
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b>2000s：</b> 深度学习框架开始集成这些技术
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b>2010s 至今：</b> 成为所有现代 AI 系统的标准组件
</p>
</li>
</ul>
<!--
...... subsection 未来发展方向......
-->
<h5 id="autosec-89"><span class="sectionnumber">9.3&#x2003;</span>未来发展方向</h5>
<a id="document-autopage-89"></a>



<p>
随着深度学习技术的不断发展，这些基础概念也在持续演进：
</p>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
计算图的扩展
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b> 动态计算图：</b> 支持条件分支和循环结构
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 分布式计算：</b> 在多个设备上并行执行计算图
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 符号微分：</b> 结合符号计算和自动微分
</p>
</li>
</ul>

</div>

</div>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
反向传播的优化
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b> 内存优化：</b> 减少中间结果的存储需求
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 计算优化：</b> 利用硬件特性加速梯度计算
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 稀疏梯度：</b> 处理大规模稀疏数据
</p>
</li>
</ul>

</div>

</div>



<div
      class="tcolorbox"
      style=" border: 1px solid #404040; background: #F2F2F2; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #404040; border-bottom: 1px solid #404040; background: #404040; color: #FFFFFF; "
>

<p>
优化算法的创新
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">‧</span> <b> 元学习：</b> 学习优化算法本身
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 联邦学习：</b> 在分布式环境中优化模型
</p>


</li>
<li>


<p>
<span class="listmarker">‧</span> <b> 量子优化：</b> 利用量子计算加速优化过程
</p>
</li>
</ul>

</div>

</div>
<!--
...... subsection 实际应用建议......
-->
<h5 id="autosec-90"><span class="sectionnumber">9.4&#x2003;</span>实际应用建议</h5>
<a id="document-autopage-90"></a>



<p>
对于实践者，我们建议：
</p>

<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker">1.</span> <b> 理解原理：</b> 深入理解这些基础概念，而不仅仅是调用 API
</p>


</li>
<li>


<p>
<span class="listmarker">2.</span> <b> 掌握调试：</b> 学会使用计算图可视化工具调试模型
</p>


</li>
<li>


<p>
<span class="listmarker">3.</span> <b> 优化策略：</b> 根据具体问题选择合适的优化算法和超参数
</p>


</li>
<li>


<p>
<span class="listmarker">4.</span> <b> 持续学习：</b> 关注最新研究成果和技术发展
</p>
</li>
</ul>



<div
      class="tcolorbox"
      style=" border: 1px solid #BF0040; background: #FFFFFF; "
>



<div
      class="tcolorboxtitle"
      style=" border-top: 1px solid #BF0040; border-bottom: 1px solid #BF0040; background: #BF0040; color: #FFFFFF; "
>

<p>
重要启示
</p>
</div>



<div
      class="tcolorboxupper"
      style=" color: #000000; "
>

<p>
计算图、反向传播和梯度下降不仅是技术工具，更是理解深度学习本质的钥匙。掌握这些基础概念，能够帮助我们更好地设计模型、调试问题和推动技术创新。
</p>
</div>

</div>
<!--
...... section 参考文献......
-->
<h4 id="autosec-91">参考文献</h4>
<a id="document-autopage-91"></a>



<ul class="list" style="list-style-type:none">


<li>
<p>
<span class="listmarker">[1]&#x2003;</span> Rumelhart, D. E., Hinton, G. E., &amp; Williams, R. J. (1986). Learning representations by back‑propagating errors. <i>Nature</i>, 323(6088), 533‑536.
</p>
</li>
<li>


<p>
<span class="listmarker">[2]&#x2003;</span> LeCun, Y., Bengio, Y., &amp; Hinton, G. (2015). Deep learning. <i>Nature</i>, 521(7553), 436‑444.
</p>
</li>
<li>


<p>
<span class="listmarker">[3]&#x2003;</span> Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). <i>Deep Learning</i>. MIT Press.
</p>
</li>
<li>


<p>
<span class="listmarker">[4]&#x2003;</span> Baydin, A. G., Pearlmutter, B. A., Radul, A. A., &amp; Siskind, J. M. (2018). Automatic differentiation in machine learning: a survey. <i>Journal of Machine Learning
Research</i>, 18(1), 5595‑5637.
</p>
</li>
<li>


<p>
<span class="listmarker">[5]&#x2003;</span> Kingma, D. P., &amp; Ba, J. (2014). Adam: A method for stochastic optimization. <i>arXiv preprint arXiv:1412.6980</i>.
</p>
</li>
<li>


<p>
<span class="listmarker">[6]&#x2003;</span> Bottou, L., Curtis, F. E., &amp; Nocedal, J. (2018). Optimization methods for large‑scale machine learning. <i>SIAM Review</i>, 60(2), 223‑311.
</p>
</li>
<li>


<p>
<span class="listmarker">[7]&#x2003;</span> Pascanu, R., Mikolov, T., &amp; Bengio, Y. (2013). On the difficulty of training recurrent neural networks. In <i>International conference on machine learning</i>
(pp. 1310‑1318).
</p>
</li>
<li>


<p>
<span class="listmarker">[8]&#x2003;</span> Sutskever, I., Martens, J., Dahl, G., &amp; Hinton, G. (2013). On the importance of initialization and momentum in deep learning. In <i>International conference
on machine learning</i> (pp. 1139‑1147).
</p>
<p>


</p>
</li>
</ul>

<a id="document-autofile-last"></a>
</section>

</main>

</div>

</body>
</html>
